{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"Intro Projects Blogs Skills About Me Contact","tags":["Welcome"]},{"location":"#welcome","title":"Hey there Hello..! Welcome to my Portfolio","text":"","tags":["Welcome"]},{"location":"#intro","title":"Hi, I'm Sunil Sharma","text":"<p>             I\u2019m a passionate Python Developer and Data Scientist who loves crafting innovative solutions and sharing my journey through engaging stories.         </p>","tags":["Welcome"]},{"location":"#explore","title":"I'm thrilled to have you here. Explore my projects, read my blogs, and get to know more about me.","text":"","tags":["Welcome"]},{"location":"#projects","title":"Featured Projects","text":"<p>Discover innovative projects where technology meets creativity.</p> Examination Management System DB <p>Multi-RDBMS exam/test management, automation, Docker, Python, CI-ready.</p> Know More S3 Faker <p>Fake data generator with S3/LocalStack integration for cloud testing.</p> Know More Paraxcel <p>Excel data extraction, transformation, and visualization toolkit.</p> Know More Test Management Site <p>Frontend web app for test management and result tracking.</p> Know More Explore Projects","tags":["Welcome"]},{"location":"#blogs","title":"Featured Blogs","text":"<p>Read my latest insights on development, data science, and tech trends.</p> Navigating the Nuances: database <p>A Developer's Guide to SQL Dialects (SQLite, MySQL, PostgreSQL)</p> Read More Beyond the Schema: database <p>A Practical Guide to Querying and Interacting with SQLite, MySQL, &amp; PostgreSQL Databases</p> Read More Read Blogs","tags":["Welcome"]},{"location":"#skills","title":"Skills &amp; Expertise","text":"<p>Continuously expanding my technical horizons</p> Programming Languages <p>                     SQL,                     Python,                     HTML,                     CSS,                     JavaScript,                 </p> Data Science <p>                     Machine Learning,                     Data Visualization,                     Statistical Analysis,                     Data Extraction,                     Data Transformation                 </p> Frameworks &amp; Libraries <p>                     Numpy,                     Pandas,                     Scikit-Learn,                 </p> Tools &amp; Technologies <p>                     Git,                     Docker,                     AWS,                     CI/CD,                     LocalStack,                     Microsoft Excel                 </p>","tags":["Welcome"]},{"location":"#about","title":"About Me","text":"<p>Learn more about my journey, skills, and experiences in tech.</p>                  Download Resume                           Get to Know Me","tags":["Welcome"]},{"location":"#contact","title":"Let's Connect","text":"<p>Have a question or a collaboration idea? Let's connect!</p>                  GitHub                               LinkedIn                           More Ways to Connect","tags":["Welcome"]},{"location":"about/","title":"About Me","text":"<p>Hi, I'm Sunil Sharma, a passionate Python Developer and Data Scientist. I specialize in building robust data-driven solutions and sharing practical knowledge through blogs. My expertise lies in data analysis, machine learning, and automation\u2014turning complex data into actionable insights.</p>"},{"location":"about/#explore-resume-and-certifications","title":"Explore Resume and Certifications","text":"Resume"},{"location":"about/#my-journey","title":"My Journey","text":""},{"location":"about/#education","title":"Education","text":"<ul> <li>Degree: Bachelor's in Computer Science</li> <li>University: Devi Ahilyabai Vishav Vidhyalay(DAVV), Indore</li> <li>Collage: PMB Gujarati Science College, Indore</li> <li>2015-2018</li> </ul>"},{"location":"about/#experience","title":"Experience","text":"<p>I have worked on impactful projects in data science, machine learning, and automation. My focus is on designing reproducible pipelines, advanced analytics, and delivering value through data.</p>"},{"location":"about/#projects","title":"Projects","text":"<ul> <li> <p>GSTN Hackathon: Predictive Binary Classification (2024)  </p> <ul> <li>Developed a high-performance, interpretable ML pipeline for binary classification on anonymized GSTN data. Achieved &gt;97% accuracy and strong F1/MCC, with strict compliance and reproducibility.</li> <li>Read more</li> </ul> </li> <li> <p>Examination Management System Database (2024)  </p> <ul> <li>Designed and implemented a modular, production-ready database system for managing exams, students, proctoring, and results. Multi-RDBMS support (SQLite, MySQL, PostgreSQL), Python automation, Dockerized environments, and automated testing.  </li> <li>Read more</li> </ul> </li> <li> <p>S3 Faker (2025)  </p> <ul> <li>Created a fake data generator with AWS S3 (LocalStack) integration, supporting CSV/JSON/Parquet and automating uploads for cloud pipeline testing.  </li> <li>Read more</li> </ul> </li> <li> <p>Paraxcel (2025)  </p> <ul> <li>Built a Python toolkit for advanced Excel data extraction, transformation, and visualization, leveraging Pandas, Openpyxl, and Tkinter for a local-first GUI.  </li> <li>Read more</li> </ul> </li> <li> <p>Naukri Webscraper (2024\u20132025)  </p> <ul> <li>Developed a Selenium-powered Python tool to automate job search and data extraction from Naukri.com, with skill-based filtering, CSV export, and robust automated testing.  </li> <li>Read more</li> </ul> </li> <li> <p>Test Management Site (2024\u20132025)  </p> <ul> <li>Designed a dynamic, responsive frontend web app for test creation, execution, and result tracking using vanilla JS, HTML, CSS, Bootstrap, and localStorage.  </li> <li>Read more</li> </ul> </li> </ul>"},{"location":"about/#explore","title":"Explore","text":"More Projects       More Blogs"},{"location":"about/#skills","title":"Skills","text":"<ul> <li> <p>Data Science &amp; Analysis: </p> <ul> <li>Advanced data wrangling, cleaning, and exploratory analysis using pandas and numpy (see GSTN Hackathon, Paraxcel, S3 Faker)</li> <li>Feature engineering, statistical analysis, and visualization with matplotlib, seaborn, Plotly.js (see GSTN Hackathon, Paraxcel)</li> </ul> </li> <li> <p>Machine Learning: </p> <ul> <li>End-to-end ML pipelines with scikit-learn, XGBoost, LightGBM (core to GSTN Hackathon)</li> <li>Model selection, hyperparameter tuning, cross-validation, and explainability (SHAP)</li> </ul> </li> <li> <p>Automation &amp; Scripting: </p> <ul> <li>Python scripting for data pipelines, automation, and ETL tasks (see S3 Faker, Naukri Webscraper)</li> </ul> </li> <li> <p>Web Scraping: </p> <ul> <li>Automated data extraction with Selenium, BeautifulSoup (Naukri Webscraper)</li> </ul> </li> <li> <p>Database &amp; Data Engineering: </p> <ul> <li>SQL (SQLite, MySQL, PostgreSQL): schema design, query optimization, triggers, and views (Examination Management System DB) </li> </ul> </li> <li> <p>Programming Languages: </p> <ul> <li>Python (primary), JavaScript, HTML, CSS</li> </ul> </li> <li> <p>Web Development: </p> <ul> <li>Flask, Django, FastAPI (see GitHub for repos)</li> </ul> </li> <li> <p>AI &amp; Agentic Space: </p> <ul> <li>Mistralai, Gemini, Langchain, Langgraph, Huggingface (see GitHub for proof-of-concept and experimental repos)</li> </ul> </li> </ul>"},{"location":"about/#tools-dev-ops","title":"Tools &amp; Dev-Ops","text":"<ul> <li>Tools: VSCode, Jupyter Notebook, Git, GitHub, Docker, Kubernetes</li> <li>OS: Windows, Linux</li> <li>Cloud: AWS, Azure, GCP</li> </ul>"},{"location":"about/#blogs-knowledge-sharing","title":"Blogs &amp; Knowledge Sharing","text":"<p>I regularly write technical blogs on data science, machine learning, and database design. See Latest Blogs for in-depth guides, including:</p> <ul> <li>Navigating the Nuances: A Developer's Guide to SQL Dialects (SQLite, MySQL, PostgreSQL)</li> <li>Beyond the Schema: A Practical Guide to Querying and Interacting with SQLite, MySQL, &amp; PostgreSQL</li> </ul>"},{"location":"about/#hobbies-interests","title":"Hobbies &amp; Interests","text":"<ul> <li>Coding</li> <li>Blogging</li> <li>Reading</li> </ul>"},{"location":"contact/","title":"Contact","text":"<p>Feel free to reach out to me for any inquiries or collaborations.</p>"},{"location":"contact/#email","title":"Email","text":"<p>  Email </p>"},{"location":"contact/#social-links","title":"Social Links","text":"LinkedIn       GitHub       DockerHub       Twitter"},{"location":"resume/","title":"Resume/CV","text":""},{"location":"resume/#download-my-resumecv","title":"Download my Resume/CV","text":"Resume"},{"location":"resume/#know-more-about-me","title":"Know more about me","text":"Know More"},{"location":"resume/#certifications","title":"Certifications","text":"<ul> <li> <p>GSTN Hackathon </p> </li> <li> <p>CS50x - Introduction to Computer Science </p> </li> <li> <p>CS50 SQL - Introduction to Databases with SQL </p> </li> <li> <p>CS50P - Introduction to Programming with Python </p> </li> <li> <p>AI/ML for Geodata Analysis </p> </li> </ul>"},{"location":"resume/#explore-more","title":"Explore More","text":"<p> Licenses &amp; certifications</p>"},{"location":"tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"tags/#tag:automation","title":"Automation","text":"<ul> <li>            Naukri Web Scraper          </li> </ul>"},{"location":"tags/#tag:binary-classification","title":"Binary Classification","text":"<ul> <li>            GSTN Hackathon          </li> </ul>"},{"location":"tags/#tag:data-analysis","title":"Data Analysis","text":"<ul> <li>            Naukri Web Scraper          </li> </ul>"},{"location":"tags/#tag:data-science","title":"Data Science","text":"<ul> <li>            GSTN Hackathon          </li> </ul>"},{"location":"tags/#tag:database-design","title":"Database Design","text":"<ul> <li>            Exam Management System Database          </li> </ul>"},{"location":"tags/#tag:database-management","title":"Database Management","text":"<ul> <li>            Exam Management System Database          </li> </ul>"},{"location":"tags/#tag:frontend","title":"Frontend","text":"<ul> <li>            Test Management Site          </li> </ul>"},{"location":"tags/#tag:hackathon","title":"Hackathon","text":"<ul> <li>            GSTN Hackathon          </li> </ul>"},{"location":"tags/#tag:machine-learning","title":"Machine Learning","text":"<ul> <li>            GSTN Hackathon          </li> </ul>"},{"location":"tags/#tag:python","title":"Python","text":"<ul> <li>            GSTN Hackathon          </li> <li>            Naukri Web Scraper          </li> <li>            QA Docx to Excel          </li> </ul>"},{"location":"tags/#tag:selenium","title":"Selenium","text":"<ul> <li>            Naukri Web Scraper          </li> </ul>"},{"location":"tags/#tag:web-scraping","title":"Web Scraping","text":"<ul> <li>            Naukri Web Scraper          </li> </ul>"},{"location":"tags/#tag:welcome","title":"Welcome","text":"<ul> <li>            Welcome          </li> </ul>"},{"location":"blog/","title":"Blogs","text":"<p>Welcome to my blog! Here you will find my latest posts and updates.</p>"},{"location":"blog/#latest-blogs","title":"Latest Blogs","text":""},{"location":"blog/2025/05/07/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/","title":"Navigating the Nuances: A Developer's Guide to SQL Dialects (SQLite, MySQL, PostgreSQL)","text":"<p>As developers, we often encounter various SQL databases, each with its own flavor of <code>SQL</code>. While the core concepts remain similar, the devil is in the details \u2013 especially when it comes to schema definitions, data types, and procedural extensions like triggers.</p> <p>Recently, while working on an <code>Exam Management System (EMS)</code>, I had the opportunity to define the database schema for <code>SQLite</code>, <code>MySQL</code>, and <code>PostgreSQL</code>. This exercise highlighted some fascinating and crucial differences between these popular relational database management systems (RDBMS).</p> <p><code>\u2139\ufe0fNote</code>: This post aims to serve as a <code>practical guide</code> and a bit of a <code>cheatsheet</code>, drawing <code>insights</code> directly from the <code>schema</code> files of project.</p> <p>Project Repo : ems-db <code>&lt;-- root-dir-name</code></p> <ul> <li>sqlite/schema.sql</li> <li>psql/schema.sql</li> <li>mysql/schema.sql</li> </ul> <p>Whether you're a fellow developer looking for a quick reference or trying to gauge a database differences, I hope this comparison proves insightful!</p>"},{"location":"blog/2025/05/07/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#key-areas-of-difference-schema","title":"Key Areas of Difference: Schema","text":"<p>Let's dive into the specific areas where these <code>SQL dialects</code> diverge:</p> <p>Drill</p> <p><code>\u26a0\ufe0fWarning</code> : Always Refer to <code>Official Docs</code>, when in doubt. \"<code>Its not ultimate source of truth. It could be good starting point.</code>\"</p> <ul> <li>Understanding : Use project as reference.</li> <li>Prerequisites : Familiar with <code>sql syntax</code>, <code>client interaction</code>, <code>Docker</code>, and <code>Python</code> (language of choice).</li> </ul>"},{"location":"blog/2025/05/07/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#1-dropping-objects-tables-views-indexes","title":"1. Dropping Objects (Tables, Views, Indexes)","text":"<p>The syntax for dropping database objects is largely similar, but identifier quoting can vary.</p> <ul> <li> <p>SQLite &amp; PostgreSQL: Use double quotes for identifiers if they contain special characters or are case-sensitive (though often optional).</p> <pre><code>-- SQLite &amp; PostgreSQL\nDROP VIEW IF EXISTS \"tests_history\";\nDROP TABLE IF EXISTS \"students\";\n</code></pre> </li> <li> <p>MySQL: Uses backticks for identifiers.</p> <pre><code>-- MySQL\nDROP VIEW IF EXISTS `tests_history`;\nDROP TABLE IF EXISTS `students`;\n</code></pre> </li> </ul>"},{"location":"blog/2025/05/07/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#2-data-types-and-auto-incrementing-ids","title":"2. Data Types and Auto-Incrementing IDs","text":"<p>This is a significant area of divergence.</p> Feature SQLite PostgreSQL MySQL Auto-Increment ID <code>id INTEGER PRIMARY KEY</code> (implicitly <code>AUTOINCREMENT</code> if it's the primary key and integer type) or explicitly <code>id INTEGER PRIMARY KEY AUTOINCREMENT</code> <code>id SERIAL PRIMARY KEY</code> (creates a sequence) <code>id INT AUTO_INCREMENT PRIMARY KEY</code> Text <code>TEXT</code> <code>VARCHAR(n)</code>, <code>TEXT</code> <code>VARCHAR(n)</code>, <code>TEXT</code> Integer <code>INTEGER</code> <code>INTEGER</code>, <code>SMALLINT</code> (for <code>is_correct</code>, <code>score</code>) <code>INT</code>, <code>TINYINT(1)</code> (often for booleans) Boolean <code>INTEGER NOT NULL CHECK (\"is_correct\" IN (0, 1))</code> <code>SMALLINT NOT NULL DEFAULT 0 CHECK (\"is_correct\" IN (0, 1))</code> (or native <code>BOOLEAN</code>) <code>TINYINT(1) NOT NULL DEFAULT '0'</code> Date/Time <code>NUMERIC</code> (stored as text, real, or int), <code>DATETIME('now', 'localtime')</code> <code>TIMESTAMP WITH TIME ZONE</code>, <code>CURRENT_TIMESTAMP</code> <code>DATETIME</code>, <code>TIME</code>, <code>CURRENT_TIMESTAMP</code>, <code>NOW()</code> Duration/Interval <code>NUMERIC</code> (e.g., storing time as text 'HH:MM:SS') <code>INTERVAL</code> <code>TIME</code> (for durations within 24h) or calculate ENUM Types Simulated with <code>CHECK</code> constraint: <code>CHECK (\"status\" IN (...))</code> Native: <code>CREATE TYPE \"events_type\" AS ENUM (...);</code> Native: <code>status ENUM ('active', 'completed')</code> <p>Example: Students Table ID</p> <ul> <li> <p>SQLite:</p> <pre><code>CREATE TABLE \"students\" (\n    \"id\" INTEGER,\n    -- ...\n    PRIMARY KEY (\"id\")\n);\n</code></pre> </li> <li> <p>PostgreSQL:</p> <pre><code>CREATE TABLE IF NOT EXISTS \"students\" (\n    \"id\" SERIAL,\n    -- ...\n    PRIMARY KEY(\"id\")\n);\n</code></pre> </li> <li> <p>MySQL:</p> <pre><code>CREATE TABLE IF NOT EXISTS `students` (\n    `id` INT AUTO_INCREMENT,\n    -- ...\n    PRIMARY KEY (`id`)\n);\n</code></pre> </li> </ul> <p>Example: ENUM for <code>tests_sessions.status</code></p> <ul> <li> <p>SQLite:</p> <pre><code>CREATE TABLE \"tests_sessions\" (\n    -- ...\n    \"status\" TEXT NOT NULL DEFAULT 'in-progress' CHECK (\n        \"status\" IN ('in-progress', 'ended', 'completed')\n    )\n    -- ...\n);\n</code></pre> </li> <li> <p>PostgreSQL:</p> <pre><code>CREATE TYPE \"tests_session_status_type\" AS ENUM ('in-progress', 'ended', 'completed');\nCREATE TABLE IF NOT EXISTS \"tests_sessions\" (\n    -- ...\n    \"status\" \"tests_session_status_type\" NOT NULL DEFAULT 'in-progress',\n    -- ...\n);\n</code></pre> </li> <li> <p>MySQL:</p> <pre><code>CREATE TABLE IF NOT EXISTS `tests_sessions` (\n    -- ...\n    `status` ENUM ('in-progress', 'ended', 'completed') NOT NULL DEFAULT 'in-progress',\n    -- ...\n);\n</code></pre> </li> </ul>"},{"location":"blog/2025/05/07/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#3-default-values-especially-timestamps","title":"3. Default Values (Especially Timestamps)","text":"<ul> <li> <p>SQLite: Uses functions like <code>DATETIME('now', 'localtime')</code>.</p> <pre><code>\"start\" NUMERIC NOT NULL DEFAULT (DATETIME('now', 'localtime'))\n</code></pre> </li> <li> <p>PostgreSQL: Uses <code>CURRENT_TIMESTAMP</code>.</p> <pre><code>\"start\" TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT CURRENT_TIMESTAMP\n</code></pre> </li> <li> <p>MySQL: Uses <code>CURRENT_TIMESTAMP</code> or <code>NOW()</code>.</p> <pre><code>`start` DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP\n</code></pre> </li> </ul>"},{"location":"blog/2025/05/07/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#4-trigger-syntax","title":"4. Trigger Syntax","text":"<p>Triggers are where the syntactical differences become very pronounced.</p> <p>Common Goal: Set the <code>end</code> time of a <code>tests_sessions</code> row upon insertion, based on the test's duration.</p> <ul> <li> <p>SQLite:</p> <pre><code>CREATE TRIGGER \"set_end_for_test_session\" AFTER INSERT ON \"tests_sessions\"\nBEGIN\nUPDATE \"tests_sessions\"\nSET\n    \"end\" = DATETIME(new.start, '+' || (\n            SELECT TIME(duration)\n            FROM \"tests\" AS t\n            WHERE t.\"id\" = new.\"test_id\"\n        ))\nWHERE \"id\" = new.id;\nEND;\n</code></pre> <ul> <li>Uses <code>BEGIN...END;</code> block.</li> <li><code>AFTER INSERT</code>.</li> <li><code>new</code> refers to the inserted row.</li> <li>Date/time arithmetic involves string concatenation for modifiers.</li> </ul> </li> <li> <p>PostgreSQL:</p> <pre><code>CREATE OR REPLACE FUNCTION set_end_for_test_session_fn()\nRETURNS TRIGGER AS $$\nBEGIN\n    NEW.end := NEW.start + (\n        SELECT \"duration\" FROM \"tests\" WHERE \"id\" = NEW.test_id\n    );\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER \"set_end_for_test_session\" BEFORE INSERT ON\n\"tests_sessions\" FOR EACH ROW\nEXECUTE FUNCTION set_end_for_test_session_fn();\n</code></pre> <ul> <li>Requires a separate trigger function written in a procedural language (e.g., <code>plpgsql</code>).</li> <li><code>BEFORE INSERT</code> (can modify <code>NEW</code> directly).</li> <li><code>FOR EACH ROW</code> is explicit.</li> <li><code>NEW</code> is a record variable; assignments use <code>:=</code>.</li> <li><code>RETURN NEW</code> is crucial for <code>BEFORE</code> triggers.</li> <li>Interval arithmetic is more direct (<code>+ duration</code>).</li> </ul> </li> <li> <p>MySQL:</p> <pre><code>DELIMITER $$\nCREATE TRIGGER `set_end_for_test_session` BEFORE INSERT ON\n`tests_sessions` FOR EACH ROW\nBEGIN\n    SET NEW.end = DATE_ADD(\n        IFNULL(NEW.start, NOW()), -- NOW() if NEW.start is not yet set\n        INTERVAL (\n            SELECT TIME_TO_SEC(`duration`) / 60 FROM `tests` WHERE `id` = NEW.`test_id`\n        ) MINUTE\n    );\nEND$$\nDELIMITER ;\n</code></pre> <ul> <li>Uses <code>DELIMITER</code> to define a multi-statement trigger body.</li> <li><code>BEFORE INSERT</code>.</li> <li><code>FOR EACH ROW</code> is explicit.</li> <li><code>SET NEW.column = ...</code> for assignments.</li> <li>Uses functions like <code>DATE_ADD</code> and <code>TIME_TO_SEC</code> for time arithmetic.</li> </ul> </li> </ul> <p>Key Trigger Differences Summary:</p> Feature SQLite PostgreSQL MySQL Structure <code>BEGIN...END</code> directly in trigger Separate function + trigger definition <code>DELIMITER $$ BEGIN...END$$ DELIMITER ;</code> Timing <code>AFTER</code>/<code>BEFORE</code>/<code>INSTEAD OF</code> <code>AFTER</code>/<code>BEFORE</code>/<code>INSTEAD OF</code> <code>AFTER</code>/<code>BEFORE</code> Row Reference <code>new</code>, <code>old</code> <code>NEW</code>, <code>OLD</code> (case-sensitive in PL/pgSQL) <code>NEW</code>, <code>OLD</code> Modification <code>UPDATE</code> statement for <code>AFTER</code> Direct assignment to <code>NEW</code> in <code>BEFORE</code> trigger Direct assignment to <code>NEW</code> in <code>BEFORE</code> trigger Return Value N/A for <code>AFTER</code> <code>RETURN NEW</code>/<code>OLD</code>/<code>NULL</code> for <code>BEFORE</code> N/A"},{"location":"blog/2025/05/07/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#5-time-and-interval-arithmetic","title":"5. Time and Interval Arithmetic","text":"<p>As seen in the trigger examples, how you add durations to timestamps varies:</p> <ul> <li> <p>SQLite: String manipulation with <code>DATETIME</code> function modifiers.</p> <pre><code>DATETIME(new.start, '+' || (SELECT TIME(duration) ...))\n</code></pre> </li> <li> <p>PostgreSQL: Direct arithmetic with <code>INTERVAL</code> types.</p> <pre><code>NEW.start + (SELECT \"duration\" FROM \"tests\" ...)\n</code></pre> </li> <li> <p>MySQL: Functions like <code>DATE_ADD()</code> and <code>INTERVAL</code> keyword.</p> <pre><code>DATE_ADD(NEW.start, INTERVAL X MINUTE) -- or HOUR, SECOND etc.\n</code></pre> <p>In my schema, I converted the <code>TIME</code> duration to seconds, then to minutes for <code>DATE_ADD</code>:</p> <pre><code>INTERVAL (SELECT TIME_TO_SEC(`duration`) / 60 FROM `tests` WHERE `id` = NEW.`test_id`) MINUTE\n</code></pre> </li> </ul>"},{"location":"blog/2025/05/07/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#6-conditional-logic-in-triggersqueries","title":"6. Conditional Logic in Triggers/Queries","text":"<ul> <li> <p>SQLite &amp; MySQL: <code>CASE WHEN ... THEN ... ELSE ... END</code> is standard.     SQLite example from <code>set_score_of_result</code> trigger:</p> <pre><code>\"feedback\" = CASE WHEN (...) = 0 THEN 'need-improvement' ELSE 'great' END\n</code></pre> <p>MySQL example from <code>set_score_of_result</code> trigger:</p> <pre><code>SET NEW.feedback = CASE WHEN (...) = 0 THEN 'need-improvement' ELSE 'great' END;\n</code></pre> </li> <li> <p>PostgreSQL: Supports <code>CASE</code> expressions, but also <code>IF...THEN...ELSE...END IF;</code> in PL/pgSQL functions.     PostgreSQL example from <code>set_score_of_result_fn</code> trigger function:</p> <pre><code>IF NEW.score = 0 THEN\n    NEW.feedback := 'need-improvement';    \nELSE\n    NEW.feedback := 'great';\nEND IF;\n</code></pre> </li> </ul>"},{"location":"blog/2025/05/07/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#7-handling-nulls-in-aggregate-functions","title":"7. Handling NULLs in Aggregate Functions","text":"<p>When summing scores, if no results exist for a test session, <code>SUM()</code> might return <code>NULL</code>.</p> <ul> <li> <p>SQLite: <code>SUM()</code> on an empty set returns <code>NULL</code>. My schema doesn't explicitly handle this for <code>final_score</code> in the trigger, which might be an oversight if a report could be generated before any results.</p> <pre><code>(SELECT SUM(\"results\".\"score\") FROM \"results\" WHERE \"results\".\"test_session_id\" = new.id)\n</code></pre> </li> <li> <p>PostgreSQL: Uses <code>COALESCE(SUM(\"score\"), 0)</code> to default to <code>0</code> if <code>SUM</code> is <code>NULL</code>.</p> <pre><code>(SELECT COALESCE(SUM(\"score\"),0) FROM \"results\" WHERE \"test_session_id\" = NEW.id)\n</code></pre> </li> <li> <p>MySQL: Uses <code>IFNULL(SUM(\\</code>score`), 0)` for the same purpose.</p> <pre><code>(SELECT IFNULL(SUM(`score`),0) FROM `results` WHERE `test_session_id` = NEW.id)\n</code></pre> </li> </ul>"},{"location":"blog/2025/05/07/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#8-create-table-if-not-exists","title":"8. <code>CREATE TABLE IF NOT EXISTS</code>","text":"<p>This useful clause prevents errors if a table already exists.</p> <ul> <li>SQLite: Supports <code>CREATE TABLE IF NOT EXISTS \"students\" (...)</code> (though not explicitly used in the provided <code>students</code> table creation, it's standard).</li> <li>PostgreSQL: <code>CREATE TABLE IF NOT EXISTS \"students\" (...)</code></li> <li>MySQL: <code>CREATE TABLE IF NOT EXISTS `students` (...)</code></li> </ul>"},{"location":"blog/2025/05/07/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#9-comments","title":"9. Comments","text":"<ul> <li>SQLite, PostgreSQL, MySQL: All support <code>--</code> for single-line comments.</li> <li>MySQL: Also supports <code>#</code> for single-line comments.</li> </ul>"},{"location":"blog/2025/05/07/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#10-index-creation","title":"10. Index Creation","text":"<p>The basic syntax is similar, but quoting and specific features (like conditional indexing) can differ.</p> <ul> <li> <p>SQLite &amp; PostgreSQL:</p> <pre><code>-- SQLite &amp; PostgreSQL\nCREATE INDEX \"idx_tests\" ON \"tests\" (\"title\");\n-- PostgreSQL specific (SQLite also supports WHERE in index but syntax might differ slightly)\nCREATE INDEX \"idx_questions_options_is_correct\" ON \"questions_options\" (\"is_correct\") WHERE \"is_correct\" = 1;\n</code></pre> </li> <li> <p>MySQL:</p> <pre><code>-- MySQL\nCREATE INDEX `idx_tests` ON `tests` (`title`);\n-- MySQL does not directly support WHERE clauses in CREATE INDEX like PostgreSQL/SQLite.\n-- For conditional indexing, you might index the column and rely on the optimizer,\n-- or use generated columns if applicable.\nCREATE INDEX `idx_questions_options_is_correct` ON `questions_options` (`is_correct`);\n</code></pre> </li> </ul>"},{"location":"blog/2025/05/07/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#11-time-zone-handling","title":"11. Time Zone Handling","text":"<ul> <li>SQLite: <code>DATETIME('now', 'localtime')</code> attempts to use local time. Time storage is less strict.</li> <li>PostgreSQL: Very robust. <code>TIMESTAMP WITH TIME ZONE</code> stores timestamps in UTC and converts them to the client's/session's time zone on retrieval. <code>SET TIME ZONE LOCAL;</code> can be used.</li> <li>MySQL: <code>DATETIME</code> stores \"wall clock\" time without time zone info. <code>TIMESTAMP</code> converts from current time zone to UTC for storage, and back on retrieval. Session time zone can be set.</li> </ul>"},{"location":"blog/2025/05/07/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#quick-cheatsheet-sqlite-vs-postgresql-vs-mysql","title":"Quick Cheatsheet: SQLite vs. PostgreSQL vs. MySQL","text":"Feature SQLite PostgreSQL MySQL Identifier Quoting <code>\"optional\"</code> <code>\"optional/case-sensitive\"</code> <code>`optional`</code> Auto Increment <code>INTEGER PRIMARY KEY [AUTOINCREMENT]</code> <code>SERIAL</code> or <code>IDENTITY</code> <code>INT AUTO_INCREMENT</code> Data Types (General) Flexible typing (TEXT, NUMERIC, INTEGER) Strict, rich types (VARCHAR, TEXT, INT, BIGINT, BOOLEAN, JSON, ARRAY, INTERVAL, ENUM) Strict types (VARCHAR, TEXT, INT, TINYINT, DATETIME, ENUM, JSON) ENUMs <code>CHECK</code> constraint <code>CREATE TYPE ... AS ENUM</code> <code>ENUM(...)</code> column type Triggers <code>BEGIN...END</code> Function-based (<code>CREATE FUNCTION ... EXECUTE FUNCTION</code>) <code>DELIMITER $$ BEGIN...END$$</code> Default Timestamp <code>DATETIME('now', 'localtime')</code> <code>CURRENT_TIMESTAMP</code> <code>CURRENT_TIMESTAMP</code> / <code>NOW()</code> Interval Arithmetic String manipulation with <code>datetime()</code> <code>+ INTERVAL '...'</code> <code>DATE_ADD(date, INTERVAL value unit)</code> Function for NULLs <code>IFNULL(val, default)</code> (or <code>COALESCE</code>) <code>COALESCE(val, default)</code> <code>IFNULL(val, default)</code> or <code>COALESCE(val, default)</code>"},{"location":"blog/2025/05/07/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#why-this-matters","title":"Why This Matters","text":"<p>For Developers:</p> <ul> <li>Adaptability: Understanding these differences allows you to switch between database systems more fluidly.</li> <li>Debugging: Syntax errors are common when moving SQL code; knowing the nuances helps pinpoint issues faster.</li> <li>Database Design: Choosing the right data types and features (like native ENUMs or interval types) can lead to a more efficient and maintainable schema.</li> <li>ORM Configuration: When using Object-Relational Mappers (ORMs), these differences are often abstracted, but knowing what's happening under the hood is invaluable for optimization and complex queries.</li> </ul> <p>Learnings:</p> <ul> <li>Depth of Understanding: Articulating these differences helps demonstrating a deeper-than-surface-level understanding of SQL and database systems.</li> <li>Practical Experience: It often indicates hands-on experience with multiple databases, which is a valuable asset in diverse tech environments.</li> <li>Problem-Solving: The ability to adapt a schema or queries for different SQL dialects showcases problem-solving skills.</li> </ul>"},{"location":"blog/2025/05/07/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#next-read","title":"Next Read \ud83d\udcd6","text":"<p>For Debunking <code>sql queries</code> and <code>clients interaction</code> differences b/w SQLite, MySQL, and PostgreSQL,</p> <ul> <li><code>Part-2</code> Beyond the Schema: A Practical Guide to Querying and Interacting with SQLite, MySQL, &amp; PostgreSQL</li> </ul> <p>Note : It's build upon where this post left.</p>"},{"location":"blog/2025/05/07/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#conclusion","title":"Conclusion","text":"<p>While <code>SQL</code> is a \"standard,\" its implementations across different RDBMSs like SQLite, MySQL, and PostgreSQL have distinct personalities.</p> <p>The journey of creating a consistent schema for my <code>EMS project</code> across these three was a great learning experience. Remember, always check the documentation for the specific dialect you're working with.</p> <p>I hope this comparative overview helps you in your database endeavors! Happy coding!</p> <p>Disclaimer</p> <p>The examples are drawn from specific project files and general knowledge. Always refer to the official documentation for the most comprehensive and up-to-date information.</p>"},{"location":"blog/2025/05/07/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#references-resources","title":"References &amp; Resources \ud83d\udd17","text":"<p>This section compiles useful links found within the <code>ems-db</code> project's documentation (<code>usage.md</code>, <code>README.Docker.md</code> files), categorized for easier navigation.</p>"},{"location":"blog/2025/05/07/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#general","title":"General","text":"<p>CS50 SQL Notes (General Syntax Differences):</p> <ul> <li>MySQL Differences</li> <li>PostgreSQL Differences</li> </ul>"},{"location":"blog/2025/05/07/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#sqlite","title":"SQLite","text":"<ul> <li>SQL As Understood By SQLite</li> <li>Python <code>sqlite3</code> Module</li> </ul>"},{"location":"blog/2025/05/07/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#postgresql","title":"PostgreSQL","text":"<ul> <li>Postgres SQL Commands</li> <li>Postgres Data Types</li> <li>Postgres Date and Time Functions</li> <li>Postgres Triggers and Trigger Functions</li> </ul>"},{"location":"blog/2025/05/07/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#mysql","title":"MySQL","text":"<ul> <li>Using Data Types from Other Database Engines</li> <li>Date and Time Functions</li> <li>Data Definition Statements</li> <li>Docker MySQL Basic Steps</li> </ul>"},{"location":"blog/2025/05/07/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/","title":"Beyond the Schema: A Practical Guide to Querying and Interacting with SQLite, MySQL, &amp; PostgreSQL","text":"<p>Okay, building on our previous discussion about SQL schema differences, let's dive into how we interact with <code>SQLite</code>, <code>MySQL</code>, and <code>PostgreSQL</code>, focusing on query execution, CLI usage, and connection methods.</p> <p>This companion blog post will use the <code>queries.sql</code>, <code>README.Docker.md</code>, and <code>usage.md</code> files from the Exam Management System (EMS) project as our practical examples.</p> <ul> <li>Project Repo : ems-db <code>&lt;-- root-dir-name</code></li> </ul> <p>If you missed the first part on schema definitions, you can catch up here:</p> <ul> <li>Navigating the Nuances: A Developer's Guide to SQL Dialects (SQLite, MySQL, PostgreSQL).</li> </ul> <p>This post will serve as another handy reference, highlighting the practical differences you'll encounter when running queries and managing these databases, especially useful for both day-to-day development and for showcasing practical database skills.</p> <p>Drill</p> <p><code>\u26a0\ufe0fWarning</code> : Refer to <code>Official Docs</code>, when in doubt. <code>\"Its not ultimate source of truth. It could be good starting point.\"</code></p> <ul> <li>Understanding : Use this project as reference.</li> <li>Prerequisites : Familiar with <code>sql syntax</code>, <code>client interaction</code>, <code>Docker</code>, and <code>Python</code> (language of choice).</li> </ul> <p>In our previous post, we explored the key differences in schema definitions across <code>SQLite</code>, <code>MySQL</code>, and <code>PostgreSQL</code> using the Exam Management System (EMS) project as a case study.</p> <p>Now, let's shift our focus to the equally important aspects of how we interact with these databases: running queries, using their command-line interfaces (CLIs), and understanding connection nuances, especially in a Dockerized environment.</p> <p><code>\u2139\ufe0fNote</code> : This guide draws insights from the following project files (within the ems-db repository):</p> <ul> <li> <p>Query Scripts:</p> <ul> <li>sqlite/queries.sql</li> <li>psql/queries.sql</li> <li>mysql/queries.sql</li> </ul> </li> <li> <p>Usage &amp; Docker Documentation:</p> <ul> <li>sqlite/usage.md &amp; sqlite/README.Docker.md</li> <li>psql/usage.md &amp; psql/README.Docker.md</li> <li>mysql/usage.md &amp; mysql/README.Docker.md</li> </ul> </li> </ul> <p>Understanding these practical differences can significantly boost your efficiency and adaptability as a developer.</p>"},{"location":"blog/2025/05/07/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#key-areas-of-difference-queries-interaction","title":"Key Areas of Difference: Queries &amp; Interaction","text":""},{"location":"blog/2025/05/07/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#1-cli-shell-access-connection","title":"1. CLI Shell Access &amp; Connection","text":"<p>Each database has its own command-line tool for direct interaction.</p> <ul> <li> <p>SQLite:</p> <ul> <li>Command: <code>sqlite3 ems.db [options]</code></li> <li>Example from <code>sqlite/usage.md</code>: <code>sqlite3 ems.db -table -echo</code><ul> <li><code>-table</code>: Sets output mode to table format.</li> <li><code>-echo</code>: Prints commands before execution.</li> </ul> </li> <li>Connection is file-based; you specify the database file path.</li> </ul> </li> <li> <p>PostgreSQL:</p> <ul> <li>Command: <code>psql [options] [dbname] [username]</code></li> <li>Example from <code>psql/usage.md</code>: <code>psql -a -b ems postgres</code><ul> <li><code>-a</code>: Echoes all input from script.</li> <li><code>-b</code>: Echoes failed commands.</li> <li><code>ems</code>: Database name.</li> <li><code>postgres</code>: Username.</li> </ul> </li> <li>In Docker, from an <code>app</code> service: <code>psql -h db ems postgres</code> (where <code>db</code> is the service name of the PostgreSQL container).</li> <li>PostgreSQL's <code>README.Docker.md</code> also mentions using <code>~/.pgpass</code> for passwordless connections in development environments.</li> </ul> </li> <li> <p>MySQL:</p> <ul> <li>Command: <code>mysql [options] -u[user] -p[password] [dbname]</code></li> <li>Example from <code>mysql/usage.md</code>: <code>mysql -t -v -uroot -psecret ems</code><ul> <li><code>-t</code>: Output in table format.</li> <li><code>-v</code>: Verbose mode.</li> <li><code>-uroot -psecret</code>: Username and password.</li> <li><code>ems</code>: Database name (as defined in <code>compose.yml</code> <code>MYSQL_DATABASE</code> env var).</li> </ul> </li> <li>MySQL's <code>README.Docker.md</code> also mentions <code>mysqlsh</code> as a more powerful alternative shell, aliased as <code>mysql</code> in the <code>app</code> service. Often used for connecting with <code>cloud native mysql server</code> from <code>client machines</code>.</li> </ul> </li> </ul> <p>\ud83d\udd0d Tip: All three databases differ significantly in how they let you inspect objects (like tables, views, indexes) from shell clients\u2014see section 5 and beyond.</p>"},{"location":"blog/2025/05/07/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#2-executing-sql-scripts-from-files","title":"2. Executing SQL Scripts from Files","text":"<p>Running a series of SQL commands from a <code>.sql</code> file is a common task.</p> <ul> <li>SQLite:<ul> <li>Shell command: <code>.read ./queries.sql</code></li> <li>CLI redirection: <code>sqlite3 ems.db -table -echo &lt; ./queries.sql</code></li> </ul> </li> </ul> <p>As seen in <code>sqlite/usage.md</code> and <code>sqlite/queries.sql</code></p> <ul> <li>PostgreSQL:<ul> <li>Shell command: <code>\\i ./queries.sql</code></li> <li>CLI redirection: <code>psql -a -b ems postgres &lt; ./queries.sql</code></li> </ul> </li> </ul> <p>As seen in <code>psql/usage.md</code> and <code>psql/queries.sql</code></p> <ul> <li>MySQL:<ul> <li>Shell command: <code>source ./queries.sql</code></li> <li>CLI redirection: <code>mysql -tv -uroot -psecret ems &lt; ./queries.sql</code></li> </ul> </li> </ul> <p>\u2728 Important: When schema files contain stored procedures, triggers, or functions that require <code>DELIMITER</code>, executing them inside the <code>mysql</code> CLI is more reliable than using <code>mysql-connector-python</code> (which doesn't support <code>DELIMITER</code>). This limitation makes shell execution the preferred approach for complex DDL.</p> <p>As seen in <code>mysql/usage.md</code> and <code>mysql/queries.sql</code></p>"},{"location":"blog/2025/05/07/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#3-resetting-auto-increment-values","title":"3. Resetting Auto-Increment Values","text":"<p>After clearing tables (e.g., with <code>DELETE FROM table;</code>), you often want to reset auto-increment counters for primary keys, especially during development or testing.</p> <ul> <li> <p>SQLite:</p> <ul> <li>If <code>AUTOINCREMENT</code> keyword is used on an <code>INTEGER PRIMARY KEY</code> column, SQLite uses an internal table <code>sqlite_sequence</code>.<ul> <li>To reset: <code>DELETE FROM sqlite_sequence WHERE name='your_table_name';</code></li> </ul> </li> <li>The <code>sqlite/queries.sql</code> file simply uses <code>DELETE FROM students;</code>. If <code>AUTOINCREMENT</code> was not explicitly used (as in the <code>students</code> table in the provided schema), SQLite might reuse IDs from deleted rows. For a true reset, the <code>sqlite_sequence</code> table would need to be managed if <code>AUTOINCREMENT</code> was present.</li> </ul> </li> <li> <p>PostgreSQL:</p> <ul> <li>Uses sequences. The <code>SERIAL</code> type automatically creates a sequence suffixed with <code>_id_seq</code>.</li> <li>Command from <code>psql/queries.sql</code>: <code>ALTER SEQUENCE students_id_seq RESTART WITH 1;</code></li> </ul> </li> <li> <p>MySQL:</p> <ul> <li>Command from <code>mysql/queries.sql</code>: <code>ALTER TABLE students AUTO_INCREMENT = 1;</code></li> </ul> </li> </ul>"},{"location":"blog/2025/05/07/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#4-data-manipulation-language-dml-snippets","title":"4. Data Manipulation Language (DML) Snippets","text":"<p>The basic syntax for <code>INSERT</code>, <code>UPDATE</code>, and <code>DELETE</code> is highly standardized. The <code>queries.sql</code> files for all three databases demonstrate this:</p> <ul> <li> <p>INSERT:</p> <pre><code>-- Common across SQLite, PostgreSQL, MySQL (quoting may vary per schema)\nINSERT INTO students (first_name, last_name, password, email)\nVALUES ('John', 'Doe', 'password123', 'john.doe@example.com');\n</code></pre> </li> <li> <p>UPDATE:</p> <pre><code>-- Common across SQLite, PostgreSQL, MySQL (quoting may vary per schema)\nUPDATE tests_sessions\nSET status = 'completed'\nWHERE id = 1;\n</code></pre> </li> <li> <p>DELETE (Clearing a table):</p> <pre><code>-- Common across SQLite, PostgreSQL, MySQL\nDELETE FROM reports;\n</code></pre> </li> </ul> <p>Identifier quoting : <code>Double quotes for SQLite/PostgreSQL</code>, <code>backticks for MySQL</code> discussed in the schema blog post also applies here.</p>"},{"location":"blog/2025/05/07/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#5-querying-data-analysis-select-explain","title":"5. Querying Data &amp; Analysis (SELECT, EXPLAIN)","text":"<p>Standard <code>SELECT</code> statements with <code>JOINs</code>, <code>WHERE</code> clauses, and subqueries are largely portable.</p> <ul> <li> <p>Example SELECT (from <code>sqlite/queries.sql</code>, similar in others):</p> <pre><code>SELECT *\nFROM tests_history\nWHERE student_id = (\n    SELECT id\n    FROM students\n    WHERE email = 'john.doe@example.com'\n);\n</code></pre> </li> <li> <p>Inspecting Tables/Views/Indexes (within shell clients):</p> <ul> <li> <p>SQLite:</p> <ul> <li>List tables: <code>.tables</code></li> <li>View DDL: <code>.schema table_name</code> or full schema: <code>.fullschema</code></li> <li>List indexes: <code>SELECT name FROM sqlite_master WHERE type='index';</code></li> </ul> </li> <li> <p>PostgreSQL:</p> <ul> <li>List tables: <code>\\dt</code></li> <li>View indexes: <code>\\di</code></li> <li>View DDL: <code>\\d+ table_name</code> or for indexes/views: <code>\\d+ index_name</code>, <code>\\d+ view_name</code></li> <li> <p>SQL alternatives:</p> <pre><code>SELECT * FROM information_schema.tables WHERE table_schema='public';\nSELECT * FROM pg_indexes WHERE schemaname = 'public';\n</code></pre> </li> </ul> </li> <li> <p>MySQL:</p> <ul> <li>List tables: <code>SHOW TABLES;</code></li> <li>List indexes: <code>SHOW INDEX FROM table_name;</code></li> <li> <p>List views:</p> <pre><code>SHOW FULL TABLES WHERE TABLE_TYPE = 'VIEW';\nSHOW CREATE VIEW view_name;\n</code></pre> </li> <li> <p>SQL alternatives via <code>information_schema</code>:</p> <pre><code>SELECT * FROM information_schema.tables WHERE table_schema = 'your_db';\nSELECT * FROM information_schema.statistics WHERE table_schema = 'your_db';\n</code></pre> </li> </ul> </li> </ul> </li> <li> <p>Query Plan Analysis:</p> <ul> <li>SQLite: <code>EXPLAIN QUERY PLAN SELECT ...;</code> (as used in comments in <code>sqlite/queries.sql</code> <code>#Line-151 or below</code>).</li> <li>PostgreSQL &amp; MySQL: <code>EXPLAIN SELECT ...;</code> (This is the standard command, though not explicitly run in the provided <code>queries.sql</code> files for PSQL/MySQL, it's the common way to analyze queries for better indexing and performance optimizations).</li> </ul> </li> </ul>"},{"location":"blog/2025/05/07/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#6-dialect-specific-functionscommands-in-queries","title":"6. Dialect-Specific Functions/Commands in Queries","text":"<p>While core SQL is similar, some functions or commands are unique.</p> <ul> <li>SQLite: Uses functions like <code>STRFTIME()</code> and <code>DATETIME()</code> for date/time manipulations (more prominent in its <code>schema.sql</code>) triggers.</li> <li>PostgreSQL: Rich set of functions; <code>INTERVAL</code> arithmetic is a key feature (seen in its <code>schema.sql</code> triggers). The <code>queries.sql</code> uses standard SQL.</li> <li>MySQL:<ul> <li><code>SLEEP(seconds)</code>: Used in <code>mysql/queries.sql</code> (<code>SELECT SLEEP(3);</code>) to pause execution, often for testing or simulation.</li> <li><code>TIMEDIFF()</code>: Used in its <code>schema.sql</code> trigger.</li> </ul> </li> </ul>"},{"location":"blog/2025/05/07/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#7-error-handling-diagnostics-clisql","title":"7. Error Handling &amp; Diagnostics (CLI/SQL)","text":"<ul> <li> <p>SQLite:</p> <ul> <li>CLI: <code>-echo</code> flag helps trace execution.</li> </ul> </li> <li> <p>PostgreSQL:</p> <ul> <li>CLI: <code>-a</code> (echo all) and <code>-b</code> (echo errors) flags.</li> <li>Shell: <code>\\set ON_ERROR_STOP on</code> can be useful in scripts.</li> </ul> </li> <li> <p>MySQL:</p> <ul> <li>SQL Commands: <code>SHOW ERRORS;</code> and <code>SHOW WARNINGS;</code> are explicitly used in <code>mysql/queries.sql</code> to check for issues after operations.</li> <li>CLI: <code>-v</code> (verbose) flag.</li> </ul> </li> </ul>"},{"location":"blog/2025/05/07/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#8-exiting-shells","title":"8. Exiting Shells","text":"<ul> <li>SQLite: <code>.exit</code> or <code>.quit</code></li> <li>PostgreSQL: <code>\\q</code> or <code>exit</code></li> <li>MySQL: <code>\\q</code> or <code>exit</code> (or <code>quit</code>)</li> </ul>"},{"location":"blog/2025/05/07/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#9-python-script-interaction-brief","title":"9. Python Script Interaction (Brief)","text":"<p>The <code>usage.md</code> files for each database mention running a <code>db.py</code> script (e.g., <code>uv run db.py</code>). While this post focuses on CLI interaction, it's important to note that these databases are typically accessed programmatically via Python using libraries:</p> <ul> <li>SQLite: <code>sqlite3</code> (standard library)</li> <li>PostgreSQL: <code>psycopg2</code> or <code>psycopg</code> (third-party)</li> <li>MySQL: <code>mysql-connector-python</code> or <code>PyMySQL</code> (third-party) These libraries handle connection and query execution, abstracting some dialect specifics but still requiring correct SQL syntax for the target database.</li> </ul> <p>MySQL Note: As mentioned earlier, MySQL's <code>mysql-connector-python</code> does not support the <code>DELIMITER</code> command needed for procedures/triggers. This makes CLI-based execution of <code>schema.sql</code> safer and more portable.</p>"},{"location":"blog/2025/05/07/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#10-docker-environment-nuances","title":"10. Docker Environment Nuances","text":"<p>The <code>README.Docker.md</code> files highlight how Docker simplifies setup and interaction:</p> <ul> <li>Universal Access: <code>docker compose exec [service_name] bash</code> provides a shell within the container, from which you can then launch the respective database CLI.<ul> <li>SQLite: <code>docker compose exec app bash</code> then <code>sqlite3 ems.db</code></li> <li>PostgreSQL: <code>docker compose exec db bash</code> then <code>psql ...</code> or <code>docker compose exec app bash</code> then <code>psql -h db ...</code></li> <li>MySQL: <code>docker compose exec db bash</code> then <code>mysql ...</code> or <code>docker compose exec app bash</code> then <code>mysql ...</code> (often <code>mysqlsh</code> aliased as <code>mysql</code>).</li> <li>Service Discovery: For PostgreSQL and MySQL, the database often runs in a service named <code>db</code> (as defined in <code>compose.yml</code>), accessible from an <code>app</code> service using this hostname (e.g., <code>psql -h db ...</code>).</li> </ul> </li> <li>Pre-configured Environments: Docker setups often pre-configure users, passwords, and databases (e.g., <code>MYSQL_ROOT_PASSWORD</code>, <code>POSTGRES_USER</code>, <code>POSTGRES_DB</code> environment variables in <code>compose.yml</code>).</li> <li>SQL Dump Mounting: <code>README.Docker.md</code> for PostgreSQL and MySQL mentions mounting SQL dumps for pre-seeding data, which automates schema creation and initial data insertion on container startup.</li> <li>GUI Tools:<ul> <li>PostgreSQL: <code>adminer</code> service often included for web-based DB management.</li> <li>MySQL: <code>phpmyadmin</code> service often included.</li> </ul> </li> </ul>"},{"location":"blog/2025/05/07/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#quick-cheatsheet-query-interaction","title":"Quick Cheatsheet: Query &amp; Interaction","text":"Feature SQLite PostgreSQL MySQL CLI Tool <code>sqlite3</code> <code>psql</code> <code>mysql</code>, <code>mysqlsh</code>(for client) Connect (Example) <code>sqlite3 ems.db</code> <code>psql -U user -d dbname</code> <code>mysql -u user -p pass dbname</code> Run SQL File (Shell) <code>.read file.sql</code> <code>\\i file.sql</code> <code>source file.sql</code> Run SQL File (CLI) <code>sqlite3 db &lt; file.sql</code> <code>psql ... &lt; file.sql</code> <code>mysql ... &lt; file.sql</code> Reset Auto-Increment <code>DELETE FROM sqlite_sequence WHERE name='tbl';</code> (if <code>AUTOINCREMENT</code> used) <code>ALTER SEQUENCE seq_name RESTART WITH 1;</code> <code>ALTER TABLE tbl AUTO_INCREMENT = 1;</code> Query Plan <code>EXPLAIN QUERY PLAN ...</code> <code>EXPLAIN ...</code> <code>EXPLAIN ...</code> Show Errors (SQL) N/A (check return codes/messages) N/A (check messages, <code>ON_ERROR_STOP</code>) <code>SHOW ERRORS;</code>, <code>SHOW WARNINGS;</code> Exit Shell <code>.exit</code>, <code>.quit</code> <code>\\q</code>, <code>exit</code> <code>\\q</code>, <code>exit</code>, <code>quit</code> Docker Exec (App) <code>docker compose exec app sqlite3 ...</code> <code>docker compose exec app psql -h db ...</code> <code>docker compose exec app mysql -h db ...</code> Docker Exec (DB) <code>docker compose exec db sqlite3 ...</code> <code>docker compose exec db psql ...</code> <code>docker compose exec db mysql ...</code> View Tables (Shell) <code>.tables</code> <code>\\dt</code> <code>SHOW TABLES;</code> View Indexes (Shell) Query <code>sqlite_master</code> <code>\\di</code> <code>SHOW INDEX FROM tbl;</code>"},{"location":"blog/2025/05/07/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#why-this-matters","title":"Why This Matters","text":"<p>For Developers:</p> <ul> <li>Efficiency: Knowing the right CLI commands, flags, and shell directives saves significant time during development and debugging.</li> <li>Scripting &amp; Automation: Understanding how to execute SQL files and manage database states (like resetting sequences) is crucial for automated testing and deployment.</li> <li>Tooling: Familiarity with Docker interaction patterns and GUI tools enhances the development workflow.</li> <li>Debugging: Using <code>EXPLAIN</code> and error-checking commands helps optimize queries and troubleshoot issues effectively.</li> </ul> <p>Learning:</p> <ul> <li>Practical Skills: These interaction nuances demonstrate hands-on experience beyond theoretical SQL knowledge.</li> <li>Versatility: Comfort with different database CLIs and Docker environments indicates adaptability.</li> <li>Problem-Solving: The ability to diagnose query performance or script execution issues points to strong troubleshooting skills.</li> </ul>"},{"location":"blog/2025/05/07/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#conclusion","title":"Conclusion","text":"<p>Mastering the art of SQL goes beyond writing <code>SELECT</code> statements. It encompasses how you connect to your database, execute scripts, analyze performance, and manage its state through various tools and environments. As demonstrated by the <code>Exam Management System (ems-db)</code> project's supporting files, each database system\u2014SQLite, MySQL, and PostgreSQL\u2014offers a slightly different, yet powerful, set of tools and commands for these tasks.</p> <p>By familiarizing yourself with these practical aspects, you become a more well-rounded and effective data professional.</p> <p>Happy querying!</p> <p>Disclaimer</p> <p>The examples are drawn from specific project files and general knowledge. Always refer to the official documentation for the most comprehensive and up-to-date information.</p>"},{"location":"blog/2025/05/07/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#references-resources","title":"References &amp; Resources \ud83d\udd17","text":"<p>This section compiles useful links found within the <code>ems-db</code> project's documentation (<code>usage.md</code>, <code>README.Docker.md</code> files), categorized for easier navigation.</p>"},{"location":"blog/2025/05/07/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#general","title":"General","text":"<ul> <li>Docker:<ul> <li>Docker Manuals</li> <li>Docker Cheatsheet</li> <li>Docker's Python guide</li> <li>Docker Quick workshop</li> <li>Compose Getting Started</li> <li>Compose Volumes</li> <li>Use Compose Watch</li> </ul> </li> <li>UV (Python Packager):<ul> <li>UV: Working on projects</li> <li>UV: Docker Integration</li> </ul> </li> <li>CS50 SQL Notes (General Syntax Differences):<ul> <li>MySQL Differences</li> <li>PostgreSQL Differences</li> </ul> </li> </ul>"},{"location":"blog/2025/05/07/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#sqlite","title":"SQLite","text":"<ul> <li>SQLite CLI Commands</li> <li>SQL As Understood By SQLite</li> <li>Python <code>sqlite3</code> Module</li> <li>SQLite Download Page</li> </ul>"},{"location":"blog/2025/05/07/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#postgresql","title":"PostgreSQL","text":"<ul> <li>Client &amp; Server Documentation:<ul> <li>Postgres Client Reference</li> <li>Postgres SQL Commands</li> <li>Postgres Date and Time Functions</li> <li>Postgres Environment Variables</li> <li>Postgres Connection Strings</li> <li>Postgres Passwords File (<code>.pgpass</code>)</li> </ul> </li> <li>Python Driver: psycopg3 Documentation</li> <li>Docker Hub: Postgres Image</li> <li>Postgres Download Page</li> </ul>"},{"location":"blog/2025/05/07/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#mysql","title":"MySQL","text":"<ul> <li>MySQL Documentation:<ul> <li>MySQL SHOW Commands</li> <li>Date and Time Functions</li> <li>Docker MySQL Deployment Topics</li> <li>mysqlsh Shell Startup</li> </ul> </li> <li>Python Driver: mysql-connector-python</li> <li>Docker Hub: MySQL Image</li> <li>MySQL Server Installation</li> <li>MySQL Client/Shell Installation</li> </ul>"},{"location":"blog/2025/05/07/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#gui-tools-mentioned-in-docker-setups","title":"GUI Tools (mentioned in Docker setups)","text":"<ul> <li>Adminer (for PostgreSQL &amp; others)</li> <li>phpMyAdmin (for MySQL)</li> </ul>"},{"location":"projects/","title":"Projects","text":"<p>Welcome to my project portfolio! Here you'll find a curated selection of my best work, spanning data engineering, automation, web development, and more. Explore featured highlights or browse the full list below.</p>"},{"location":"projects/#featured-projects","title":"\ud83d\ude80 Featured Projects","text":"<ul> <li> <p> GSTN Hackathon: Predictive Binary Classification</p> <p>Robust, interpretable ML pipeline for binary classification on anonymized GSTN data. Achieved &gt;97% accuracy, strong F1/MCC, and strict compliance with competition rules. Python, scikit-learn, XGBoost/LightGBM, SHAP, and reproducibility scripts.</p> <p> Read More</p> </li> <li> <p> Examination Management System DB</p> <p>Robust, production-ready database system for managing exams, students, proctoring, and results. Multi-RDBMS support (SQLite, MySQL, PostgreSQL), Python automation, Dockerized environments, and automated testing.</p> <p> Read More</p> </li> <li> <p> S3 Faker</p> <p>Fake data generator with AWS S3 (LocalStack) integration. Generates large datasets using Python &amp; Faker, supports CSV/JSON/Parquet, and automates uploads for testing cloud pipelines.</p> <p> Read More</p> </li> <li> <p> Paraxcel</p> <p>Python toolkit for advanced Excel data extraction, transformation, and visualization. Built with Pandas, Openpyxl, Matplotlib, and Seaborn for seamless spreadsheet analytics.</p> <p> Read More</p> </li> <li> <p> Naukri Webscraper</p> <p>Selenium-powered Python tool to automate job search and data extraction from Naukri.com. Features skill-based filtering, CSV export, and robust automated testing with pytest.</p> <p> Read More</p> </li> <li> <p> Test Management Site</p> <p>Dynamic, responsive web app for test creation, execution, and result tracking. Built with vanilla JS, HTML, CSS, Bootstrap, and localStorage for a seamless frontend experience.</p> <p> Read More</p> </li> </ul>"},{"location":"projects/#all-projects","title":"\ud83d\udcda All Projects","text":"LatestA\u2013Z <ul> <li> <p> Examination Management System DB Multi-RDBMS exam/test management, automation, Docker, Python, CI-ready.</p> </li> <li> <p> S3 Faker Fake data generator with S3/LocalStack integration for cloud testing.</p> </li> <li> <p> Paraxcel Excel data extraction, transformation, and visualization toolkit.</p> </li> <li> <p> Naukri Webscraper Automated job scraping, filtering, and CSV export from Naukri.com.</p> </li> <li> <p> Test Management Site Frontend web app for test management and result tracking.</p> </li> <li> <p> GSTN Hackathon: Predictive Binary Classification Robust, interpretable ML pipeline for binary classification on anonymized GSTN data. &gt;97% accuracy, strong F1/MCC, reproducibility, and compliance.</p> </li> </ul> <ol> <li> Examination Management System DB</li> <li> GSTN Hackathon: Predictive Binary Classification</li> <li> Naukri Webscraper</li> <li> Paraxcel</li> <li> S3 Faker</li> <li> Test Management Site</li> </ol>"},{"location":"projects/#why-these-projects","title":"\ud83c\udf1f Why These Projects?","text":"<p>Each featured project demonstrates a unique blend of technical depth, problem-solving, and real-world impact\u2014from scalable database design and cloud automation to advanced data analytics and modern web development. Explore the detailed write-ups for code samples, visuals, and outcomes.</p>"},{"location":"projects/ems-db/","title":"Examination Management System Database","text":"In-Hurry Summary <p>Examination Management System Database A database designed to manage tests and examinations, covering student information, test details, questions, proctoring, and results.</p> <ul> <li>Context: <code>Personal Project</code>, <code>Apr 2024</code>, <code>SQLite</code>, <code>MySQL</code>, <code>PostgreSQL</code>, <code>DB Design</code></li> <li>Role: Sole Database Designer and Implementer</li> <li>Impact: Enabled efficient test administration and monitoring by creating a structured database, facilitating quick data retrieval and reporting.</li> </ul>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#overview","title":"Overview","text":"<p>The Examination Management System Database project aimed to design and implement a robust database for managing tests and examinations. It covered key aspects such as student details, tests, questions, test sessions, proctors, and results. The project was initially implemented in SQLite, and later expanded into three variants: SQLite, MySQL, and PostgreSQL, each in its own directory with a consistent file structure.</p> <p>Recent updates:</p> <ul> <li>Project restructured into three dedicated directories: sqlite, mysql, and psql, each with its own schema, queries, and scripts.  </li> <li>Added Docker-based development environments for each variant (see each directory's <code>compose.yaml</code>).  </li> <li>Introduced Python automation scripts (<code>db.py</code>) and pytest-based test suites for all variants, using the appropriate Python database connectors.  </li> <li>Enhanced schema with advanced triggers, views, and indexes for performance and integrity.  </li> <li>Improved documentation and usage instructions (see <code>README.md</code>).</li> </ul>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#goals","title":"Goals","text":"<p>The primary objectives of the project were:</p> <ul> <li>To create a robust, modular database schema to support all core processes of test administration.</li> <li>To enable efficient monitoring and analysis of test sessions, including proctoring and event auditing.</li> <li>To facilitate easy generation of reports summarizing test outcomes and student performance.</li> <li>To ensure extensibility and maintainability across multiple RDBMS backends.</li> </ul>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#responsibilities","title":"Responsibilities","text":"<p>As the sole database designer and implementer, my responsibilities included:</p> <ul> <li>Designing the database schema for SQLite, MySQL, and PostgreSQL.</li> <li>Implementing tables, relationships, triggers, views, and indexes for each variant.</li> <li>Developing and maintaining Python scripts (<code>db.py</code>) for database automation and management.</li> <li>Creating and running automated tests using pytest for all database variants.</li> <li>Setting up Docker-based development environments for consistent local and CI/CD workflows.</li> <li>Writing and maintaining comprehensive documentation.</li> </ul>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#technologies-used","title":"Technologies Used","text":"<ul> <li>Languages: SQL, Python</li> <li>Databases: SQLite, MySQL, PostgreSQL</li> <li>Python Connectors: <ul> <li><code>sqlite3</code> (for SQLite)  </li> <li><code>mysql-connector-python</code> (for MySQL)  </li> <li><code>psycopg2</code> (for PostgreSQL)</li> </ul> </li> <li>Project Management: <ul> <li>Consistent directory structure for all variants  </li> <li><code>README.md</code>, <code>usage.md</code>, and <code>README.Docker.md</code> for each variant</li> </ul> </li> </ul> Tools <ul> <li>Testing: <code>pytest</code> (with <code>test_db.py</code> in each variant)</li> <li>Dependency Management: <code>uv</code> (for fast Python environment setup)</li> <li>Containerization: Docker, Docker Compose (with dedicated <code>Dockerfile</code> and <code>compose.yaml</code> in each variant)</li> <li>Shells/CLI: <ul> <li><code>sqlite3</code> CLI (for SQLite)  </li> <li><code>mysql</code>/<code>mysqlsh</code> CLI (for MySQL)  </li> <li><code>psql</code> CLI (for PostgreSQL)</li> </ul> </li> <li>Database GUIs (via Docker):<ul> <li><code>phpMyAdmin</code> (for MySQL)  </li> <li><code>adminer</code> (for PostgreSQL)</li> </ul> </li> <li>Reverse Proxy: Traefik (for local service routing in Docker)</li> <li>Documentation: Markdown, Mermaid (for ER diagrams)</li> </ul> <p>Note : All development and testing environments are containerized for consistency and reproducibility.</p>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#process","title":"Process","text":"<p>The project followed a structured approach:</p> <ol> <li>Conceptual Design: Identified entities and relationships required for the examination system.</li> <li>Logical Design: Translated the conceptual design into detailed schemas for each RDBMS.</li> <li>Physical Design: Implemented the schema, triggers, indexes, and views in each database.</li> <li>Testing: Inserted sample data and ran automated queries and tests to validate the design and performance.</li> <li>Optimization: Added indexes and views to improve query performance and usability.</li> <li>Automation: Developed Python-based scripts and test suites for all variants.</li> <li>Containerization: Provided Docker Compose files for reproducible development environments.</li> </ol>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#recognition","title":"Recognition","text":"<p>I am proud to share that I have successfully completed the <code>CS50 SQL - Introduction to Databases with SQL</code> course.</p>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#certificate","title":"Certificate","text":"","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#challenges-solutions","title":"Challenges &amp;  Solutions","text":"<ol> <li> <p>Multi-Database Support </p> <ul> <li> Ensuring consistent schema and logic across SQLite, MySQL, and PostgreSQL.  </li> <li> Modularized schema and queries, and used automated tests to validate behavior across all supported databases.</li> </ul> </li> <li> <p>Data Consistency and Integrity </p> <ul> <li> Maintaining data integrity with complex triggers and relationships.  </li> <li> Implemented advanced triggers and constraints in each variant, with automated testing for validation.</li> </ul> </li> <li> <p>Query Performance Optimization </p> <ul> <li> Optimizing query performance for complex reporting and history tracking.  </li> <li> Created targeted indexes and materialized complex logic into views for efficient access.</li> </ul> </li> <li> <p>Automation and Environment Consistency </p> <ul> <li> Ensuring reliable development and testing environments across platforms.  </li> <li> Used Docker Compose for each variant and automated CI/CD with GitHub Actions.</li> </ul> </li> </ol>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#achievements","title":"Achievements","text":"<ul> <li>Designed and implemented a comprehensive, production-ready database schema for managing an examination system in SQLite, MySQL, and PostgreSQL.</li> <li>Automated scoring, feedback, event logging, and report generation using advanced triggers.</li> <li>Simplified complex queries and reporting through reusable views.</li> <li>Improved query performance by adding strategic indexes.</li> <li>Achieved high test coverage for schema logic and data flows in all variants.</li> <li>Provided Docker-based environments for easy setup and reproducibility.</li> </ul>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#key-learnings","title":"Key Learnings","text":"<ul> <li>The importance of modular design for multi-database support.</li> <li>Effective use of triggers, indexes, and views for data integrity and performance.</li> <li>How to automate database testing and management with Python and Docker.</li> <li>The value of consistent documentation and directory structure for maintainability.</li> </ul>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#outcomes","title":"Outcomes","text":"<p>The database provides a structured and efficient way to manage tests and examinations. It supports CRUD operations, test creation, session monitoring, proctoring event auditing, and automated report generation. The use of triggers, views, and indexes significantly improved data integrity and query performance. Automated tests and Docker environments ensure ongoing reliability and ease of use across all supported databases.</p>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#visuals","title":"Visuals","text":"","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#er-diagram","title":"ER Diagram","text":"<p> screenshot of the DB for SQLite/MySQL/PostgreSQL showing the schema.</p>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#video-overview","title":"Video overview","text":"","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#links","title":"Links","text":"<p>GitHub Repository</p>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#read-studies-insights","title":"Read Studies &amp; Insights","text":"<ul> <li>Navigating the Nuances: A Developer's Guide to SQL Dialects (SQLite, MySQL, PostgreSQL).</li> <li>Beyond the Schema: A Practical Guide to Querying and Interacting with SQLite, MySQL, &amp; PostgreSQL</li> </ul>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#conclusion","title":"Conclusion","text":"<p>The Examination Management System Database project successfully delivered a robust and efficient solution for managing tests and examinations across multiple database platforms. It met the outlined goals and provided valuable insights into database design, automation, and optimization. The project demonstrated the importance of structured database design, modularity, and the effective use of triggers, views, indexes, and automated testing to enhance performance and maintain data integrity.</p> AI Skill Assessment <p>Prompt<sup>1</sup> Source </p> <ol> <li> <p>This <code>AI skill assessment</code> was generated based on the skill-assessment-prompt.md and the provided project documentation. It is intended as an illustrative summary and should be interpreted in the context of the available code and documentation in codebase.\u00a0\u21a9</p> </li> </ol>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#strengths","title":"Strengths","text":"<ul> <li> <p>Relational Database Design: </p> <ul> <li>Strong understanding of relational modeling, normalization, and entity relationships.</li> <li>Consistent schema design across SQLite, MySQL, and PostgreSQL, with appropriate use of constraints, foreign keys, and indexes.</li> <li>Advanced use of triggers for automation (e.g., scoring, feedback, session/event management).</li> </ul> </li> <li> <p>SQL Proficiency: </p> <ul> <li>Proficient in writing complex SQL queries, views, and batch scripts for all three RDBMS.</li> <li>Good use of views to abstract and simplify reporting and analytics.</li> </ul> </li> <li> <p>Python Automation &amp; Testing: </p> <ul> <li>Automated database setup and validation using Python (<code>db.py</code> scripts).</li> <li>Pytest-based test suites for each variant, using correct connectors (<code>sqlite3</code>, <code>mysql-connector-python</code>, <code>psycopg2</code>).</li> <li>Use of fixtures and in-memory databases for efficient, isolated testing.</li> </ul> </li> <li> <p>DevOps &amp; Environment Management: </p> <ul> <li>Docker and Docker Compose used for reproducible development environments for each database variant.</li> <li>Clear, modular directory structure and environment setup instructions.</li> <li>Use of uv for Python dependency management.</li> </ul> </li> <li> <p>Documentation: </p> <ul> <li>Well-structured Markdown documentation, usage guides, and ER diagrams.</li> <li>Clear separation of concerns and instructions for each database backend.</li> </ul> </li> </ul>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#areas-for-growth","title":"Areas for Growth","text":"<ul> <li> <p>CI/CD Integration: </p> <ul> <li>No current implementation of automated CI/CD pipelines (e.g., GitHub Actions, GitLab CI).</li> <li>Adding automated build/test on push would further professionalize the workflow.</li> </ul> </li> <li> <p>GUI/UX Tools: </p> <ul> <li>No use of GUI database tools (e.g., DB Browser for SQLite) in workflow; all interactions are CLI or script-based.</li> <li>Could consider adding optional GUI instructions for broader accessibility.</li> </ul> </li> <li> <p>Security &amp; Advanced Features: </p> <ul> <li>No implementation of advanced security (role-based access, encryption, etc.).</li> <li>No support for subjective question types or broader educational/administrative features.</li> </ul> </li> <li> <p>Scalability &amp; Production Readiness: </p> <ul> <li>Focus is on schema and logic, not on production deployment, backup, or scaling strategies.</li> </ul> </li> </ul>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#role-suitability","title":"Role Suitability","text":"","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#best-fit-roles","title":"Best Fit Roles","text":"<ul> <li>Database Engineer / Database Developer</li> <li>Backend Developer (with strong SQL/database focus)</li> <li>DevOps Engineer (entry to mid-level, especially for DB environments)</li> <li>QA Automation Engineer (for database systems)</li> </ul>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#well-suited-for","title":"Well-Suited For","text":"<ul> <li>Projects requiring robust relational schema design and automation.</li> <li>Teams needing multi-database support and migration-ready code.</li> <li>Environments where automated testing and reproducible dev setups are valued.</li> </ul>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#less-suited-for","title":"Less Suited For","text":"<ul> <li>Frontend/UI/UX-heavy roles.</li> <li>Roles requiring deep experience in cloud-native, distributed, or NoSQL systems.</li> <li>Security-focused or enterprise-scale production DB admin roles (without further experience).</li> </ul> <p>Summary: You demonstrate strong skills in relational database design, SQL, Python automation, and environment management. You are well-suited for roles focused on database engineering, backend development, and DevOps for database-driven projects. Expanding into CI/CD, security, and production operations would further broaden your profile.</p>","tags":["Database Design","Database Management"]},{"location":"projects/gstn-pbc/","title":"GSTN Hackathon: Predictive Binary Classification Project","text":"Quick Summary <p>GSTN Hackathon: Predictive Binary Classification Developed a robust, interpretable machine learning pipeline for binary classification on anonymized GSTN data, focusing on model performance, compliance, and reproducibility.</p> <ul> <li>Context: <code>Sep 2024</code>, <code>Python</code>, <code>scikit-learn</code>, <code>XGBoost/LightGBM</code>, <code>pandas</code>, <code>matplotlib</code>, <code>Jupyter</code>, solo project for GSTN Hackathon.</li> <li>Role: Sole developer\u2014handled all phases: EDA, feature engineering, model selection, hyperparameter tuning, evaluation, and documentation.</li> <li>Impact: Achieved &gt;97% accuracy and strong F1/MCC on imbalanced GSTN data by designing a reproducible, competition-compliant pipeline with anonymized model/code and no data leakage.</li> </ul>","tags":["Machine Learning","Binary Classification","Python","Data Science","Hackathon"]},{"location":"projects/gstn-pbc/#overview","title":"Overview","text":"<p>This project was developed for the GSTN Hackathon, aiming to build a high-performance, interpretable binary classification model on a large, anonymized GSTN dataset. The solution strictly adheres to competition rules: all model names and parameters are anonymized/randomized, and no original or derivative datasets are included in the repository.</p> <p>The workflow covers the full ML lifecycle: data integrity checks, EDA, preprocessing, feature engineering, model selection (tree-based models), hyperparameter tuning (GridSearchCV, Bayesian Optimization), cross-validation, threshold optimization, and comprehensive evaluation/reporting.</p> <p>Recent updates:</p> <ul> <li>Final model and code anonymized for submission.</li> <li>All data files excluded for compliance.</li> <li>Extensive documentation and reproducibility scripts added.</li> </ul> About the GSTN Hackathon <p>The Government of India hosted an AI/ML-based Hackathon to develop predictive models for GST analytics. This initiative was designed to engage students, researchers, and professionals in designing innovative AI/ML solutions using a large, anonymized dataset of approximately 900,000 records with 21 attributes.</p>","tags":["Machine Learning","Binary Classification","Python","Data Science","Hackathon"]},{"location":"projects/gstn-pbc/#key-details","title":"Key Details","text":"<ul> <li>Participation: Open to Indian students, researchers, and professionals from academia, startups, or companies.</li> <li>Registration: Required sign-up via Janparichay and submission of participant details.</li> <li>Teams: Up to five members per team, with one team lead required. Solo participation was allowed.</li> </ul>","tags":["Machine Learning","Binary Classification","Python","Data Science","Hackathon"]},{"location":"projects/gstn-pbc/#hackathon-structure","title":"Hackathon Structure","text":"","tags":["Machine Learning","Binary Classification","Python","Data Science","Hackathon"]},{"location":"projects/gstn-pbc/#online-phase","title":"Online Phase","text":"<ul> <li>Dataset Access: Participants received anonymized, labeled datasets for training and testing.</li> <li>Problem Statement: Given:</li> <li><code>Dtrain</code>: Training data matrix (m \u00d7 n)</li> <li><code>Dtest</code>: Test data matrix (m\u2081 \u00d7 n)</li> <li><code>Ytrain</code>: Target variable for training (m \u00d7 1)</li> <li><code>Ytest</code>: Target variable for testing (m\u2081 \u00d7 1)</li> <li>Objective: Construct a predictive model F\u03b8(X) \u2192 Ypred that accurately estimates the target variable Y for new, unseen inputs X.</li> </ul>","tags":["Machine Learning","Binary Classification","Python","Data Science","Hackathon"]},{"location":"projects/gstn-pbc/#workflow-steps","title":"Workflow Steps","text":"<ol> <li>Model Construction: Define a predictive function F\u03b8(X) parameterized by \u03b8 to map input features X to predicted outputs Ypred, capturing the relationship between features and target variable.</li> <li>Training: Optimize model parameters \u03b8 by minimizing a loss function L(Y, F\u03b8(X)) using Dtrain. Feature engineering and selection were encouraged to enhance performance.</li> <li>Testing: Apply the optimized model F\u03b8*(X) to Dtest to generate predictions Ypred for each input Xj.</li> <li>Performance Optimization: Evaluate model performance using metrics such as accuracy, precision, recall, F1 score, and AUC-ROC. Refine the model iteratively to improve these metrics.</li> <li>Submission: Submit predicted outputs Ypred_test along with:</li> <li>Source code (preferably on GitHub)</li> <li>A detailed report (modeling approach, code comments, citations, evaluation metrics, and key performance indicators)</li> </ol>","tags":["Machine Learning","Binary Classification","Python","Data Science","Hackathon"]},{"location":"projects/gstn-pbc/#offline-phase","title":"Offline Phase","text":"<ul> <li>Shortlisting: Top 9\u201315 teams were shortlisted based on initial evaluation.</li> <li>Final Round: Shortlisted teams refined their models using additional data and SME feedback, then presented their solutions and participated in interviews with the jury in Delhi.</li> </ul>","tags":["Machine Learning","Binary Classification","Python","Data Science","Hackathon"]},{"location":"projects/gstn-pbc/#evaluation-jury","title":"Evaluation &amp; Jury","text":"<ul> <li>Jury Composition: Senior data scientists, tax administration experts, academic professionals, and representatives from GSTN and NIC.</li> <li>Evaluation Process:</li> <li>Initial Screening: Compliance and basic functionality check.</li> <li>Technical Evaluation: Assessment of model performance, innovation, and robustness.</li> <li>Practical Usability: Evaluation of real-world applicability.</li> <li>Decision Making: Prizes awarded to top three teams; special prize for all-women teams. Consolation prizes at jury\u2019s discretion. The jury\u2019s decision was final.</li> </ul>","tags":["Machine Learning","Binary Classification","Python","Data Science","Hackathon"]},{"location":"projects/gstn-pbc/#timeline","title":"Timeline","text":"<ul> <li>Submission Period: August 12, 2024 \u2013 October 12, 2024</li> <li>Hackathon Duration: 45 days (from registration to final submission)</li> </ul>","tags":["Machine Learning","Binary Classification","Python","Data Science","Hackathon"]},{"location":"projects/gstn-pbc/#goals","title":"Goals","text":"<ul> <li>Build a robust, interpretable binary classifier for GSTN data.</li> <li>Address severe class imbalance and missing values.</li> <li>Ensure reproducibility and compliance with competition/legal requirements.</li> <li>Provide clear documentation and visualizations for model transparency.</li> </ul>","tags":["Machine Learning","Binary Classification","Python","Data Science","Hackathon"]},{"location":"projects/gstn-pbc/#responsibilities","title":"Responsibilities","text":"<ul> <li>Led all stages: EDA, feature engineering, model selection, tuning, evaluation, and reporting.</li> <li>Designed and implemented utility functions for cross-validation, threshold optimization, and evaluation.</li> <li>Automated model training, saving, and reproducibility via scripts and notebooks.</li> <li>Created all documentation, visualizations, and presentation materials.</li> <li>Ensured strict compliance with anonymization and data handling rules.</li> </ul>","tags":["Machine Learning","Binary Classification","Python","Data Science","Hackathon"]},{"location":"projects/gstn-pbc/#technologies-used","title":"Technologies Used","text":"<ul> <li>Languages: Python</li> <li>Libraries/Frameworks:<ul> <li>pandas, numpy (data manipulation)</li> <li>scikit-learn (modeling, metrics, pipelines)</li> <li>XGBoost, LightGBM (tree-based models)</li> <li>imbalanced-learn (SMOTE, RUS)</li> <li>matplotlib, seaborn (visualization)</li> <li>joblib (model persistence)</li> <li>SHAP (model explainability)</li> </ul> </li> <li>Tools:<ul> <li>Jupyter Notebook (EDA, prototyping)</li> <li>Markdown (documentation)</li> <li>Git (version control)</li> </ul> </li> </ul> Tools <ul> <li>dask (optional, for large data)</li> <li>scikit-optimize (Bayesian optimization)</li> <li>Google Colab (cloud prototyping)</li> </ul>","tags":["Machine Learning","Binary Classification","Python","Data Science","Hackathon"]},{"location":"projects/gstn-pbc/#process","title":"Process","text":"<ul> <li>Data Integrity: Verified dataset integrity using SHA256 checksums.</li> <li>EDA: Explored distributions, missing values, outliers, and feature correlations.</li> <li>Preprocessing: Median imputation for missing values, outlier handling (Winsorization), feature selection via correlation and statistical tests.</li> <li>Modeling: Compared Logistic Regression, Random Forest, XGBoost, LightGBM; focused on tree-based models for best performance.</li> <li>Imbalance Handling: Used Random Under Sampling (RUS) and SMOTE; tuned <code>scale_pos_weight</code> for tree models.</li> <li>Hyperparameter Tuning: Employed GridSearchCV and Bayesian Optimization for optimal parameters.</li> <li>Evaluation: Used stratified k-fold cross-validation, optimized threshold for F1, and computed metrics (Accuracy, F1, MCC, ROC-AUC, etc.).</li> <li>Explainability: Analyzed feature importance and SHAP values.</li> <li>Reporting: Documented all steps, results, and compliance measures.</li> </ul>","tags":["Machine Learning","Binary Classification","Python","Data Science","Hackathon"]},{"location":"projects/gstn-pbc/#selection-recognition","title":"Selection &amp; Recognition","text":"<ul> <li>Selection: This project, developed as a single-member (solo) team, was selected for the interview and presentation round\u2014ranking among the top 17 teams out of more than 200 participating teams (most of which were multi-member).</li> <li>Recognition: Received a certificate and prize for reaching the final round.</li> <li>Note: Unfortunately, the competition did not proceed beyond the interview/presentation stage due to unknown or unsatisfactory results for the GSTN Hackathon organizers.</li> </ul>","tags":["Machine Learning","Binary Classification","Python","Data Science","Hackathon"]},{"location":"projects/gstn-pbc/#certificate","title":"Certificate","text":"","tags":["Machine Learning","Binary Classification","Python","Data Science","Hackathon"]},{"location":"projects/gstn-pbc/#challenges-solutions","title":"Challenges &amp;  Solutions","text":"<ol> <li> <p>Severe Class Imbalance</p> <ul> <li> 91%/9% class split risked model bias.</li> <li> Used RUS, SMOTE, and tuned <code>scale_pos_weight</code> to balance classes and maximize recall/F1.</li> </ul> </li> <li> <p>Missing Values &amp; Outliers</p> <ul> <li> Columns with &gt;50% missing; outliers in many features.</li> <li> Dropped high-missing columns, used median imputation, and Winsorized outliers.</li> </ul> </li> <li> <p>Data Leakage Risk</p> <ul> <li> Potential for train-test contamination and target leakage.</li> <li> Strict separation of train/test, careful pipeline design, and validation of all preprocessing steps.</li> </ul> </li> <li> <p>Compliance &amp; Anonymization</p> <ul> <li> Competition required anonymized model names/params and no data sharing.</li> <li> Before sharing code and documentation publicly, randomized/anonymized all model identifiers and parameters; excluded all data files.</li> </ul> </li> </ol>","tags":["Machine Learning","Binary Classification","Python","Data Science","Hackathon"]},{"location":"projects/gstn-pbc/#achievements","title":"Achievements","text":"<ul> <li>Accomplished &gt;97% accuracy and strong F1/MCC on imbalanced GSTN data, as measured by cross-validation and test set metrics, by designing a robust, reproducible ML pipeline.</li> <li>Delivered a fully anonymized, competition-compliant codebase with clear documentation and reproducibility scripts.</li> <li>Produced comprehensive visualizations and reports for model transparency.</li> </ul>","tags":["Machine Learning","Binary Classification","Python","Data Science","Hackathon"]},{"location":"projects/gstn-pbc/#key-learnings","title":"Key Learnings","text":"<ul> <li>Advanced techniques for handling class imbalance in real-world datasets.</li> <li>Importance of strict data handling to prevent leakage and ensure reproducibility.</li> <li>Practical experience with hyperparameter tuning (GridSearchCV, Bayesian Optimization) and model explainability (SHAP).</li> <li>Effective communication of technical results through documentation and presentations.</li> </ul>","tags":["Machine Learning","Binary Classification","Python","Data Science","Hackathon"]},{"location":"projects/gstn-pbc/#outcomes","title":"Outcomes","text":"<ul> <li>Final model achieved:<ul> <li>Accuracy: ~97.8%</li> <li>F1 Score: ~0.89</li> <li>MCC: ~0.88</li> <li>ROC-AUC: ~0.99</li> </ul> </li> <li>All results reproducible via provided scripts (with user-supplied data).</li> <li>Project recognized for compliance and transparency in hackathon setting.</li> </ul>","tags":["Machine Learning","Binary Classification","Python","Data Science","Hackathon"]},{"location":"projects/gstn-pbc/#visuals","title":"Visuals","text":"<ul> <li>Precision-Recall Curve </li> <li>Confusion Matrix </li> <li>Feature Importance </li> </ul>","tags":["Machine Learning","Binary Classification","Python","Data Science","Hackathon"]},{"location":"projects/gstn-pbc/#links","title":"Links","text":"<ul> <li>GitHub Repository </li> </ul>","tags":["Machine Learning","Binary Classification","Python","Data Science","Hackathon"]},{"location":"projects/gstn-pbc/#conclusion","title":"Conclusion","text":"<p>This project demonstrates a full-cycle, competition-compliant approach to binary classification on challenging, imbalanced, and anonymized data. Key outcomes include high model performance, robust handling of data issues, and strict adherence to legal/competition requirements. The experience reinforced best practices in reproducibility, compliance, and transparent ML development.</p> AI Skill Assessment <p>Prompt<sup>1</sup> Source </p> <ol> <li> <p>This AI skill assessment was generated based on the skill-assessment-prompt.md and the provided project documentation. It is intended as an illustrative summary and should be interpreted in the context of the available code and documentation in codebase.\u00a0\u21a9</p> </li> </ol>","tags":["Machine Learning","Binary Classification","Python","Data Science","Hackathon"]},{"location":"projects/gstn-pbc/#strengths","title":"Strengths","text":"<ul> <li> <p>Comprehensive ML Workflow Implementation</p> <ul> <li>Demonstrates end-to-end machine learning pipeline: data integrity checks, EDA, preprocessing, feature engineering, model selection, hyperparameter tuning (GridSearchCV, Bayesian Optimization), cross-validation, threshold optimization, and evaluation.</li> <li>Uses advanced techniques for imbalanced data (SMOTE, RUS, <code>scale_pos_weight</code>).</li> <li>Implements robust evaluation with stratified k-fold cross-validation and threshold tuning for F1 optimization.</li> </ul> </li> <li> <p>Reproducibility &amp; Compliance</p> <ul> <li>All code and documentation are anonymized for competition compliance; no data files are included, and model identifiers are randomized.</li> <li>Clear instructions for reproducibility, including requirements files and explicit notes on data paths.</li> </ul> </li> <li> <p>Documentation &amp; Communication</p> <ul> <li>Extensive, well-structured documentation in Markdown: project overview, goals, process, challenges, achievements, and visuals.</li> <li>Visual aids (precision-recall curve, confusion matrix, feature importance) are generated and referenced.</li> <li>Presentation and storytelling materials are included, showing strong communication skills.</li> </ul> </li> <li> <p>Automation &amp; Utility Functions</p> <ul> <li>Modular utility functions for cross-validation, threshold finding, evaluation, and plotting.</li> <li>Scripts and notebooks automate model training, saving, and evaluation.</li> </ul> </li> <li> <p>Model Explainability</p> <ul> <li>Uses SHAP for feature importance and model transparency.</li> <li>Reports and presentations highlight interpretability and explainability.</li> </ul> </li> <li> <p>Attention to Data Integrity</p> <ul> <li>SHA256 checks for data integrity.</li> <li>Careful handling of missing values, outliers, and prevention of data leakage.</li> </ul> </li> </ul>","tags":["Machine Learning","Binary Classification","Python","Data Science","Hackathon"]},{"location":"projects/gstn-pbc/#areas-for-growth","title":"Areas for Growth","text":"<ul> <li> <p>Testing &amp; CI/CD</p> <ul> <li>No evidence of automated unit/integration tests or CI/CD pipelines (e.g., GitHub Actions, pytest).</li> <li>Adding tests for utility functions and model evaluation would improve robustness.</li> </ul> </li> <li> <p>Security &amp; Data Privacy</p> <ul> <li>While compliance is strong, there is no mention of secure handling of sensitive data or privacy-preserving ML techniques (beyond anonymization).</li> </ul> </li> <li> <p>Scalability &amp; Deployment</p> <ul> <li>No deployment scripts, Dockerfiles, or cloud deployment instructions.</li> <li>No evidence of API endpoints or serving models in production.</li> </ul> </li> <li> <p>Software Engineering Practices</p> <ul> <li>No use of type hints, code linting, or static analysis tools.</li> <li>Could benefit from more modularization (e.g., separating data, model, and utility modules).</li> </ul> </li> <li> <p>GUI/Interactive Tools</p> <ul> <li>No GUI or dashboard for model interaction or result visualization.</li> </ul> </li> </ul>","tags":["Machine Learning","Binary Classification","Python","Data Science","Hackathon"]},{"location":"projects/gstn-pbc/#role-suitability","title":"Role Suitability","text":"","tags":["Machine Learning","Binary Classification","Python","Data Science","Hackathon"]},{"location":"projects/gstn-pbc/#best-fit-roles","title":"Best Fit Roles","text":"<ul> <li> <p>Machine Learning Engineer / Data Scientist</p> <ul> <li>Strong evidence of end-to-end ML workflow, advanced modeling, and evaluation on real-world, imbalanced data.</li> <li>Experience with model explainability, compliance, and reproducibility.</li> </ul> </li> <li> <p>ML Research/Prototyping</p> <ul> <li>Demonstrated ability to experiment with multiple models, hyperparameter tuning, and reporting.</li> </ul> </li> <li> <p>Technical Documentation Specialist</p> <ul> <li>High-quality, clear, and comprehensive documentation and presentation materials.</li> </ul> </li> </ul>","tags":["Machine Learning","Binary Classification","Python","Data Science","Hackathon"]},{"location":"projects/gstn-pbc/#well-suited-for","title":"Well-Suited For","text":"<ul> <li> <p>ML/AI Competition Participant</p> <ul> <li>Familiarity with competition constraints, anonymization, and reproducibility.</li> </ul> </li> <li> <p>Data Analyst (Advanced)</p> <ul> <li>Strong EDA, feature engineering, and reporting skills.</li> </ul> </li> </ul>","tags":["Machine Learning","Binary Classification","Python","Data Science","Hackathon"]},{"location":"projects/gstn-pbc/#less-suited-for","title":"Less Suited For","text":"<ul> <li>DevOps Engineer / MLOps<ul> <li>No evidence of CI/CD, containerization, or production deployment.</li> </ul> </li> <li>Backend/API Developer<ul> <li>No API or backend service implementation.</li> </ul> </li> <li>Frontend/GUI Developer<ul> <li>No GUI or dashboard development.</li> </ul> </li> </ul> <p>Summary: The developer demonstrates strong skills in machine learning engineering, with a focus on robust, reproducible pipelines for imbalanced binary classification. Strengths include advanced modeling, compliance, documentation, and explainability. The codebase is best suited for ML engineering, data science, and technical documentation roles. Areas for growth include automated testing, CI/CD, deployment, and software engineering best practices. The developer is less suited for DevOps, backend, or frontend roles based on the current evidence.</p>","tags":["Machine Learning","Binary Classification","Python","Data Science","Hackathon"]},{"location":"projects/naukri-webscraper/","title":"Naukri Webscraper","text":"Quick Summary <p>Naukri Webscraper A Python tool that automates job searches on Naukri.com, enabling users to filter listings by skills and export structured data for analysis.</p> <ul> <li>Context: <code>Personal Project</code>, <code>Mar 2024 \u2013 Mar 2025</code>, <code>Python</code>, <code>Selenium</code>, <code>Pandas</code></li> <li>Role: Sole developer\u2014designed, implemented, tested, and documented the project</li> <li>Impact: Automated skill-based job search and CSV export, validated by automated tests and real-world data extraction</li> </ul>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#overview","title":"Overview","text":"<p>Naukri Webscraper is a Python-based automation tool that scrapes job listings from Naukri.com. It extracts job titles, companies, salaries, locations, and required skills, then filters results based on user-specified skills. The project outputs structured data as CSV files for further analysis. The project was developed independently as a personal automation and data analysis initiative.</p> <p>Recent updates:</p> <ul> <li>Added automated tests for core scraping and filtering logic (<code>test_project.py</code>) using <code>pytest</code></li> <li>Improved error handling and robustness in dynamic content extraction</li> <li>Updated documentation and usage instructions in <code>README.md</code></li> </ul>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#goals","title":"Goals","text":"<ul> <li>Automate the retrieval and filtering of job listings from Naukri.com based on user-defined skills.</li> <li>Simplify and accelerate the job search process by eliminating manual filtering.</li> <li>Provide structured, exportable data for further analysis.</li> </ul>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#responsibilities","title":"Responsibilities","text":"<ul> <li>Designed and implemented the scraping logic using Selenium WebDriver.</li> <li>Developed skill-based filtering and CSV export using Pandas.</li> <li>Addressed dynamic content loading and missing data scenarios.</li> <li>Authored automated tests with <code>pytest</code> to ensure code correctness and reliability.</li> <li>Wrote comprehensive documentation and usage instructions.</li> </ul>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#technologies-used","title":"Technologies Used","text":"<ul> <li>Languages: Python (primary language for all scripts and logic)</li> <li>Frameworks/Libraries: <ul> <li>Selenium (browser automation and web scraping)  </li> <li>Pandas (data manipulation and CSV export)</li> </ul> </li> <li>Testing: <ul> <li>pytest (for automated tests in <code>test_project.py</code>)</li> </ul> </li> <li>DevOps/Tools: <ul> <li>Git (version control)</li> <li>Chrome WebDriver (browser automation)</li> </ul> </li> <li>Documentation: <ul> <li>Markdown (<code>README.md</code> for usage and setup)</li> </ul> </li> </ul> Tools <ul> <li>Git (version control)</li> <li>Chrome WebDriver (browser automation)</li> <li>Markdown (project documentation)</li> <li>pytest (automated testing)</li> </ul>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#process","title":"Process","text":"<ol> <li>Planning: <ul> <li>Identified key data fields (title, company, salary, location, skills) to extract from Naukri.com.</li> </ul> </li> <li>Implementation: <ul> <li>Used Selenium to automate browser actions and extract job data.</li> <li>Employed Pandas for data structuring and CSV export.</li> <li>Developed a filtering mechanism for user-specified skills.</li> </ul> </li> <li>Testing: <ul> <li>Created automated tests (<code>test_project.py</code>) using <code>pytest</code> to validate scraping and filtering logic.</li> </ul> </li> <li>Documentation: <ul> <li>Documented setup, usage, and troubleshooting in <code>README.md</code>.</li> </ul> </li> </ol>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#recognition","title":"Recognition","text":"<p>I am proud to share that I have successfully completed the <code>CS50P - Introduction to Programming with Python</code> course.</p>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#certificate","title":"Certificate","text":"","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#challenges-solutions","title":"Challenges &amp;  Solutions","text":"<ol> <li> <p>Dynamic Content Loading</p> <ul> <li> Naukri.com uses JavaScript to render job listings, causing timing issues for scraping.</li> <li> Used Selenium's <code>WebDriverWait</code> to ensure elements are loaded before extraction.</li> </ul> </li> <li> <p>Complex HTML Structures</p> <ul> <li> Extracting data from inconsistent or nested HTML elements.</li> <li> Implemented a helper function (<code>get_text_or_default</code>) for robust text extraction.</li> </ul> </li> <li> <p>Performance Bottlenecks</p> <ul> <li> Slow scraping due to large result sets and dynamic content.</li> <li> Optimized data extraction loops and used efficient Pandas operations for filtering/export.</li> </ul> </li> <li> <p>Testing Automation</p> <ul> <li> Ensuring scraping logic remains reliable as site structure changes.</li> <li> Developed automated tests with <code>pytest</code> to validate core logic and catch regressions.</li> </ul> </li> </ol> Note : On Site Changes and Locators <ul> <li>The HTML structure and element locators (CSS selectors, XPaths) used in <code>project.py</code> are based on the current version of Naukri.com.  </li> <li>If the website updates its layout or class names, you may need to update these locators in the code to restore scraping functionality.  </li> <li>Review and adjust selectors in <code>project.py</code> if you encounter errors or missing data after a site update.</li> </ul>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#achievements","title":"Achievements","text":"<ul> <li>Automated the extraction and filtering of job listings from Naukri.com.</li> <li>Enabled skill-based filtering and CSV export for downstream analysis.</li> <li>Developed a test suite (<code>test_project.py</code>) using <code>pytest</code> to ensure reliability.</li> <li>Improved scraping robustness and error handling based on real-world site changes.</li> </ul>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#key-learnings","title":"Key Learnings","text":"<ul> <li>Gained practical experience with Selenium for dynamic web scraping.</li> <li>Enhanced skills in data manipulation and export using Pandas.</li> <li>Learned to write maintainable, testable code for web automation projects.</li> <li>Understood the importance of robust error handling and documentation.</li> <li>Applied <code>pytest</code> for effective and maintainable automated testing.</li> </ul>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#outcomes","title":"Outcomes","text":"<ul> <li>Successfully automated job search and filtering for Naukri.com.</li> <li>Produced structured CSV datasets for analysis.</li> <li>Provided a reusable, documented tool for job seekers and data analysts.</li> <li>Ensured code reliability through automated testing with <code>pytest</code>.</li> </ul>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#visuals","title":"Visuals","text":"","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#video-demo","title":"Video Demo","text":"","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#links","title":"Links","text":"<ul> <li>GitHub Repository</li> </ul>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#conclusion","title":"Conclusion","text":"<p>Naukri Webscraper demonstrates the power of Python automation for real-world data extraction and analysis. By combining Selenium and Pandas, the project streamlines job searches, enhances productivity, and provides actionable insights through structured data exports. The codebase is robust, tested with <code>pytest</code>, and well-documented for future use and extension.</p> AI Skill Assessment <p>Prompt<sup>1</sup> Source </p> <ol> <li> <p>This <code>AI skill assessment</code> was generated based on the skill-assessment-prompt.md and the provided project documentation. It is intended as an illustrative summary and should be interpreted in the context of the available code and documentation in codebase.\u00a0\u21a9</p> </li> </ol>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#strengths","title":"Strengths","text":"<ul> <li> <p>Web Scraping Automation</p> <ul> <li>Demonstrates strong proficiency with Selenium for browser automation, including headless operation, custom user agents, and dynamic navigation (e.g., paginated scraping, handling search forms).</li> <li>Robust handling of web elements using both CSS selectors and XPaths, with fallback/default logic for missing elements.</li> </ul> </li> <li> <p>Data Handling &amp; Export</p> <ul> <li>Uses Pandas effectively for data manipulation and CSV export.</li> <li>Implements structured data extraction with a clear schema (job title, company, salary, skills, etc.).</li> </ul> </li> <li> <p>Testing &amp; Quality Assurance</p> <ul> <li>Provides automated tests using <code>pytest</code>, including fixtures, mocking user input, and live web tests (with appropriate skips for anti-bot/site change issues).</li> <li>Tests cover both core scraping logic and utility functions.</li> </ul> </li> <li> <p>Documentation</p> <ul> <li>Comprehensive technical documentation (<code>doc.md</code>) and user-facing README.md with clear instructions, schema definitions, troubleshooting, and usage examples.</li> <li>Documents function purposes, parameters, and expected behaviors in code docstrings.</li> </ul> </li> <li> <p>User Interaction &amp; Error Handling</p> <ul> <li>Interactive CLI prompts for user input (search terms, page count, skill filters).</li> <li>Handles invalid input and exceptions gracefully (e.g., <code>KeyboardInterrupt</code>, <code>ValueError</code>, missing elements).</li> </ul> </li> <li> <p>Project Structure &amp; Packaging</p> <ul> <li>Uses pyproject.toml for dependency management and project metadata.</li> <li>Separates main logic, tests, and documentation cleanly.</li> </ul> </li> </ul>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#areas-for-growth","title":"Areas for Growth","text":"<ul> <li> <p>Security &amp; Anti-Bot Evasion</p> <ul> <li>No evidence of advanced anti-bot evasion techniques (e.g., proxy rotation, CAPTCHA handling, request throttling beyond simple sleep).</li> <li>No explicit handling of robots.txt or ethical scraping considerations in code.</li> </ul> </li> <li> <p>Scalability &amp; Performance</p> <ul> <li>Scraping is single-threaded and synchronous; not optimized for large-scale or parallel scraping.</li> <li>No batching, queuing, or distributed scraping logic.</li> </ul> </li> <li> <p>CI/CD &amp; DevOps</p> <ul> <li>No evidence of CI/CD pipelines, Dockerization, or deployment automation.</li> <li>No Makefile or scripts for environment setup/testing.</li> </ul> </li> <li> <p>Code Modularity &amp; Extensibility</p> <ul> <li>All logic is in a single script (<code>project.py</code>); could benefit from modularization (e.g., separating scraping, filtering, and CLI logic).</li> <li>No plugin or configuration system for adapting to site changes.</li> </ul> </li> <li> <p>Error Logging &amp; Monitoring</p> <ul> <li>Uses print statements for errors; lacks structured logging or monitoring for production use.</li> </ul> </li> <li> <p>GUI/UX</p> <ul> <li>No GUI or web interface; CLI-only interaction.</li> </ul> </li> </ul>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#role-suitability","title":"Role Suitability","text":"","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#best-fit-roles","title":"Best Fit Roles","text":"<ul> <li>Python Backend Developer<ul> <li>Strong evidence of backend scripting, data processing, and automation skills.</li> </ul> </li> <li>Web Scraping/Data Extraction Engineer<ul> <li>Demonstrated expertise in Selenium, data extraction, and handling dynamic web content.</li> </ul> </li> <li>QA Automation Engineer<ul> <li>Experience with automated testing, mocking, and test-driven development using <code>pytest</code>.</li> </ul> </li> <li>Technical Writer/Documentation Specialist<ul> <li>High-quality, thorough documentation and user guides.</li> </ul> </li> </ul>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#well-suited-for","title":"Well-Suited For","text":"<ul> <li>Data Analyst (with Python)<ul> <li>Familiarity with Pandas and CSV data workflows.</li> </ul> </li> <li>SDET (Software Development Engineer in Test)<ul> <li>Automated test coverage and test design.</li> </ul> </li> </ul>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#less-suited-for","title":"Less Suited For","text":"<ul> <li>Frontend Developer<ul> <li>No evidence of frontend/UI development (web or desktop).</li> </ul> </li> <li>DevOps Engineer<ul> <li>Lacks CI/CD, containerization, and deployment automation.</li> </ul> </li> <li>Cloud/Distributed Systems Engineer<ul> <li>No cloud integration, distributed scraping, or scalable architecture.</li> </ul> </li> </ul> <p>Summary: The developer demonstrates strong skills in Python scripting, web scraping automation with Selenium, data processing with Pandas, and automated testing with <code>pytest</code>. The codebase is well-documented and user-friendly, with robust error handling and interactive CLI features. Areas for growth include modularization, scalability, advanced anti-bot techniques, and DevOps practices. The developer is best suited for backend, automation, and data extraction roles, and less suited for frontend or DevOps-focused positions based on the current codebase.</p>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/paraxcel/","title":"Paraxcel","text":"Quick Summary <p>Paraxcel A lightweight, local-first Python desktop application using Tkinter to convert Microsoft Word DOCX files containing multiple-choice questions into structured Excel spreadsheets.</p> <ul> <li>Context: <code>Python</code>, <code>Tkinter</code>, <code>Pandas</code>, <code>Pydantic</code>, <code>python-docx</code>, <code>Solo Project</code>, <code>Feb-Mar 2025</code></li> <li>Role: Sole developer responsible for design, implementation, testing, documentation, and packaging of the application.</li> <li>Impact: Created a tool that automates the extraction of questions and answers from DOCX files, reducing manual data entry time for educators and content creators, by implementing parsing logic with <code>python-docx</code> and structuring output with <code>pandas</code>.</li> </ul>","tags":["Python"]},{"location":"projects/paraxcel/#overview","title":"Overview","text":"<p>Paraxcel is a Python desktop application built with Tkinter that addresses the need for converting multiple-choice questions from DOCX files into an organized Excel format, it targets educators, content creators, and assessment professionals who need to manage question banks efficiently. The application provides a simple graphical user interface for file selection and conversion, running entirely locally.</p>","tags":["Python"]},{"location":"projects/paraxcel/#goals","title":"Goals","text":"<p>The primary goals for the Paraxcel project were:</p> <ul> <li>To automate the tedious and time-consuming manual process of extracting multiple-choice questions and their corresponding answers from Microsoft Word documents.</li> <li>To structure the extracted data into a usable and organized Excel format.</li> <li>To create a simple, reliable, and accessible desktop tool for educators and content creators.</li> </ul>","tags":["Python"]},{"location":"projects/paraxcel/#responsibilities","title":"Responsibilities","text":"<ul> <li>Designed the application architecture, including module separation (<code>docx_parser</code>, <code>excel_writer</code>, <code>model</code>, <code>para_utility</code>, <code>interface</code>) for maintainability and scalability.</li> <li>Implemented robust <code>DOCX</code> parsing using <code>python-docx</code> to accurately extract <code>questions</code>, <code>answer options</code>, and identify the <code>correct answer</code> based on formatting (<code>color/highlight</code>).</li> <li>Utilized pandas to structure extracted data into a standardized, clean format, enabling reliable export to <code>.xlsx</code> files.</li> <li>Built a user-friendly graphical interface with <code>Tkinter</code>, enabling users to easily select input files/folders and initiate the conversion process.</li> <li>Integrated Pydantic for rigorous data validation of extracted question data, ensuring data integrity before export.</li> <li>Created essential utility functions (<code>para_utility.py</code>) for text cleaning, format handling, and precise answer detection.</li> <li>Authored comprehensive technical (<code>doc.md</code>) and user (<code>README.md</code>) documentation.</li> <li>Packaged the application into a standalone executable using <code>PyInstaller</code> for straightforward distribution and use on Windows.</li> </ul>","tags":["Python"]},{"location":"projects/paraxcel/#technologies-used","title":"Technologies Used","text":"<ul> <li>Languages: Python</li> <li>GUI: Tkinter (Standard Python library) - For building the desktop graphical interface.</li> <li>DOCX Parsing: <code>python-docx</code> - For reading and analyzing <code>.docx</code> file content.</li> <li>Data Handling &amp; Excel Export: <code>pandas</code> - For structuring the extracted data and writing to <code>.xlsx</code> files.</li> <li>Data Validation: <code>Pydantic</code> - For validating the structure and types of extracted question data.</li> <li>Documentation: <code>Markdown</code> - For <code>README.md</code> and <code>doc.md</code>.</li> </ul> Tools <ul> <li>Version Control: Git</li> <li>Packaging: PyInstaller - For creating the standalone executable.</li> <li>Development Environment: VS Code</li> </ul>","tags":["Python"]},{"location":"projects/paraxcel/#process","title":"Process","text":"<p>The development process involved identifying the need for a simple DOCX-to-Excel conversion tool for MCQs and followed a structured approach focused on modularity and ease of use.</p> <ol> <li>Requirement Gathering: Defined the core functionality: parse DOCX files containing questions followed by four options and export them to Excel, including support for detecting marked answers.</li> <li>Technology Stack Selection: Chose libraries (<code>python-docx</code>, <code>pandas</code>, <code>Tkinter</code>, <code>Pydantic</code>) best suited for the task, balancing functionality with ease of deployment (local-first, standard libraries).</li> <li>Modular Implementation: Developed each component (<code>parsing</code>, <code>writing</code>, <code>GUI</code>, <code>validation</code>) as a distinct module.</li> <li>Testing &amp; Refinement: Used sample files to rigorously test parsing accuracy and output format.</li> <li>Documentation: Created user and technical guides to support adoption and understanding.</li> <li>Packaging: Prepared the application for distribution as a single executable.</li> </ol>","tags":["Python"]},{"location":"projects/paraxcel/#recognition","title":"Recognition","text":"<p>I am proud to share that I have successfully completed the <code>CS50x - Introduction to Computer Science</code> course.</p>","tags":["Python"]},{"location":"projects/paraxcel/#certificate","title":"Certificate","text":"","tags":["Python"]},{"location":"projects/paraxcel/#challenges-solutions","title":"Challenges &amp;  Solutions","text":"<ol> <li> <p>Handling Varied DOCX Formatting</p> <ul> <li> Parsing semi-structured DOCX files presented challenges due to inconsistencies in formatting, numbering, and spacing. Reliably detecting the correct answer based on subtle formatting like font color or highlighting was a key challenge.</li> <li> Developed flexible parsing logic (<code>parse_para</code>) designed to accommodate common variations. Implemented specialized utility functions (<code>remove_prefix</code>, <code>find_marked_answer</code>) that leverage python-docx's capabilities to accurately identify marked answers by inspecting run-level formatting properties. Documented input format expectations clearly to guide users.</li> </ul> </li> <li> <p>Ensuring Data Quality and Consistency</p> <ul> <li> Extracting data from a semi-structured format like DOCX risked incomplete or malformed records before export.</li> <li> Integrated Pydantic models (<code>Question</code>) to enforce a strict schema for extracted data. This validation step acts as a safeguard, ensuring that only correctly structured and typed data proceeds to the <code>Excel</code> export, preventing errors and ensuring reliable output.</li> </ul> </li> <li> <p>Creating an Accessible Tool for Non-Technical Users</p> <ul> <li> The goal was a tool usable by educators without programming knowledge, requiring a simple interface and easy installation.</li> <li> Built a straightforward and intuitive GUI using Tkinter, Python's standard library, minimizing external dependencies. Used <code>PyInstaller</code> to bundle the application and all its dependencies into a single, easy-to-distribute executable (<code>paraxcel.exe</code>), significantly lowering the barrier to entry for end-users.</li> </ul> </li> </ol>","tags":["Python"]},{"location":"projects/paraxcel/#achievements","title":"Achievements","text":"<ul> <li>Developed and launched Paraxcel, a functional desktop application, automating the conversion of MCQs from DOCX to a structured Excel format.</li> <li>Implemented advanced parsing features, including the ability to detect correct answers based on font color or highlighting within the DOCX file.</li> <li>Incorporated basic text formatting handling (superscript/subscript) during extraction for improved data fidelity.</li> <li>Provided clear, user-focused documentation (<code>README.md</code>) and technical insights (<code>doc.md</code>).</li> <li>Packaged the application into a convenient standalone executable using PyInstaller, simplifying deployment and usage.</li> </ul> <p>Impact: Enabled educators and content creators to save significant time and effort (quantified by reduced manual data entry hours) previously spent on manual data entry.</p>","tags":["Python"]},{"location":"projects/paraxcel/#key-learnings","title":"Key Learnings","text":"<ul> <li>Gained practical experience using the <code>python-docx</code> library to parse the structure and formatting of Word documents programmatically.</li> <li>Developed skills in building simple desktop GUIs with Python's built-in <code>Tkinter</code> library.</li> <li>Applied <code>Pydantic</code> for robust data validation in a data processing pipeline.</li> <li>Utilized <code>pandas</code> for efficient data structuring and exporting to Excel formats.</li> <li>Learned the process of packaging Python applications into standalone executables using <code>PyInstaller</code>, including handling dependencies and data files.</li> <li>Understood the challenges and importance of defining clear input format expectations when parsing semi-structured documents like DOCX.</li> </ul>","tags":["Python"]},{"location":"projects/paraxcel/#outcomes","title":"Outcomes","text":"<ul> <li>A working, local-first desktop application (<code>paraxcel.exe</code>) capable of converting DOCX files (containing questions and 4 options) into structured Excel (<code>.xlsx</code>) files.</li> <li>Source code is available on GitHub, along with documentation and sample files.</li> <li>A video demonstration showcasing the application's functionality.</li> </ul>","tags":["Python"]},{"location":"projects/paraxcel/#visuals","title":"Visuals","text":"<p>Docx Input</p> <p>Q1. What is the capital of France? A. Berlin B. Madrid C. Paris (Highlighted as correct) D. Rome  </p> <p>\u2705 Excel Output</p> Question Option 1 Option 2 Option 3 Option 4 Answer Index What is the capital of France? Berlin Madrid Paris Rome 3","tags":["Python"]},{"location":"projects/paraxcel/#screenshots","title":"\ud83d\uddbc\ufe0f Screenshots","text":"<p>Paraxcel Tkinter GUI showing file/folder selection fields and buttons.</p> <p></p> <p>Sample input DOCX file snippet showing question/option format.</p> <p></p> <p>Resulting Excel file snippet showing structured data.</p>","tags":["Python"]},{"location":"projects/paraxcel/#video-demo","title":"\ud83d\udd17 Video Demo","text":"","tags":["Python"]},{"location":"projects/paraxcel/#links","title":"Links","text":"<ul> <li>GitHub Repository</li> </ul>","tags":["Python"]},{"location":"projects/paraxcel/#conclusion","title":"Conclusion","text":"<p>Paraxcel successfully provides a targeted solution for automating the often tedious task of extracting multiple-choice questions from DOCX files into a more usable Excel format. By leveraging libraries like <code>python-docx</code>, <code>pandas</code>, <code>Pydantic</code>, and <code>Tkinter</code>, the project delivers a functional, easy-to-use desktop tool for educators and content creators. Key takeaways include the practical application of these libraries for document parsing, data handling, validation, GUI development, and application packaging, resulting in a useful utility that addresses a specific workflow challenge.</p> AI Skill Assessment <p>Prompt<sup>1</sup> Source </p> <ol> <li> <p>This AI skill assessment was generated based on the skill-assessment-prompt.md and the provided project documentation. It is intended as an illustrative summary and should be interpreted in the context of the available code and documentation in codebase.\u00a0\u21a9</p> </li> </ol>","tags":["Python"]},{"location":"projects/paraxcel/#strengths","title":"Strengths","text":"<ul> <li>Python Application Development: Proven ability to design, develop, and package a complete, modular desktop application.</li> <li>GUI Development (Tkinter): Experience building functional graphical interfaces for user interaction.</li> <li>Document Parsing &amp; Data Processing: Skilled in extracting structured data from complex document formats (<code>.docx</code>) and processing it using <code>pandas</code>.</li> <li>Data Validation: Practical application of <code>Pydantic</code> for ensuring data integrity and correctness.</li> <li>Comprehensive Documentation: Ability to create clear technical and user-focused documentation.</li> <li>Application Packaging &amp; Distribution: Experience using <code>PyInstaller</code> for creating standalone executables and managing dependencies.</li> <li>CI/CD Implementation: Basic experience setting up automated workflows for testing, security checks, and builds using GitHub Actions.</li> <li>Software Reliability Basics: Inclusion of testing tools and security scanning indicates an understanding of foundational quality practices.</li> </ul>","tags":["Python"]},{"location":"projects/paraxcel/#areas-for-improvement","title":"Areas for Improvement","text":"<ul> <li>Testing Depth: Expanding test coverage and visibility would further strengthen quality assurance processes.</li> <li>Advanced Error Handling: Implementing more granular logging and exception handling could enhance application robustness.</li> <li>Performance Optimization: Exploring techniques for handling very large files more efficiently could improve scalability.</li> <li>UI/UX: For projects requiring more complex interfaces, exploring modern GUI frameworks might be beneficial.</li> <li>Cross-Platform Deployment: Expanding build support beyond Windows would increase application accessibility.</li> </ul>","tags":["Python"]},{"location":"projects/paraxcel/#relevant-roles","title":"Relevant Roles","text":"","tags":["Python"]},{"location":"projects/paraxcel/#strong-fit","title":"Strong Fit","text":"<ul> <li>Python Application Developer: Directly aligns with the project's nature.</li> <li>Automation Engineer: Demonstrates strong skills in automating data extraction and processing workflows.</li> </ul>","tags":["Python"]},{"location":"projects/paraxcel/#good-fit","title":"Good Fit","text":"<ul> <li>Backend Developer (Data Focus): Relevant experience in data parsing, validation, and structuring.</li> <li>Junior DevOps/Build Engineer: Basic experience with CI/CD automation and application packaging.</li> </ul>","tags":["Python"]},{"location":"projects/paraxcel/#less-direct-fit","title":"Less Direct Fit","text":"<ul> <li>Frontend Web Developer: No web technology experience shown.</li> <li>Data Scientist/ML Engineer: Project focuses on extraction, not analysis or modeling.</li> <li>Senior DevOps/SRE: Lacks infrastructure, monitoring, or cloud services.</li> <li>Mobile Developer: No mobile development experience shown.</li> </ul>","tags":["Python"]},{"location":"projects/paraxcel/#conclusion_1","title":"Conclusion","text":"<p>This project effectively showcases capabilities in end-to-end Python application development, particularly in document processing, data handling, and automation. The inclusion of data validation, packaging, and basic CI/CD demonstrates a well-rounded approach to software development. This experience is highly relevant for roles focused on Python application development, automation, and data processing pipelines.</p>","tags":["Python"]},{"location":"projects/s3-faker/","title":"S3 Faker","text":"<p>S3 Faker is a tool designed to generate fake data based on a JSON configuration file. The generated data can be saved locally and also uploaded to an AWS S3 bucket. This project is ideal for testing and development purposes, allowing developers to simulate S3 environments without the need for actual AWS resources. Key features include data generation using the Faker library, support for multiple output formats (CSV, JSON, Parquet), and integration with AWS S3 via s3fs.</p>"},{"location":"projects/s3-faker/#responsibilities","title":"Responsibilities","text":"<ul> <li>Designed and implemented the core functionalities of the S3 Faker project.</li> <li>Developed scripts and modules to accurately simulate S3 behavior.</li> <li>Led the integration of the project with existing development and testing pipelines.</li> <li>Ensured the project adhered to best practices in terms of security and performance.</li> <li>Coordinated with team members to gather requirements and provide technical guidance.</li> </ul>"},{"location":"projects/s3-faker/#technologies-used","title":"Technologies Used","text":"<ul> <li>Languages: Python, PowerShell, Shell</li> <li>Frameworks/Libraries: Faker, Pandas, fsspec</li> <li>Tools: Git, Docker, LocalStack, AWS CLI</li> </ul>"},{"location":"projects/s3-faker/#challenges-and-solutions","title":"Challenges and Solutions","text":"<ul> <li>Challenge: Simulating the comprehensive feature set of Amazon S3, including edge cases.</li> <li>Solution: Conducted extensive research on S3 APIs and utilized <code>fsspec</code> to implement accurate simulations. Developed custom scripts to handle edge cases and ensure robustness.</li> <li>Challenge: Ensuring performance and scalability of the local S3 environment.</li> <li>Solution: Optimized code and utilized Docker for containerization, allowing for scalable and isolated testing environments.</li> </ul>"},{"location":"projects/s3-faker/#achievements","title":"Achievements","text":"<ul> <li>Successfully created a fully functional S3 simulation environment, reducing reliance on actual S3 resources by 80%.</li> <li>Integrated the project with CI/CD pipelines, significantly speeding up the development and testing cycles.</li> <li>Received positive feedback from team members and external testers for the accuracy and reliability of the simulation.</li> </ul>"},{"location":"projects/s3-faker/#key-learnings","title":"Key Learnings","text":"<ul> <li>Gained in-depth knowledge of Amazon S3 APIs and their intricacies.</li> <li>Enhanced skills in Python and PowerShell scripting.</li> <li>Improved understanding of containerization and its benefits in development and testing environments.</li> <li>Learned the importance of thorough testing and documentation in ensuring project success.</li> </ul>"},{"location":"projects/s3-faker/#link-to-project","title":"Link to Project","text":"<ul> <li>GitHub Repository</li> </ul>"},{"location":"projects/s3-faker/#screenshots","title":"Screenshots","text":""},{"location":"projects/test-site/","title":"Test Management Site","text":"In-Hurry Summary <p>Test Management Site A demo vanilla javascripting application for managing tests, enabling test creation, execution, and result tracking. Aims to showcase basic web application functionality using dynamic UI updates and local storage.</p> <ul> <li>Context: <code>Personal Project</code>, <code>Jun 2024</code>, <code>HTML</code>, <code>CSS</code>, <code>JavaScript</code>, <code>Bootstrap</code></li> <li>Role: Sole Developer - Responsible for designing, developing, and implementing all features.</li> <li>Impact: Demonstrated full-stack capabilities by creating a functional test management system, showcasing skills in UI design, dynamic content loading, and local data storage.</li> </ul>","tags":["Frontend"]},{"location":"projects/test-site/#overview","title":"Overview","text":"<p>The Test Management Site is a demo vanilla javascripts application developed in April 2025 to showcase test management capabilities. It allows users to create, manage, take tests, and view results. The application uses HTML, CSS, JavaScript, and Bootstrap for a responsive and dynamic user interface.</p>","tags":["Frontend"]},{"location":"projects/test-site/#goals","title":"Goals","text":"<ul> <li>Provide a platform to create, update, and delete tests.</li> <li>Enable users to take tests with a specified duration.</li> <li>Display test results and history.</li> <li>Implement user management features like login, logout, and password changes.</li> <li>Showcase dynamic UI updates and local data storage.</li> </ul>","tags":["Frontend"]},{"location":"projects/test-site/#responsibilities","title":"Responsibilities","text":"<ul> <li>Designed and developed the entire application from scratch.</li> <li>Implemented dynamic UI updates using JavaScript.</li> <li>Managed data storage using <code>localStorage</code>.</li> <li>Created user authentication and session management features.</li> <li>Integrated Bootstrap for responsive design and UI components.</li> </ul>","tags":["Frontend"]},{"location":"projects/test-site/#technologies-used","title":"Technologies Used","text":"<ul> <li>Languages: JavaScript, HTML, CSS</li> <li>Frameworks/Libraries: Bootstrap, PapaParse, XLSX.js, Plotly.js</li> <li>Tools: Visual Studio Code, GitHub Actions (for CI/CD and static site deployment)</li> <li>Browser APIs: localStorage, sessionStorage, Fetch API</li> <li>Other: Modular JavaScript (ES6 modules), CSS custom properties, GitHub Pages (hosting)</li> </ul>","tags":["Frontend"]},{"location":"projects/test-site/#process","title":"Process","text":"<p>The project followed an iterative development approach. Initially, the basic HTML structure and CSS styling were set up. JavaScript was then used to dynamically load content, manage user sessions, and handle data storage. Bootstrap was integrated to ensure a responsive design. Challenges were addressed through continuous debugging and refinement of the code.</p>","tags":["Frontend"]},{"location":"projects/test-site/#challenges-solutions","title":"Challenges &amp;  Solutions","text":"<ol> <li> <p>Dynamic Content Loading </p> <ul> <li> Loading and updating content dynamically using JavaScript.  </li> <li> Used <code>fetch</code> API to load HTML templates and JavaScript to manipulate the DOM, ensuring smooth transitions and updates.</li> </ul> </li> <li> <p>Data Management with <code>localStorage</code> </p> <ul> <li> Managing and persisting data using <code>localStorage</code>.  </li> <li> Implemented functions to serialize and deserialize data, ensuring data integrity and persistence across sessions.</li> </ul> </li> <li> <p>Responsive Design </p> <ul> <li> Ensuring the application is responsive across different devices.  </li> <li> Utilized Bootstrap's grid system and CSS media queries to create a responsive layout.</li> </ul> </li> </ol>","tags":["Frontend"]},{"location":"projects/test-site/#achievements","title":"Achievements","text":"<ul> <li>Successfully implemented all core features: test management, test taking, results display, and user management.</li> <li>Created a dynamic and responsive user interface.</li> <li>Demonstrated proficiency in JavaScript, HTML, CSS, and Bootstrap.</li> <li>Implemented data persistence using <code>localStorage</code>.</li> </ul>","tags":["Frontend"]},{"location":"projects/test-site/#key-learnings","title":"Key Learnings","text":"<ul> <li>Gained a deeper understanding of dynamic content loading and manipulation using JavaScript.</li> <li>Learned how to effectively use <code>localStorage</code> for data persistence.</li> <li>Improved skills in responsive web design using Bootstrap.</li> <li>Enhanced problem-solving abilities through debugging and refining the code.</li> </ul>","tags":["Frontend"]},{"location":"projects/test-site/#outcomes","title":"Outcomes","text":"<p>The project resulted in a functional test management application that showcases dynamic UI updates and local data storage. The application allows users to create, manage, take tests, and view results.</p>","tags":["Frontend"]},{"location":"projects/test-site/#visuals","title":"Visuals","text":"","tags":["Frontend"]},{"location":"projects/test-site/#screenshot-project-images","title":"Screenshot Project Images","text":"<p>Here are some images showcasing the project:</p>","tags":["Frontend"]},{"location":"projects/test-site/#login-page","title":"Login Page","text":"","tags":["Frontend"]},{"location":"projects/test-site/#register-page","title":"Register Page","text":"","tags":["Frontend"]},{"location":"projects/test-site/#test-selection","title":"Test Selection","text":"","tags":["Frontend"]},{"location":"projects/test-site/#test-page","title":"Test Page","text":"","tags":["Frontend"]},{"location":"projects/test-site/#test-timeout","title":"Test Timeout","text":"","tags":["Frontend"]},{"location":"projects/test-site/#dashboard","title":"Dashboard","text":"","tags":["Frontend"]},{"location":"projects/test-site/#user-management","title":"User Management","text":"","tags":["Frontend"]},{"location":"projects/test-site/#crud-operations","title":"CRUD Operations","text":"","tags":["Frontend"]},{"location":"projects/test-site/#test-results","title":"Test Results","text":"","tags":["Frontend"]},{"location":"projects/test-site/#error-handling","title":"Error Handling","text":"","tags":["Frontend"]},{"location":"projects/test-site/#links","title":"Links","text":"<ul> <li>Live Project</li> <li>GitHub Repository</li> </ul>","tags":["Frontend"]},{"location":"projects/test-site/#conclusion","title":"Conclusion","text":"<p>The Test Management Site project was a valuable learning experience that allowed me to demonstrate my full-stack capabilities. By creating a functional application with dynamic UI updates and local data storage, I showcased my skills in JavaScript, HTML, CSS, and Bootstrap. Success was measured by the successful implementation of all core features and the creation of a responsive and dynamic user interface.</p> AI Skill Assessment <p>Prompt<sup>1</sup> Source </p> <ol> <li> <p>This <code>AI skill assessment</code> was generated based on the skill-assessment-prompt.md and the provided project documentation. It is intended as an illustrative summary and should be interpreted in the context of the available code and documentation in codebase.\u00a0\u21a9</p> </li> </ol>","tags":["Frontend"]},{"location":"projects/test-site/#strengths","title":"Strengths","text":"<ul> <li> <p>Frontend Engineering (HTML/CSS/JS):</p> <ul> <li>Demonstrates strong skills in vanilla JavaScript for DOM manipulation, modular code organization, and dynamic UI updates (e.g., main.js, <code>js/tests_content.js</code>).</li> <li>Uses modern CSS with custom properties, responsive design, and theme toggling (see <code>styles/css/styles.css</code>).</li> <li>Implements modular HTML with reusable components loaded dynamically (e.g., sidebar, topnav, profile, as seen in sidebar.js and HTML assets).</li> </ul> </li> <li> <p>Data Handling &amp; Storage:</p> <ul> <li>Effectively uses <code>localStorage</code> and <code>sessionStorage</code> for persistent and session-based data (user profiles, test data, results).</li> <li>Handles file uploads and parsing for CSV and Excel formats using third-party libraries (PapaParse, XLSX.js), with robust error handling and user feedback (<code>js/utility/file_handle.js</code>).</li> </ul> </li> <li> <p>Object-Oriented Design:</p> <ul> <li>Defines clear, extensible classes for domain entities (User, Profiles, Test, Question, Result, etc.) with encapsulated logic (<code>js/utility/temp_data.js</code>, <code>js/utility/temp_profile.js</code>).</li> </ul> </li> <li> <p>User Experience &amp; Usability:</p> <ul> <li>Provides detailed user feedback and error messages throughout the UI (e.g., form validation, alerts for missing data, dynamic content updates).</li> <li>Implements accessibility features such as keyboard navigation and clear visual cues for active elements.</li> </ul> </li> <li> <p>Documentation &amp; Readability:</p> <ul> <li>Includes a comprehensive readme.md with project overview, features, technical details, folder structure, and screenshots.</li> <li>Uses descriptive variable/function names and inline comments for clarity.</li> </ul> </li> <li> <p>Basic DevOps Awareness:</p> <ul> <li>Includes a GitHub Actions workflow for static site deployment to GitHub Pages (<code>.github/workflows/deploy.yml</code>).</li> </ul> </li> </ul>","tags":["Frontend"]},{"location":"projects/test-site/#areas-for-growth","title":"Areas for Growth","text":"<ul> <li> <p>Testing:</p> <ul> <li>No evidence of automated unit, integration, or end-to-end tests. All logic appears to be tested manually via the UI.</li> <li>No test framework or test scripts present.</li> </ul> </li> <li> <p>Security:</p> <ul> <li>User authentication is handled entirely client-side with passwords stored in plain text in localStorage/sessionStorage.</li> <li>No input sanitization or protection against XSS/CSRF.</li> <li>No encryption or secure handling of sensitive data.</li> </ul> </li> <li> <p>Scalability &amp; Backend Integration:</p> <ul> <li>The application is entirely client-side; there is no backend API, database, or server-side logic.</li> <li>Not suitable for multi-user or production environments without significant changes.</li> </ul> </li> <li> <p>Accessibility &amp; Internationalization:</p> <ul> <li>While some accessibility is present, there is no evidence of ARIA roles, screen reader support, or internationalization/localization.</li> </ul> </li> <li> <p>Advanced DevOps/CI/CD:</p> <ul> <li>Only basic static site deployment is present; no evidence of automated testing, linting, or code quality checks in CI.</li> </ul> </li> <li> <p>Code Reuse &amp; DRY Principles:</p> <ul> <li>Some repeated logic (e.g., dynamic form handling, error messages) could be further abstracted for maintainability.</li> </ul> </li> </ul>","tags":["Frontend"]},{"location":"projects/test-site/#role-suitability","title":"Role Suitability","text":"","tags":["Frontend"]},{"location":"projects/test-site/#best-fit-roles","title":"Best Fit Roles","text":"<ul> <li> <p>Frontend Developer (Vanilla JS/HTML/CSS):</p> <ul> <li>Demonstrated ability to build interactive, modular, and responsive web UIs from scratch without frameworks.</li> </ul> </li> <li> <p>UI/UX Engineer (Prototype/Demo Level):</p> <ul> <li>Strong focus on user flows, feedback, and dynamic content for demo or MVP applications.</li> </ul> </li> <li> <p>Web Application Prototyper:</p> <ul> <li>Skilled at quickly assembling functional prototypes using client-side technologies and third-party libraries.</li> </ul> </li> </ul>","tags":["Frontend"]},{"location":"projects/test-site/#well-suited-for","title":"Well-Suited For","text":"<ul> <li> <p>Technical Writer/Documenter:</p> <ul> <li>Good documentation practices and clear code organization.</li> </ul> </li> <li> <p>Client-Side Data Engineer:</p> <ul> <li>Experience with data parsing, transformation, and storage in browser environments.</li> </ul> </li> </ul>","tags":["Frontend"]},{"location":"projects/test-site/#less-suited-for","title":"Less Suited For","text":"<ul> <li>Backend Developer / Full Stack Engineer:<ul> <li>No evidence of backend, API, or database design/implementation.</li> </ul> </li> <li>DevOps Engineer (Advanced):<ul> <li>Only basic CI/CD; lacks advanced automation, monitoring, or infrastructure-as-code.</li> </ul> </li> <li>Security Engineer:<ul> <li>No secure authentication, authorization, or data protection practices.</li> </ul> </li> <li>QA/Test Automation Engineer:<ul> <li>No automated testing or test infrastructure.</li> </ul> </li> </ul> <p>Summary: The developer demonstrates strong skills in vanilla JavaScript, modular frontend architecture, dynamic UI/UX, and client-side data handling. The codebase is well-documented, readable, and suitable for demo or prototype-level applications. However, there is a lack of automated testing, security best practices, backend integration, and advanced DevOps. The developer is best suited for roles focused on frontend development, rapid prototyping, and UI/UX engineering in client-side environments.</p>","tags":["Frontend"]},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/category/database/","title":"database","text":""}]}