{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"Intro Projects Blogs Skills About Me Contact","tags":["Welcome"]},{"location":"#welcome","title":"Hey there Hello..! Welcome to my Portfolio","text":"","tags":["Welcome"]},{"location":"#intro","title":"Hi, I'm Sunil Sharma","text":"<p>             I\u2019m a passionate Python Developer and Data Scientist who loves crafting innovative solutions and sharing my journey through engaging stories.         </p>","tags":["Welcome"]},{"location":"#explore","title":"I'm thrilled to have you here. Explore my projects, read my blogs, and get to know more about me.","text":"","tags":["Welcome"]},{"location":"#projects","title":"Featured Projects","text":"<p>Discover innovative projects where technology meets creativity.</p> Examination Management System DB <p>Multi-RDBMS exam/test management, automation, Docker, Python, CI-ready.</p> S3 Faker <p>Fake data generator with S3/LocalStack integration for cloud testing.</p> Paraxcel <p>Excel data extraction, transformation, and visualization toolkit.</p> Test Management Site <p>Frontend web app for test management and result tracking.</p> Explore Projects","tags":["Welcome"]},{"location":"#blogs","title":"Featured Blogs","text":"<p>Read my latest insights on development, data science, and tech trends.</p> blog 1 title <p>Short description of the project.</p> blog 2 title <p>Short description of the project.</p> blog 1 title <p>Short description of the project.</p> blog 2 title <p>Short description of the project.</p> Read Blogs","tags":["Welcome"]},{"location":"#skills","title":"Skills &amp; Expertise","text":"<p>Continuously expanding my technical horizons</p> Programming Languages <p>                     Python,                     JavaScript,                     SQL                 </p> Data Science <p>                     Machine Learning,                     Data Visualization,                     Statistical Analysis                 </p> Frameworks &amp; Libraries <p>                     React,                     TensorFlow,                     Pandas                 </p> Tools &amp; Technologies <p>                     Git,                     Docker,                     AWS                 </p>","tags":["Welcome"]},{"location":"#about","title":"About Me","text":"<p>Learn more about my journey, skills, and experiences in tech.</p>                  Download Resume                           Get to Know Me","tags":["Welcome"]},{"location":"#contact","title":"Let's Connect","text":"<p>Have a question or a collaboration idea? Let's connect!</p>                  GitHub                               LinkedIn                           More Ways to Connect","tags":["Welcome"]},{"location":"about/","title":"About Me","text":"<p>Hi, I'm Sunil Sharma, a passionate Python Developer and Data Scientist. I specialize in creating innovative solutions and sharing my knowledge through blogs. With a strong background in data analysis and machine learning, I strive to turn data into actionable insights.</p>"},{"location":"about/#resumecv","title":"Resume/CV","text":"Resume"},{"location":"about/#my-journey","title":"My Journey","text":""},{"location":"about/#education","title":"Education","text":"<ul> <li>Degree: Bachelor's in Computer Science</li> <li>University: Devi Ahilyabai Vishav Vidhyalay(DAVV), Indore</li> <li>Collage: PMB Gujarati Science College, Indore</li> <li>2015-2018</li> </ul>"},{"location":"about/#experience","title":"Experience","text":"<p>I have worked on various projects involving data analysis, machine learning, and web development. My goal is to leverage my skills to solve real-world problems and contribute to the tech community.</p> <ul> <li>Hackathon</li> <li>GSTN Hackathon | 2024</li> <li> <p>Developing a Binary Classification model on highly anonymized data.</p> </li> <li> <p>Academic Project</p> </li> <li>Company Name | Dates</li> <li>Describe your role and responsibilities.</li> </ul>"},{"location":"about/#explore","title":"Explore","text":"More Projects       More Blogs"},{"location":"about/#skills","title":"Skills","text":"<ul> <li>Python, JavaScript, HTML, CSS</li> <li>Data Analysis (Numpy, pandas)</li> <li>Machine Learning (Scikit-Learn, XGBoost)</li> <li>Web Development (Flask, Django, Fastapi)</li> <li>Data Visualization (Matplotlib, Seaborn)</li> <li>Web Scrapping (BeautifulSoup, Selenium)</li> <li>Ai &amp; Agentic space:</li> <li>Mistralai, Gemini</li> <li>Langchain, Langgraph</li> <li>Huggingface</li> <li>Database:</li> <li><code>SQL</code>: Sql, MySql, Postgresql</li> <li><code>NoSQL</code>: MongoDB, Redis</li> </ul>"},{"location":"about/#tools-dev-ops","title":"Tools &amp; Dev-Ops","text":"<ul> <li>Tools: VSCode, Notebook, Git, Github, Docker, Kubernetes</li> <li>OS: Windows, Linux</li> <li>cloud: AWS, Azure, GCP</li> </ul>"},{"location":"about/#hobbies-interests","title":"Hobbies &amp; Interests","text":"<ul> <li>Coding</li> <li>Blogging</li> <li>Reading</li> </ul>"},{"location":"contact/","title":"Contact","text":"<p>Feel free to reach out to me for any inquiries or collaborations.</p>"},{"location":"contact/#email","title":"Email","text":"<p>  Email </p>"},{"location":"contact/#social-links","title":"Social Links","text":"LinkedIn       GitHub       DockerHub       Twitter"},{"location":"resume/","title":"Resume/CV","text":""},{"location":"resume/#download-my-resumecv","title":"Download my Resume/CV","text":"Resume"},{"location":"resume/#know-more-about-me","title":"Know more about me","text":"Know More      <p>Paraxcel \u2013 Python GUI Application Developed a standalone desktop tool using Python, Tkinter, Pandas, and python-docx that converts multiple-choice questions from DOCX to Excel. Implemented logic to detect formatting-based correct answers, validated data using Pydantic, and distributed the tool as an <code>.exe</code> using PyInstaller.</p>"},{"location":"tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"tags/#tag:automation","title":"Automation","text":"<ul> <li>            Naukri Web Scraper          </li> </ul>"},{"location":"tags/#tag:data-analysis","title":"Data Analysis","text":"<ul> <li>            Naukri Web Scraper          </li> </ul>"},{"location":"tags/#tag:database-design","title":"Database Design","text":"<ul> <li>            Exam Management System Database          </li> </ul>"},{"location":"tags/#tag:database-management","title":"Database Management","text":"<ul> <li>            Exam Management System Database          </li> </ul>"},{"location":"tags/#tag:frontend","title":"Frontend","text":"<ul> <li>            Test Management Site          </li> </ul>"},{"location":"tags/#tag:python","title":"Python","text":"<ul> <li>            Naukri Web Scraper          </li> <li>            QA Docx to Excel          </li> </ul>"},{"location":"tags/#tag:selenium","title":"Selenium","text":"<ul> <li>            Naukri Web Scraper          </li> </ul>"},{"location":"tags/#tag:web-scraping","title":"Web Scraping","text":"<ul> <li>            Naukri Web Scraper          </li> </ul>"},{"location":"tags/#tag:welcome","title":"Welcome","text":"<ul> <li>            Welcome          </li> </ul>"},{"location":"blog/","title":"Blogs","text":"<p>Welcome to my blog! Here you will find my latest posts and updates.</p>"},{"location":"blog/#latest-blogs","title":"Latest Blogs","text":""},{"location":"blog/2025/05/25/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/","title":"Navigating the Nuances: A Developer's Guide to SQL Dialects (SQLite, MySQL, PostgreSQL)","text":"<p>As developers, we often encounter various SQL databases, each with its own flavor of <code>SQL</code>. While the core concepts remain similar, the devil is in the details \u2013 especially when it comes to schema definitions, data types, and procedural extensions like triggers.</p> <p>Recently, while working on an <code>Exam Management System (EMS)</code>, I had the opportunity to define the database schema for <code>SQLite</code>, <code>MySQL</code>, and <code>PostgreSQL</code>. This exercise highlighted some fascinating and crucial differences between these popular relational database management systems (RDBMS).</p> <p>Note : This post aims to serve as a practical guide and a bit of a cheatsheet, drawing directly from the <code>schema</code> files of project.</p> <p>Project Repo : ems-db <code>&lt;-- root-dir-name</code></p> <ul> <li>sqlite/schema.sql</li> <li>psql/schema.sql</li> <li>mysql/schema.sql</li> </ul> <p>Whether you're a fellow developer looking for a quick reference or trying to gauge a database differences, I hope this comparison proves insightful!</p>"},{"location":"blog/2025/05/25/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#key-areas-of-difference-schema","title":"Key Areas of Difference: Schema","text":"<p>Let's dive into the specific areas where these <code>SQL dialects</code> diverge:</p> <p>Drill</p> <ul> <li>Warning : Refer to <code>Official Docs</code>, when in doubt. <code>\"Its not ultimate source of truth. It could be good starting point.\"</code></li> <li>Understanding : Use project as reference.</li> <li>Prerequisites : Familiar with <code>sql syntax</code>, <code>client interaction</code>, <code>Docker</code>, and <code>Python</code> (language of choice).</li> </ul>"},{"location":"blog/2025/05/25/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#1-dropping-objects-tables-views-indexes","title":"1. Dropping Objects (Tables, Views, Indexes)","text":"<p>The syntax for dropping database objects is largely similar, but identifier quoting can vary.</p> <ul> <li> <p>SQLite &amp; PostgreSQL: Use double quotes for identifiers if they contain special characters or are case-sensitive (though often optional).</p> <pre><code>-- SQLite &amp; PostgreSQL\nDROP VIEW IF EXISTS \"tests_history\";\nDROP TABLE IF EXISTS \"students\";\n</code></pre> </li> <li> <p>MySQL: Uses backticks for identifiers.</p> <pre><code>-- MySQL\nDROP VIEW IF EXISTS `tests_history`;\nDROP TABLE IF EXISTS `students`;\n</code></pre> </li> </ul>"},{"location":"blog/2025/05/25/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#2-data-types-and-auto-incrementing-ids","title":"2. Data Types and Auto-Incrementing IDs","text":"<p>This is a significant area of divergence.</p> Feature SQLite PostgreSQL MySQL Auto-Increment ID <code>id INTEGER PRIMARY KEY</code> (implicitly <code>AUTOINCREMENT</code> if it's the primary key and integer type) or explicitly <code>id INTEGER PRIMARY KEY AUTOINCREMENT</code> <code>id SERIAL PRIMARY KEY</code> (creates a sequence) <code>id INT AUTO_INCREMENT PRIMARY KEY</code> Text <code>TEXT</code> <code>VARCHAR(n)</code>, <code>TEXT</code> <code>VARCHAR(n)</code>, <code>TEXT</code> Integer <code>INTEGER</code> <code>INTEGER</code>, <code>SMALLINT</code> (for <code>is_correct</code>, <code>score</code>) <code>INT</code>, <code>TINYINT(1)</code> (often for booleans) Boolean <code>INTEGER NOT NULL CHECK (\"is_correct\" IN (0, 1))</code> <code>SMALLINT NOT NULL DEFAULT 0 CHECK (\"is_correct\" IN (0, 1))</code> (or native <code>BOOLEAN</code>) <code>TINYINT(1) NOT NULL DEFAULT '0'</code> Date/Time <code>NUMERIC</code> (stored as text, real, or int), <code>DATETIME('now', 'localtime')</code> <code>TIMESTAMP WITH TIME ZONE</code>, <code>CURRENT_TIMESTAMP</code> <code>DATETIME</code>, <code>TIME</code>, <code>CURRENT_TIMESTAMP</code>, <code>NOW()</code> Duration/Interval <code>NUMERIC</code> (e.g., storing time as text 'HH:MM:SS') <code>INTERVAL</code> <code>TIME</code> (for durations within 24h) or calculate ENUM Types Simulated with <code>CHECK</code> constraint: <code>CHECK (\"status\" IN (...))</code> Native: <code>CREATE TYPE \"events_type\" AS ENUM (...);</code> Native: <code>status ENUM ('active', 'completed')</code> <p>Example: Students Table ID</p> <ul> <li> <p>SQLite:</p> <pre><code>CREATE TABLE \"students\" (\n    \"id\" INTEGER,\n    -- ...\n    PRIMARY KEY (\"id\")\n);\n</code></pre> </li> <li> <p>PostgreSQL:</p> <pre><code>CREATE TABLE IF NOT EXISTS \"students\" (\n    \"id\" SERIAL,\n    -- ...\n    PRIMARY KEY(\"id\")\n);\n</code></pre> </li> <li> <p>MySQL:</p> <pre><code>CREATE TABLE IF NOT EXISTS `students` (\n    `id` INT AUTO_INCREMENT,\n    -- ...\n    PRIMARY KEY (`id`)\n);\n</code></pre> </li> </ul> <p>Example: ENUM for <code>tests_sessions.status</code></p> <ul> <li> <p>SQLite:</p> <pre><code>CREATE TABLE \"tests_sessions\" (\n    -- ...\n    \"status\" TEXT NOT NULL DEFAULT 'in-progress' CHECK (\n        \"status\" IN ('in-progress', 'ended', 'completed')\n    )\n    -- ...\n);\n</code></pre> </li> <li> <p>PostgreSQL:</p> <pre><code>CREATE TYPE \"tests_session_status_type\" AS ENUM ('in-progress', 'ended', 'completed');\nCREATE TABLE IF NOT EXISTS \"tests_sessions\" (\n    -- ...\n    \"status\" \"tests_session_status_type\" NOT NULL DEFAULT 'in-progress',\n    -- ...\n);\n</code></pre> </li> <li> <p>MySQL:</p> <pre><code>CREATE TABLE IF NOT EXISTS `tests_sessions` (\n    -- ...\n    `status` ENUM ('in-progress', 'ended', 'completed') NOT NULL DEFAULT 'in-progress',\n    -- ...\n);\n</code></pre> </li> </ul>"},{"location":"blog/2025/05/25/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#3-default-values-especially-timestamps","title":"3. Default Values (Especially Timestamps)","text":"<ul> <li> <p>SQLite: Uses functions like <code>DATETIME('now', 'localtime')</code>.</p> <pre><code>\"start\" NUMERIC NOT NULL DEFAULT (DATETIME('now', 'localtime'))\n</code></pre> </li> <li> <p>PostgreSQL: Uses <code>CURRENT_TIMESTAMP</code>.</p> <pre><code>\"start\" TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT CURRENT_TIMESTAMP\n</code></pre> </li> <li> <p>MySQL: Uses <code>CURRENT_TIMESTAMP</code> or <code>NOW()</code>.</p> <pre><code>`start` DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP\n</code></pre> </li> </ul>"},{"location":"blog/2025/05/25/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#4-trigger-syntax","title":"4. Trigger Syntax","text":"<p>Triggers are where the syntactical differences become very pronounced.</p> <p>Common Goal: Set the <code>end</code> time of a <code>tests_sessions</code> row upon insertion, based on the test's duration.</p> <ul> <li> <p>SQLite:</p> <pre><code>CREATE TRIGGER \"set_end_for_test_session\" AFTER INSERT ON \"tests_sessions\"\nBEGIN\nUPDATE \"tests_sessions\"\nSET\n    \"end\" = DATETIME(new.start, '+' || (\n            SELECT TIME(duration)\n            FROM \"tests\" AS t\n            WHERE t.\"id\" = new.\"test_id\"\n        ))\nWHERE \"id\" = new.id;\nEND;\n</code></pre> <ul> <li>Uses <code>BEGIN...END;</code> block.</li> <li><code>AFTER INSERT</code>.</li> <li><code>new</code> refers to the inserted row.</li> <li>Date/time arithmetic involves string concatenation for modifiers.</li> </ul> </li> <li> <p>PostgreSQL:</p> <pre><code>CREATE OR REPLACE FUNCTION set_end_for_test_session_fn()\nRETURNS TRIGGER AS $$\nBEGIN\n    NEW.end := NEW.start + (\n        SELECT \"duration\" FROM \"tests\" WHERE \"id\" = NEW.test_id\n    );\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER \"set_end_for_test_session\" BEFORE INSERT ON\n\"tests_sessions\" FOR EACH ROW\nEXECUTE FUNCTION set_end_for_test_session_fn();\n</code></pre> <ul> <li>Requires a separate trigger function written in a procedural language (e.g., <code>plpgsql</code>).</li> <li><code>BEFORE INSERT</code> (can modify <code>NEW</code> directly).</li> <li><code>FOR EACH ROW</code> is explicit.</li> <li><code>NEW</code> is a record variable; assignments use <code>:=</code>.</li> <li><code>RETURN NEW</code> is crucial for <code>BEFORE</code> triggers.</li> <li>Interval arithmetic is more direct (<code>+ duration</code>).</li> </ul> </li> <li> <p>MySQL:</p> <pre><code>DELIMITER $$\nCREATE TRIGGER `set_end_for_test_session` BEFORE INSERT ON\n`tests_sessions` FOR EACH ROW\nBEGIN\n    SET NEW.end = DATE_ADD(\n        IFNULL(NEW.start, NOW()), -- NOW() if NEW.start is not yet set\n        INTERVAL (\n            SELECT TIME_TO_SEC(`duration`) / 60 FROM `tests` WHERE `id` = NEW.`test_id`\n        ) MINUTE\n    );\nEND$$\nDELIMITER ;\n</code></pre> <ul> <li>Uses <code>DELIMITER</code> to define a multi-statement trigger body.</li> <li><code>BEFORE INSERT</code>.</li> <li><code>FOR EACH ROW</code> is explicit.</li> <li><code>SET NEW.column = ...</code> for assignments.</li> <li>Uses functions like <code>DATE_ADD</code> and <code>TIME_TO_SEC</code> for time arithmetic.</li> </ul> </li> </ul> <p>Key Trigger Differences Summary:</p> Feature SQLite PostgreSQL MySQL Structure <code>BEGIN...END</code> directly in trigger Separate function + trigger definition <code>DELIMITER $$ BEGIN...END$$ DELIMITER ;</code> Timing <code>AFTER</code>/<code>BEFORE</code>/<code>INSTEAD OF</code> <code>AFTER</code>/<code>BEFORE</code>/<code>INSTEAD OF</code> <code>AFTER</code>/<code>BEFORE</code> Row Reference <code>new</code>, <code>old</code> <code>NEW</code>, <code>OLD</code> (case-sensitive in PL/pgSQL) <code>NEW</code>, <code>OLD</code> Modification <code>UPDATE</code> statement for <code>AFTER</code> Direct assignment to <code>NEW</code> in <code>BEFORE</code> trigger Direct assignment to <code>NEW</code> in <code>BEFORE</code> trigger Return Value N/A for <code>AFTER</code> <code>RETURN NEW</code>/<code>OLD</code>/<code>NULL</code> for <code>BEFORE</code> N/A"},{"location":"blog/2025/05/25/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#5-time-and-interval-arithmetic","title":"5. Time and Interval Arithmetic","text":"<p>As seen in the trigger examples, how you add durations to timestamps varies:</p> <ul> <li> <p>SQLite: String manipulation with <code>DATETIME</code> function modifiers.</p> <pre><code>DATETIME(new.start, '+' || (SELECT TIME(duration) ...))\n</code></pre> </li> <li> <p>PostgreSQL: Direct arithmetic with <code>INTERVAL</code> types.</p> <pre><code>NEW.start + (SELECT \"duration\" FROM \"tests\" ...)\n</code></pre> </li> <li> <p>MySQL: Functions like <code>DATE_ADD()</code> and <code>INTERVAL</code> keyword.</p> <pre><code>DATE_ADD(NEW.start, INTERVAL X MINUTE) -- or HOUR, SECOND etc.\n</code></pre> <p>In my schema, I converted the <code>TIME</code> duration to seconds, then to minutes for <code>DATE_ADD</code>:</p> <pre><code>INTERVAL (SELECT TIME_TO_SEC(`duration`) / 60 FROM `tests` WHERE `id` = NEW.`test_id`) MINUTE\n</code></pre> </li> </ul>"},{"location":"blog/2025/05/25/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#6-conditional-logic-in-triggersqueries","title":"6. Conditional Logic in Triggers/Queries","text":"<ul> <li> <p>SQLite &amp; MySQL: <code>CASE WHEN ... THEN ... ELSE ... END</code> is standard.     SQLite example from <code>set_score_of_result</code> trigger:</p> <pre><code>\"feedback\" = CASE WHEN (...) = 0 THEN 'need-improvement' ELSE 'great' END\n</code></pre> <p>MySQL example from <code>set_score_of_result</code> trigger:</p> <pre><code>SET NEW.feedback = CASE WHEN (...) = 0 THEN 'need-improvement' ELSE 'great' END;\n</code></pre> </li> <li> <p>PostgreSQL: Supports <code>CASE</code> expressions, but also <code>IF...THEN...ELSE...END IF;</code> in PL/pgSQL functions.     PostgreSQL example from <code>set_score_of_result_fn</code> trigger function:</p> <pre><code>IF NEW.score = 0 THEN\n    NEW.feedback := 'need-improvement';    \nELSE\n    NEW.feedback := 'great';\nEND IF;\n</code></pre> </li> </ul>"},{"location":"blog/2025/05/25/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#7-handling-nulls-in-aggregate-functions","title":"7. Handling NULLs in Aggregate Functions","text":"<p>When summing scores, if no results exist for a test session, <code>SUM()</code> might return <code>NULL</code>.</p> <ul> <li> <p>SQLite: <code>SUM()</code> on an empty set returns <code>NULL</code>. My schema doesn't explicitly handle this for <code>final_score</code> in the trigger, which might be an oversight if a report could be generated before any results.</p> <pre><code>(SELECT SUM(\"results\".\"score\") FROM \"results\" WHERE \"results\".\"test_session_id\" = new.id)\n</code></pre> </li> <li> <p>PostgreSQL: Uses <code>COALESCE(SUM(\"score\"), 0)</code> to default to <code>0</code> if <code>SUM</code> is <code>NULL</code>.</p> <pre><code>(SELECT COALESCE(SUM(\"score\"),0) FROM \"results\" WHERE \"test_session_id\" = NEW.id)\n</code></pre> </li> <li> <p>MySQL: Uses <code>IFNULL(SUM(\\</code>score`), 0)` for the same purpose.</p> <pre><code>(SELECT IFNULL(SUM(`score`),0) FROM `results` WHERE `test_session_id` = NEW.id)\n</code></pre> </li> </ul>"},{"location":"blog/2025/05/25/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#8-create-table-if-not-exists","title":"8. <code>CREATE TABLE IF NOT EXISTS</code>","text":"<p>This useful clause prevents errors if a table already exists.</p> <ul> <li>SQLite: Supports <code>CREATE TABLE IF NOT EXISTS \"students\" (...)</code> (though not explicitly used in the provided <code>students</code> table creation, it's standard).</li> <li>PostgreSQL: <code>CREATE TABLE IF NOT EXISTS \"students\" (...)</code></li> <li>MySQL: <code>CREATE TABLE IF NOT EXISTS `students` (...)</code></li> </ul>"},{"location":"blog/2025/05/25/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#9-comments","title":"9. Comments","text":"<ul> <li>SQLite, PostgreSQL, MySQL: All support <code>--</code> for single-line comments.</li> <li>MySQL: Also supports <code>#</code> for single-line comments.</li> </ul>"},{"location":"blog/2025/05/25/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#10-index-creation","title":"10. Index Creation","text":"<p>The basic syntax is similar, but quoting and specific features (like conditional indexing) can differ.</p> <ul> <li> <p>SQLite &amp; PostgreSQL:</p> <pre><code>-- SQLite &amp; PostgreSQL\nCREATE INDEX \"idx_tests\" ON \"tests\" (\"title\");\n-- PostgreSQL specific (SQLite also supports WHERE in index but syntax might differ slightly)\nCREATE INDEX \"idx_questions_options_is_correct\" ON \"questions_options\" (\"is_correct\") WHERE \"is_correct\" = 1;\n</code></pre> </li> <li> <p>MySQL:</p> <pre><code>-- MySQL\nCREATE INDEX `idx_tests` ON `tests` (`title`);\n-- MySQL does not directly support WHERE clauses in CREATE INDEX like PostgreSQL/SQLite.\n-- For conditional indexing, you might index the column and rely on the optimizer,\n-- or use generated columns if applicable.\nCREATE INDEX `idx_questions_options_is_correct` ON `questions_options` (`is_correct`);\n</code></pre> </li> </ul>"},{"location":"blog/2025/05/25/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#11-time-zone-handling","title":"11. Time Zone Handling","text":"<ul> <li>SQLite: <code>DATETIME('now', 'localtime')</code> attempts to use local time. Time storage is less strict.</li> <li>PostgreSQL: Very robust. <code>TIMESTAMP WITH TIME ZONE</code> stores timestamps in UTC and converts them to the client's/session's time zone on retrieval. <code>SET TIME ZONE LOCAL;</code> can be used.</li> <li>MySQL: <code>DATETIME</code> stores \"wall clock\" time without time zone info. <code>TIMESTAMP</code> converts from current time zone to UTC for storage, and back on retrieval. Session time zone can be set.</li> </ul>"},{"location":"blog/2025/05/25/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#quick-cheatsheet-sqlite-vs-postgresql-vs-mysql","title":"Quick Cheatsheet: SQLite vs. PostgreSQL vs. MySQL","text":"Feature SQLite PostgreSQL MySQL Identifier Quoting <code>\"optional\"</code> <code>\"optional/case-sensitive\"</code> <code>`optional`</code> Auto Increment <code>INTEGER PRIMARY KEY [AUTOINCREMENT]</code> <code>SERIAL</code> or <code>IDENTITY</code> <code>INT AUTO_INCREMENT</code> Data Types (General) Flexible typing (TEXT, NUMERIC, INTEGER) Strict, rich types (VARCHAR, TEXT, INT, BIGINT, BOOLEAN, JSON, ARRAY, INTERVAL, ENUM) Strict types (VARCHAR, TEXT, INT, TINYINT, DATETIME, ENUM, JSON) ENUMs <code>CHECK</code> constraint <code>CREATE TYPE ... AS ENUM</code> <code>ENUM(...)</code> column type Triggers <code>BEGIN...END</code> Function-based (<code>CREATE FUNCTION ... EXECUTE FUNCTION</code>) <code>DELIMITER $$ BEGIN...END$$</code> Default Timestamp <code>DATETIME('now', 'localtime')</code> <code>CURRENT_TIMESTAMP</code> <code>CURRENT_TIMESTAMP</code> / <code>NOW()</code> Interval Arithmetic String manipulation with <code>datetime()</code> <code>+ INTERVAL '...'</code> <code>DATE_ADD(date, INTERVAL value unit)</code> Function for NULLs <code>IFNULL(val, default)</code> (or <code>COALESCE</code>) <code>COALESCE(val, default)</code> <code>IFNULL(val, default)</code> or <code>COALESCE(val, default)</code>"},{"location":"blog/2025/05/25/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#why-this-matters","title":"Why This Matters","text":"<p>For Developers:</p> <ul> <li>Adaptability: Understanding these differences allows you to switch between database systems more fluidly.</li> <li>Debugging: Syntax errors are common when moving SQL code; knowing the nuances helps pinpoint issues faster.</li> <li>Database Design: Choosing the right data types and features (like native ENUMs or interval types) can lead to a more efficient and maintainable schema.</li> <li>ORM Configuration: When using Object-Relational Mappers (ORMs), these differences are often abstracted, but knowing what's happening under the hood is invaluable for optimization and complex queries.</li> </ul> <p>Learnings:</p> <ul> <li>Depth of Understanding: Articulating these differences helps demonstrating a deeper-than-surface-level understanding of SQL and database systems.</li> <li>Practical Experience: It often indicates hands-on experience with multiple databases, which is a valuable asset in diverse tech environments.</li> <li>Problem-Solving: The ability to adapt a schema or queries for different SQL dialects showcases problem-solving skills.</li> </ul>"},{"location":"blog/2025/05/25/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#next-read","title":"Next Read","text":"<p>For Debunking <code>sql queries</code> and <code>clients interaction</code> differences b/w SQLite, MySQL, and PostgreSQL,</p> <ul> <li><code>Part-2</code> Beyond the Schema: A Practical Guide to Querying and Interacting with SQLite, MySQL, &amp; PostgreSQL</li> </ul> <p>Note : It's build upon where this post left.</p>"},{"location":"blog/2025/05/25/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#conclusion","title":"Conclusion","text":"<p>While <code>SQL</code> is a \"standard,\" its implementations across different RDBMSs like SQLite, MySQL, and PostgreSQL have distinct personalities.</p> <p>The journey of creating a consistent schema for my <code>EMS project</code> across these three was a great learning experience. Remember, always check the documentation for the specific dialect you're working with.</p> <p>I hope this comparative overview helps you in your database endeavors! Happy coding!</p> <p>Disclaimer : The examples are drawn from specific schema files and might not cover every single difference or advanced feature of each RDBMS.</p>"},{"location":"blog/2025/05/25/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#references-resources","title":"References &amp; Resources","text":"<p>This section compiles useful links found within the <code>ems-db</code> project's documentation (<code>usage.md</code>, <code>README.Docker.md</code> files), categorized for easier navigation.</p>"},{"location":"blog/2025/05/25/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#general","title":"General","text":"<p>CS50 SQL Notes (General Syntax Differences):</p> <ul> <li>MySQL Differences</li> <li>PostgreSQL Differences</li> </ul>"},{"location":"blog/2025/05/25/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#sqlite","title":"SQLite","text":"<ul> <li>SQL As Understood By SQLite</li> <li>Python <code>sqlite3</code> Module</li> </ul>"},{"location":"blog/2025/05/25/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#postgresql","title":"PostgreSQL","text":"<ul> <li>Postgres SQL Commands</li> <li>Postgres Data Types</li> <li>Postgres Date and Time Functions</li> <li>Postgres Triggers and Trigger Functions</li> </ul>"},{"location":"blog/2025/05/25/navigating-the-nuances-a-developers-guide-to-sql-dialects-sqlite-mysql-postgresql/#mysql","title":"MySQL","text":"<ul> <li>Using Data Types from Other Database Engines</li> <li>Date and Time Functions</li> <li>Data Definition Statements</li> <li>Docker MySQL Basic Steps</li> </ul>"},{"location":"blog/2025/05/25/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/","title":"Beyond the Schema: A Practical Guide to Querying and Interacting with SQLite, MySQL, &amp; PostgreSQL","text":"<p>Okay, building on our previous discussion about SQL schema differences, let's dive into how we interact with <code>SQLite</code>, <code>MySQL</code>, and <code>PostgreSQL</code>, focusing on query execution, CLI usage, and connection methods.</p> <p>This companion blog post will use the <code>queries.sql</code>, <code>README.Docker.md</code>, and <code>usage.md</code> files from the Exam Management System (EMS) project as our practical examples.</p> <ul> <li>Project Repo : ems-db <code>&lt;-- root-dir-name</code></li> </ul> <p>If you missed the first part on schema definitions, you can catch up here:</p> <ul> <li>Navigating the Nuances: A Developer's Guide to SQL Dialects (SQLite, MySQL, PostgreSQL).</li> </ul> <p>This post will serve as another handy reference, highlighting the practical differences you'll encounter when running queries and managing these databases, especially useful for both day-to-day development and for showcasing practical database skills.</p> <p>Drill</p> <ul> <li>Warning : Refer to <code>Official Docs</code>, when in doubt. <code>\"Its not ultimate source of truth. It could be good starting point.\"</code></li> <li>Understanding : Use this project as reference.</li> <li>Prerequisites : Familiar with <code>sql syntax</code>, <code>client interaction</code>, <code>Docker</code>, and <code>Python</code> (language of choice).</li> </ul> <p>In our previous post, we explored the key differences in schema definitions across <code>SQLite</code>, <code>MySQL</code>, and <code>PostgreSQL</code> using the Exam Management System (EMS) project as a case study.</p> <p>Now, let's shift our focus to the equally important aspects of how we interact with these databases: running queries, using their command-line interfaces (CLIs), and understanding connection nuances, especially in a Dockerized environment.</p> <p>This guide draws insights from the following project files (within the ems-db repository):</p> <ul> <li> <p>Query Scripts:</p> <ul> <li>sqlite/queries.sql</li> <li>psql/queries.sql</li> <li>mysql/queries.sql</li> </ul> </li> <li> <p>Usage &amp; Docker Documentation:</p> <ul> <li>sqlite/usage.md &amp; sqlite/README.Docker.md</li> <li>psql/usage.md &amp; psql/README.Docker.md</li> <li>mysql/usage.md &amp; mysql/README.Docker.md</li> </ul> </li> </ul> <p>Understanding these practical differences can significantly boost your efficiency and adaptability as a developer.</p>"},{"location":"blog/2025/05/25/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#key-areas-of-difference-queries-interaction","title":"Key Areas of Difference: Queries &amp; Interaction","text":""},{"location":"blog/2025/05/25/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#1-cli-shell-access-connection","title":"1. CLI Shell Access &amp; Connection","text":"<p>Each database has its own command-line tool for direct interaction.</p> <ul> <li> <p>SQLite:</p> <ul> <li>Command: <code>sqlite3 ems.db [options]</code></li> <li>Example from <code>sqlite/usage.md</code>: <code>sqlite3 ems.db -table -echo</code><ul> <li><code>-table</code>: Sets output mode to table format.</li> <li><code>-echo</code>: Prints commands before execution.</li> </ul> </li> <li>Connection is file-based; you specify the database file path.</li> </ul> </li> <li> <p>PostgreSQL:</p> <ul> <li>Command: <code>psql [options] [dbname] [username]</code></li> <li>Example from <code>psql/usage.md</code>: <code>psql -a -b ems postgres</code><ul> <li><code>-a</code>: Echoes all input from script.</li> <li><code>-b</code>: Echoes failed commands.</li> <li><code>ems</code>: Database name.</li> <li><code>postgres</code>: Username.</li> </ul> </li> <li>In Docker, from an <code>app</code> service: <code>psql -h db ems postgres</code> (where <code>db</code> is the service name of the PostgreSQL container).</li> <li>PostgreSQL's <code>README.Docker.md</code> also mentions using <code>~/.pgpass</code> for passwordless connections in development environments.</li> </ul> </li> <li> <p>MySQL:</p> <ul> <li>Command: <code>mysql [options] -u[user] -p[password] [dbname]</code></li> <li>Example from <code>mysql/usage.md</code>: <code>mysql -t -v -uroot -psecret ems</code><ul> <li><code>-t</code>: Output in table format.</li> <li><code>-v</code>: Verbose mode.</li> <li><code>-uroot -psecret</code>: Username and password.</li> <li><code>ems</code>: Database name (as defined in <code>compose.yml</code> <code>MYSQL_DATABASE</code> env var).</li> </ul> </li> <li>MySQL's <code>README.Docker.md</code> also mentions <code>mysqlsh</code> as a more powerful alternative shell, aliased as <code>mysql</code> in the <code>app</code> service. Often used for connecting with <code>cloud native mysql server</code> from <code>client machines</code>.</li> </ul> </li> </ul> <p>\ud83d\udd0d Tip: All three databases differ significantly in how they let you inspect objects (like tables, views, indexes) from shell clients\u2014see section 5 and beyond.</p>"},{"location":"blog/2025/05/25/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#2-executing-sql-scripts-from-files","title":"2. Executing SQL Scripts from Files","text":"<p>Running a series of SQL commands from a <code>.sql</code> file is a common task.</p> <ul> <li>SQLite:<ul> <li>Shell command: <code>.read ./queries.sql</code></li> <li>CLI redirection: <code>sqlite3 ems.db -table -echo &lt; ./queries.sql</code></li> </ul> </li> </ul> <p>As seen in <code>sqlite/usage.md</code> and <code>sqlite/queries.sql</code></p> <ul> <li>PostgreSQL:<ul> <li>Shell command: <code>\\i ./queries.sql</code></li> <li>CLI redirection: <code>psql -a -b ems postgres &lt; ./queries.sql</code></li> </ul> </li> </ul> <p>As seen in <code>psql/usage.md</code> and <code>psql/queries.sql</code></p> <ul> <li>MySQL:<ul> <li>Shell command: <code>source ./queries.sql</code></li> <li>CLI redirection: <code>mysql -tv -uroot -psecret ems &lt; ./queries.sql</code></li> </ul> </li> </ul> <p>\ud83e\udde9 Important: When schema files contain stored procedures, triggers, or functions that require <code>DELIMITER</code>, executing them inside the <code>mysql</code> CLI is more reliable than using <code>mysql-connector-python</code> (which doesn't support <code>DELIMITER</code>). This limitation makes shell execution the preferred approach for complex DDL.</p> <p>As seen in <code>mysql/usage.md</code> and <code>mysql/queries.sql</code></p>"},{"location":"blog/2025/05/25/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#3-resetting-auto-increment-values","title":"3. Resetting Auto-Increment Values","text":"<p>After clearing tables (e.g., with <code>DELETE FROM table;</code>), you often want to reset auto-increment counters for primary keys, especially during development or testing.</p> <ul> <li> <p>SQLite:</p> <ul> <li>If <code>AUTOINCREMENT</code> keyword is used on an <code>INTEGER PRIMARY KEY</code> column, SQLite uses an internal table <code>sqlite_sequence</code>.<ul> <li>To reset: <code>DELETE FROM sqlite_sequence WHERE name='your_table_name';</code></li> </ul> </li> <li>The <code>sqlite/queries.sql</code> file simply uses <code>DELETE FROM students;</code>. If <code>AUTOINCREMENT</code> was not explicitly used (as in the <code>students</code> table in the provided schema), SQLite might reuse IDs from deleted rows. For a true reset, the <code>sqlite_sequence</code> table would need to be managed if <code>AUTOINCREMENT</code> was present.</li> </ul> </li> <li> <p>PostgreSQL:</p> <ul> <li>Uses sequences. The <code>SERIAL</code> type automatically creates a sequence suffixed with <code>_id_seq</code>.</li> <li>Command from <code>psql/queries.sql</code>: <code>ALTER SEQUENCE students_id_seq RESTART WITH 1;</code></li> </ul> </li> <li> <p>MySQL:</p> <ul> <li>Command from <code>mysql/queries.sql</code>: <code>ALTER TABLE students AUTO_INCREMENT = 1;</code></li> </ul> </li> </ul>"},{"location":"blog/2025/05/25/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#4-data-manipulation-language-dml-snippets","title":"4. Data Manipulation Language (DML) Snippets","text":"<p>The basic syntax for <code>INSERT</code>, <code>UPDATE</code>, and <code>DELETE</code> is highly standardized. The <code>queries.sql</code> files for all three databases demonstrate this:</p> <ul> <li> <p>INSERT:</p> <pre><code>-- Common across SQLite, PostgreSQL, MySQL (quoting may vary per schema)\nINSERT INTO students (first_name, last_name, password, email)\nVALUES ('John', 'Doe', 'password123', 'john.doe@example.com');\n</code></pre> </li> <li> <p>UPDATE:</p> <pre><code>-- Common across SQLite, PostgreSQL, MySQL (quoting may vary per schema)\nUPDATE tests_sessions\nSET status = 'completed'\nWHERE id = 1;\n</code></pre> </li> <li> <p>DELETE (Clearing a table):</p> <pre><code>-- Common across SQLite, PostgreSQL, MySQL\nDELETE FROM reports;\n</code></pre> </li> </ul> <p>Identifier quoting : <code>Double quotes for SQLite/PostgreSQL</code>, <code>backticks for MySQL</code> discussed in the schema blog post also applies here.</p>"},{"location":"blog/2025/05/25/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#5-querying-data-analysis-select-explain","title":"5. Querying Data &amp; Analysis (SELECT, EXPLAIN)","text":"<p>Standard <code>SELECT</code> statements with <code>JOINs</code>, <code>WHERE</code> clauses, and subqueries are largely portable.</p> <ul> <li> <p>Example SELECT (from <code>sqlite/queries.sql</code>, similar in others):</p> <pre><code>SELECT *\nFROM tests_history\nWHERE student_id = (\n    SELECT id\n    FROM students\n    WHERE email = 'john.doe@example.com'\n);\n</code></pre> </li> <li> <p>Inspecting Tables/Views/Indexes (within shell clients):</p> <ul> <li> <p>SQLite:</p> <ul> <li>List tables: <code>.tables</code></li> <li>View DDL: <code>.schema table_name</code> or full schema: <code>.fullschema</code></li> <li>List indexes: <code>SELECT name FROM sqlite_master WHERE type='index';</code></li> </ul> </li> <li> <p>PostgreSQL:</p> <ul> <li>List tables: <code>\\dt</code></li> <li>View indexes: <code>\\di</code></li> <li>View DDL: <code>\\d+ table_name</code> or for indexes/views: <code>\\d+ index_name</code>, <code>\\d+ view_name</code></li> <li>SQL alternatives:</li> </ul> <pre><code>SELECT * FROM information_schema.tables WHERE table_schema='public';\nSELECT * FROM pg_indexes WHERE schemaname = 'public';\n</code></pre> </li> <li> <p>MySQL:</p> <ul> <li>List tables: <code>SHOW TABLES;</code></li> <li>List indexes: <code>SHOW INDEX FROM table_name;</code></li> <li>List views:</li> </ul> <pre><code>SHOW FULL TABLES WHERE TABLE_TYPE = 'VIEW';\nSHOW CREATE VIEW view_name;\n</code></pre> <ul> <li>SQL alternatives via <code>information_schema</code>:</li> </ul> <pre><code>SELECT * FROM information_schema.tables WHERE table_schema = 'your_db';\nSELECT * FROM information_schema.statistics WHERE table_schema = 'your_db';\n</code></pre> </li> </ul> </li> <li> <p>Query Plan Analysis:</p> <ul> <li>SQLite: <code>EXPLAIN QUERY PLAN SELECT ...;</code> (as used in comments in <code>sqlite/queries.sql</code> <code>#Line-151 or below</code>).</li> <li>PostgreSQL &amp; MySQL: <code>EXPLAIN SELECT ...;</code> (This is the standard command, though not explicitly run in the provided <code>queries.sql</code> files for PSQL/MySQL, it's the common way to analyze queries for better indexing and performance optimizations).</li> </ul> </li> </ul>"},{"location":"blog/2025/05/25/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#6-dialect-specific-functionscommands-in-queries","title":"6. Dialect-Specific Functions/Commands in Queries","text":"<p>While core SQL is similar, some functions or commands are unique.</p> <ul> <li>SQLite: Uses functions like <code>STRFTIME()</code> and <code>DATETIME()</code> for date/time manipulations (more prominent in its <code>schema.sql</code>) triggers).</li> <li>PostgreSQL: Rich set of functions; <code>INTERVAL</code> arithmetic is a key feature (seen in its <code>schema.sql</code> triggers). The <code>queries.sql</code> uses standard SQL.</li> <li>MySQL:<ul> <li><code>SLEEP(seconds)</code>: Used in <code>mysql/queries.sql</code> (<code>SELECT SLEEP(3);</code>) to pause execution, often for testing or simulation.</li> <li><code>TIMEDIFF()</code>: Used in its <code>schema.sql</code> trigger.</li> </ul> </li> </ul>"},{"location":"blog/2025/05/25/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#7-error-handling-diagnostics-clisql","title":"7. Error Handling &amp; Diagnostics (CLI/SQL)","text":"<ul> <li> <p>SQLite:</p> <ul> <li>CLI: <code>-echo</code> flag helps trace execution.</li> </ul> </li> <li> <p>PostgreSQL:</p> <ul> <li>CLI: <code>-a</code> (echo all) and <code>-b</code> (echo errors) flags.</li> <li>Shell: <code>\\set ON_ERROR_STOP on</code> can be useful in scripts.</li> </ul> </li> <li> <p>MySQL:</p> <ul> <li>SQL Commands: <code>SHOW ERRORS;</code> and <code>SHOW WARNINGS;</code> are explicitly used in <code>mysql/queries.sql</code> to check for issues after operations.</li> <li>CLI: <code>-v</code> (verbose) flag.</li> </ul> </li> </ul>"},{"location":"blog/2025/05/25/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#8-exiting-shells","title":"8. Exiting Shells","text":"<ul> <li>SQLite: <code>.exit</code> or <code>.quit</code></li> <li>PostgreSQL: <code>\\q</code> or <code>exit</code></li> <li>MySQL: <code>\\q</code> or <code>exit</code> (or <code>quit</code>)</li> </ul>"},{"location":"blog/2025/05/25/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#9-python-script-interaction-brief","title":"9. Python Script Interaction (Brief)","text":"<p>The <code>usage.md</code> files for each database mention running a <code>db.py</code> script (e.g., <code>uv run db.py</code>). While this post focuses on CLI interaction, it's important to note that these databases are typically accessed programmatically via Python using libraries:</p> <ul> <li>SQLite: <code>sqlite3</code> (standard library)</li> <li>PostgreSQL: <code>psycopg2</code> or <code>psycopg</code> (third-party)</li> <li>MySQL: <code>mysql-connector-python</code> or <code>PyMySQL</code> (third-party) These libraries handle connection and query execution, abstracting some dialect specifics but still requiring correct SQL syntax for the target database.</li> </ul> <p>MySQL Note: As mentioned earlier, MySQL's <code>mysql-connector-python</code> does not support the <code>DELIMITER</code> command needed for procedures/triggers. This makes CLI-based execution of <code>schema.sql</code> safer and more portable.</p>"},{"location":"blog/2025/05/25/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#10-docker-environment-nuances","title":"10. Docker Environment Nuances","text":"<p>The <code>README.Docker.md</code> files highlight how Docker simplifies setup and interaction:</p> <ul> <li>Universal Access: <code>docker compose exec [service_name] bash</code> provides a shell within the container, from which you can then launch the respective database CLI.<ul> <li>SQLite: <code>docker compose exec app bash</code> then <code>sqlite3 ems.db</code></li> <li>PostgreSQL: <code>docker compose exec db bash</code> then <code>psql ...</code> or <code>docker compose exec app bash</code> then <code>psql -h db ...</code></li> <li>MySQL: <code>docker compose exec db bash</code> then <code>mysql ...</code> or <code>docker compose exec app bash</code> then <code>mysql ...</code> (often <code>mysqlsh</code> aliased as <code>mysql</code>).</li> <li>Service Discovery: For PostgreSQL and MySQL, the database often runs in a service named <code>db</code> (as defined in <code>compose.yml</code>), accessible from an <code>app</code> service using this hostname (e.g., <code>psql -h db ...</code>).</li> </ul> </li> <li>Pre-configured Environments: Docker setups often pre-configure users, passwords, and databases (e.g., <code>MYSQL_ROOT_PASSWORD</code>, <code>POSTGRES_USER</code>, <code>POSTGRES_DB</code> environment variables in <code>compose.yml</code>).</li> <li>SQL Dump Mounting: <code>README.Docker.md</code> for PostgreSQL and MySQL mentions mounting SQL dumps for pre-seeding data, which automates schema creation and initial data insertion on container startup.</li> <li>GUI Tools:<ul> <li>PostgreSQL: <code>adminer</code> service often included for web-based DB management.</li> <li>MySQL: <code>phpmyadmin</code> service often included.</li> </ul> </li> </ul>"},{"location":"blog/2025/05/25/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#quick-cheatsheet-query-interaction","title":"Quick Cheatsheet: Query &amp; Interaction","text":"Feature SQLite PostgreSQL MySQL CLI Tool <code>sqlite3</code> <code>psql</code> <code>mysql</code>, <code>mysqlsh</code>(for client) Connect (Example) <code>sqlite3 ems.db</code> <code>psql -U user -d dbname</code> <code>mysql -u user -p pass dbname</code> Run SQL File (Shell) <code>.read file.sql</code> <code>\\i file.sql</code> <code>source file.sql</code> Run SQL File (CLI) <code>sqlite3 db &lt; file.sql</code> <code>psql ... &lt; file.sql</code> <code>mysql ... &lt; file.sql</code> Reset Auto-Increment <code>DELETE FROM sqlite_sequence WHERE name='tbl';</code> (if <code>AUTOINCREMENT</code> used) <code>ALTER SEQUENCE seq_name RESTART WITH 1;</code> <code>ALTER TABLE tbl AUTO_INCREMENT = 1;</code> Query Plan <code>EXPLAIN QUERY PLAN ...</code> <code>EXPLAIN ...</code> <code>EXPLAIN ...</code> Show Errors (SQL) N/A (check return codes/messages) N/A (check messages, <code>ON_ERROR_STOP</code>) <code>SHOW ERRORS;</code>, <code>SHOW WARNINGS;</code> Exit Shell <code>.exit</code>, <code>.quit</code> <code>\\q</code>, <code>exit</code> <code>\\q</code>, <code>exit</code>, <code>quit</code> Docker Exec (App) <code>docker compose exec app sqlite3 ...</code> <code>docker compose exec app psql -h db ...</code> <code>docker compose exec app mysql -h db ...</code> Docker Exec (DB) <code>docker compose exec db sqlite3 ...</code> <code>docker compose exec db psql ...</code> <code>docker compose exec db mysql ...</code> View Tables (Shell) <code>.tables</code> <code>\\dt</code> <code>SHOW TABLES;</code> View Indexes (Shell) Query <code>sqlite_master</code> <code>\\di</code> <code>SHOW INDEX FROM tbl;</code>"},{"location":"blog/2025/05/25/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#why-this-matters","title":"Why This Matters","text":"<p>For Developers:</p> <ul> <li>Efficiency: Knowing the right CLI commands, flags, and shell directives saves significant time during development and debugging.</li> <li>Scripting &amp; Automation: Understanding how to execute SQL files and manage database states (like resetting sequences) is crucial for automated testing and deployment.</li> <li>Tooling: Familiarity with Docker interaction patterns and GUI tools enhances the development workflow.</li> <li>Debugging: Using <code>EXPLAIN</code> and error-checking commands helps optimize queries and troubleshoot issues effectively.</li> </ul> <p>Learning:</p> <ul> <li>Practical Skills: These interaction nuances demonstrate hands-on experience beyond theoretical SQL knowledge.</li> <li>Versatility: Comfort with different database CLIs and Docker environments indicates adaptability.</li> <li>Problem-Solving: The ability to diagnose query performance or script execution issues points to strong troubleshooting skills.</li> </ul>"},{"location":"blog/2025/05/25/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#conclusion","title":"Conclusion","text":"<p>Mastering the art of SQL goes beyond writing <code>SELECT</code> statements. It encompasses how you connect to your database, execute scripts, analyze performance, and manage its state through various tools and environments. As demonstrated by the <code>Exam Management System (ems-db)</code> project's supporting files, each database system\u2014SQLite, MySQL, and PostgreSQL\u2014offers a slightly different, yet powerful, set of tools and commands for these tasks.</p> <p>By familiarizing yourself with these practical aspects, you become a more well-rounded and effective data professional.</p> <p>Happy querying!</p> <p>Disclaimer: The examples are drawn from specific project files and general knowledge. Always refer to the official documentation for the most comprehensive and up-to-date information.</p>"},{"location":"blog/2025/05/25/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#references-resources","title":"References &amp; Resources","text":"<p>This section compiles useful links found within the <code>ems-db</code> project's documentation (<code>usage.md</code>, <code>README.Docker.md</code> files), categorized for easier navigation.</p>"},{"location":"blog/2025/05/25/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#general","title":"General","text":"<ul> <li>Docker:<ul> <li>Docker Manuals</li> <li>Docker Cheatsheet</li> <li>Docker's Python guide</li> <li>Docker Quick workshop</li> <li>Compose Getting Started</li> <li>Compose Volumes</li> <li>Use Compose Watch</li> </ul> </li> <li>UV (Python Packager):<ul> <li>UV: Working on projects</li> <li>UV: Docker Integration</li> </ul> </li> <li>CS50 SQL Notes (General Syntax Differences):<ul> <li>MySQL Differences</li> <li>PostgreSQL Differences</li> </ul> </li> </ul>"},{"location":"blog/2025/05/25/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#sqlite","title":"SQLite","text":"<ul> <li>SQLite CLI Commands</li> <li>SQL As Understood By SQLite</li> <li>Python <code>sqlite3</code> Module</li> <li>SQLite Download Page</li> </ul>"},{"location":"blog/2025/05/25/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#postgresql","title":"PostgreSQL","text":"<ul> <li>Client &amp; Server Documentation:<ul> <li>Postgres Client Reference</li> <li>Postgres SQL Commands</li> <li>Postgres Date and Time Functions</li> <li>Postgres Environment Variables</li> <li>Postgres Connection Strings</li> <li>Postgres Passwords File (<code>.pgpass</code>)</li> </ul> </li> <li>Python Driver: psycopg3 Documentation</li> <li>Docker Hub: Postgres Image</li> <li>Postgres Download Page</li> </ul>"},{"location":"blog/2025/05/25/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#mysql","title":"MySQL","text":"<ul> <li>MySQL Documentation:<ul> <li>MySQL SHOW Commands</li> <li>Date and Time Functions</li> <li>Docker MySQL Deployment Topics</li> <li>mysqlsh Shell Startup</li> </ul> </li> <li>Python Driver: mysql-connector-python</li> <li>Docker Hub: MySQL Image</li> <li>MySQL Server Installation</li> <li>MySQL Client/Shell Installation</li> </ul>"},{"location":"blog/2025/05/25/beyond-the-schema-a-practical-guide-to-querying-and-interacting-with-sqlite-mysql--postgresql/#gui-tools-mentioned-in-docker-setups","title":"GUI Tools (mentioned in Docker setups)","text":"<ul> <li>Adminer (for PostgreSQL &amp; others)</li> <li>phpMyAdmin (for MySQL)</li> </ul>"},{"location":"projects/","title":"Projects","text":"<p>Welcome to my project portfolio! Here you'll find a curated selection of my best work, spanning data engineering, automation, web development, and more. Explore featured highlights or browse the full list below.</p>"},{"location":"projects/#featured-projects","title":"\ud83d\ude80 Featured Projects","text":"<ul> <li> <p> Examination Management System DB</p> <p>Robust, production-ready database system for managing exams, students, proctoring, and results. Multi-RDBMS support (SQLite, MySQL, PostgreSQL), Python automation, Dockerized environments, and automated testing.</p> <p> Read More</p> </li> <li> <p> S3 Faker</p> <p>Fake data generator with AWS S3 (LocalStack) integration. Generates large datasets using Python &amp; Faker, supports CSV/JSON/Parquet, and automates uploads for testing cloud pipelines.</p> <p> Read More</p> </li> <li> <p> Paraxcel</p> <p>Python toolkit for advanced Excel data extraction, transformation, and visualization. Built with Pandas, Openpyxl, Matplotlib, and Seaborn for seamless spreadsheet analytics.</p> <p> Read More</p> </li> <li> <p> Naukri Webscraper</p> <p>Selenium-powered Python tool to automate job search and data extraction from Naukri.com. Features skill-based filtering, CSV export, and robust automated testing with pytest.</p> <p> Read More</p> </li> <li> <p> Test Management Site</p> <p>Dynamic, responsive web app for test creation, execution, and result tracking. Built with vanilla JS, HTML, CSS, Bootstrap, and localStorage for a seamless frontend experience.</p> <p> Read More</p> </li> </ul>"},{"location":"projects/#all-projects","title":"\ud83d\udcda All Projects","text":"LatestA\u2013Z <ul> <li> <p> Examination Management System DB Multi-RDBMS exam/test management, automation, Docker, Python, CI-ready.</p> </li> <li> <p> S3 Faker Fake data generator with S3/LocalStack integration for cloud testing.</p> </li> <li> <p> Paraxcel Excel data extraction, transformation, and visualization toolkit.</p> </li> <li> <p> Naukri Webscraper Automated job scraping, filtering, and CSV export from Naukri.com.</p> </li> <li> <p> Test Management Site Frontend web app for test management and result tracking.</p> </li> </ul> <ol> <li> Examination Management System DB</li> <li> Naukri Webscraper</li> <li> Paraxcel</li> <li> S3 Faker</li> <li> Test Management Site</li> </ol>"},{"location":"projects/#why-these-projects","title":"\ud83c\udf1f Why These Projects?","text":"<p>Each featured project demonstrates a unique blend of technical depth, problem-solving, and real-world impact\u2014from scalable database design and cloud automation to advanced data analytics and modern web development. Explore the detailed write-ups for code samples, visuals, and outcomes.</p>"},{"location":"projects/ems-db/","title":"Examination Management System Database","text":"In-Hurry Summary <p>Examination Management System Database A database designed to manage tests and examinations, covering student information, test details, questions, proctoring, and results.</p> <ul> <li>Context: <code>Personal Project</code>, <code>Apr 2024</code>, <code>SQLite</code>, <code>MySQL</code>, <code>PostgreSQL</code>, <code>DB Design</code></li> <li>Role: Sole Database Designer and Implementer</li> <li>Impact: Enabled efficient test administration and monitoring by creating a structured database, facilitating quick data retrieval and reporting.</li> </ul>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#overview","title":"Overview","text":"<p>The Examination Management System Database project aimed to design and implement a robust database for managing tests and examinations. It covered key aspects such as student details, tests, questions, test sessions, proctors, and results. The project was initially implemented in SQLite, and later expanded into three variants: SQLite, MySQL, and PostgreSQL, each in its own directory with a consistent file structure.</p> <p>Recent updates:</p> <ul> <li>Project restructured into three dedicated directories: sqlite, mysql, and psql, each with its own schema, queries, and scripts.  </li> <li>Added Docker-based development environments for each variant (see each directory's <code>compose.yaml</code>).  </li> <li>Introduced Python automation scripts (<code>db.py</code>) and pytest-based test suites for all variants, using the appropriate Python database connectors.  </li> <li>Enhanced schema with advanced triggers, views, and indexes for performance and integrity.  </li> <li>Improved documentation and usage instructions (see <code>README.md</code>).</li> </ul>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#goals","title":"Goals","text":"<p>The primary objectives of the project were:</p> <ul> <li>To create a robust, modular database schema to support all core processes of test administration.</li> <li>To enable efficient monitoring and analysis of test sessions, including proctoring and event auditing.</li> <li>To facilitate easy generation of reports summarizing test outcomes and student performance.</li> <li>To ensure extensibility and maintainability across multiple RDBMS backends.</li> </ul>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#responsibilities","title":"Responsibilities","text":"<p>As the sole database designer and implementer, my responsibilities included:</p> <ul> <li>Designing the database schema for SQLite, MySQL, and PostgreSQL.</li> <li>Implementing tables, relationships, triggers, views, and indexes for each variant.</li> <li>Developing and maintaining Python scripts (<code>db.py</code>) for database automation and management.</li> <li>Creating and running automated tests using pytest for all database variants.</li> <li>Setting up Docker-based development environments for consistent local and CI/CD workflows.</li> <li>Writing and maintaining comprehensive documentation.</li> </ul>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#technologies-used","title":"Technologies Used","text":"<ul> <li>Languages: SQL, Python</li> <li>Databases: SQLite, MySQL, PostgreSQL</li> <li>Python Connectors: <ul> <li><code>sqlite3</code> (for SQLite)  </li> <li><code>mysql-connector-python</code> (for MySQL)  </li> <li><code>psycopg2</code> (for PostgreSQL)</li> </ul> </li> <li>Project Management: <ul> <li>Consistent directory structure for all variants  </li> <li><code>README.md</code>, <code>usage.md</code>, and <code>README.Docker.md</code> for each variant</li> </ul> </li> </ul> Tools <ul> <li>Testing: <code>pytest</code> (with <code>test_db.py</code> in each variant)</li> <li>Dependency Management: <code>uv</code> (for fast Python environment setup)</li> <li>Containerization: Docker, Docker Compose (with dedicated <code>Dockerfile</code> and <code>compose.yaml</code> in each variant)</li> <li>Shells/CLI: <ul> <li><code>sqlite3</code> CLI (for SQLite)  </li> <li><code>mysql</code>/<code>mysqlsh</code> CLI (for MySQL)  </li> <li><code>psql</code> CLI (for PostgreSQL)</li> </ul> </li> <li>Database GUIs (via Docker):<ul> <li><code>phpMyAdmin</code> (for MySQL)  </li> <li><code>adminer</code> (for PostgreSQL)</li> </ul> </li> <li>Reverse Proxy: Traefik (for local service routing in Docker)</li> <li>Documentation: Markdown, Mermaid (for ER diagrams)</li> </ul> <p>Note : All development and testing environments are containerized for consistency and reproducibility.</p>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#process","title":"Process","text":"<p>The project followed a structured approach:</p> <ol> <li>Conceptual Design: Identified entities and relationships required for the examination system.</li> <li>Logical Design: Translated the conceptual design into detailed schemas for each RDBMS.</li> <li>Physical Design: Implemented the schema, triggers, indexes, and views in each database.</li> <li>Testing: Inserted sample data and ran automated queries and tests to validate the design and performance.</li> <li>Optimization: Added indexes and views to improve query performance and usability.</li> <li>Automation: Developed Python-based scripts and test suites for all variants.</li> <li>Containerization: Provided Docker Compose files for reproducible development environments.</li> </ol>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#challenges-solutions","title":"Challenges &amp;  Solutions","text":"<ol> <li> <p>Multi-Database Support </p> <ul> <li> Ensuring consistent schema and logic across SQLite, MySQL, and PostgreSQL.  </li> <li> Modularized schema and queries, and used automated tests to validate behavior across all supported databases.</li> </ul> </li> <li> <p>Data Consistency and Integrity </p> <ul> <li> Maintaining data integrity with complex triggers and relationships.  </li> <li> Implemented advanced triggers and constraints in each variant, with automated testing for validation.</li> </ul> </li> <li> <p>Query Performance Optimization </p> <ul> <li> Optimizing query performance for complex reporting and history tracking.  </li> <li> Created targeted indexes and materialized complex logic into views for efficient access.</li> </ul> </li> <li> <p>Automation and Environment Consistency </p> <ul> <li> Ensuring reliable development and testing environments across platforms.  </li> <li> Used Docker Compose for each variant and automated CI/CD with GitHub Actions.</li> </ul> </li> </ol>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#achievements","title":"Achievements","text":"<ul> <li>Designed and implemented a comprehensive, production-ready database schema for managing an examination system in SQLite, MySQL, and PostgreSQL.</li> <li>Automated scoring, feedback, event logging, and report generation using advanced triggers.</li> <li>Simplified complex queries and reporting through reusable views.</li> <li>Improved query performance by adding strategic indexes.</li> <li>Achieved high test coverage for schema logic and data flows in all variants.</li> <li>Provided Docker-based environments for easy setup and reproducibility.</li> </ul>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#key-learnings","title":"Key Learnings","text":"<ul> <li>The importance of modular design for multi-database support.</li> <li>Effective use of triggers, indexes, and views for data integrity and performance.</li> <li>How to automate database testing and management with Python and Docker.</li> <li>The value of consistent documentation and directory structure for maintainability.</li> </ul>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#outcomes","title":"Outcomes","text":"<p>The database provides a structured and efficient way to manage tests and examinations. It supports CRUD operations, test creation, session monitoring, proctoring event auditing, and automated report generation. The use of triggers, views, and indexes significantly improved data integrity and query performance. Automated tests and Docker environments ensure ongoing reliability and ease of use across all supported databases.</p>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#visuals","title":"Visuals","text":"","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#er-diagram","title":"ER Diagram","text":"<p> screenshot of the DB for SQLite/MySQL/PostgreSQL showing the schema.</p>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#video-overview","title":"Video overview","text":"","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#links","title":"Links","text":"<ul> <li>GitHub Repository</li> </ul>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#conclusion","title":"Conclusion","text":"<p>The Examination Management System Database project successfully delivered a robust and efficient solution for managing tests and examinations across multiple database platforms. It met the outlined goals and provided valuable insights into database design, automation, and optimization. The project demonstrated the importance of structured database design, modularity, and the effective use of triggers, views, indexes, and automated testing to enhance performance and maintain data integrity.</p> AI Skill Assessment <p>Prompt<sup>1</sup> Source </p> <ol> <li> <p>This <code>AI skill assessment</code> was generated based on the skill-assessment-prompt.md and the provided project documentation. It is intended as an illustrative summary and should be interpreted in the context of the available code and documentation in codebase.\u00a0\u21a9</p> </li> </ol>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#strengths","title":"Strengths","text":"<ul> <li> <p>Relational Database Design: </p> <ul> <li>Strong understanding of relational modeling, normalization, and entity relationships.</li> <li>Consistent schema design across SQLite, MySQL, and PostgreSQL, with appropriate use of constraints, foreign keys, and indexes.</li> <li>Advanced use of triggers for automation (e.g., scoring, feedback, session/event management).</li> </ul> </li> <li> <p>SQL Proficiency: </p> <ul> <li>Proficient in writing complex SQL queries, views, and batch scripts for all three RDBMS.</li> <li>Good use of views to abstract and simplify reporting and analytics.</li> </ul> </li> <li> <p>Python Automation &amp; Testing: </p> <ul> <li>Automated database setup and validation using Python (<code>db.py</code> scripts).</li> <li>Pytest-based test suites for each variant, using correct connectors (<code>sqlite3</code>, <code>mysql-connector-python</code>, <code>psycopg2</code>).</li> <li>Use of fixtures and in-memory databases for efficient, isolated testing.</li> </ul> </li> <li> <p>DevOps &amp; Environment Management: </p> <ul> <li>Docker and Docker Compose used for reproducible development environments for each database variant.</li> <li>Clear, modular directory structure and environment setup instructions.</li> <li>Use of uv for Python dependency management.</li> </ul> </li> <li> <p>Documentation: </p> <ul> <li>Well-structured Markdown documentation, usage guides, and ER diagrams.</li> <li>Clear separation of concerns and instructions for each database backend.</li> </ul> </li> </ul>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#areas-for-growth","title":"Areas for Growth","text":"<ul> <li> <p>CI/CD Integration: </p> <ul> <li>No current implementation of automated CI/CD pipelines (e.g., GitHub Actions, GitLab CI).</li> <li>Adding automated build/test on push would further professionalize the workflow.</li> </ul> </li> <li> <p>GUI/UX Tools: </p> <ul> <li>No use of GUI database tools (e.g., DB Browser for SQLite) in workflow; all interactions are CLI or script-based.</li> <li>Could consider adding optional GUI instructions for broader accessibility.</li> </ul> </li> <li> <p>Security &amp; Advanced Features: </p> <ul> <li>No implementation of advanced security (role-based access, encryption, etc.).</li> <li>No support for subjective question types or broader educational/administrative features.</li> </ul> </li> <li> <p>Scalability &amp; Production Readiness: </p> <ul> <li>Focus is on schema and logic, not on production deployment, backup, or scaling strategies.</li> </ul> </li> </ul>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#role-suitability","title":"Role Suitability","text":"","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#best-fit-roles","title":"Best Fit Roles","text":"<ul> <li>Database Engineer / Database Developer</li> <li>Backend Developer (with strong SQL/database focus)</li> <li>DevOps Engineer (entry to mid-level, especially for DB environments)</li> <li>QA Automation Engineer (for database systems)</li> </ul>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#well-suited-for","title":"Well-Suited For","text":"<ul> <li>Projects requiring robust relational schema design and automation.</li> <li>Teams needing multi-database support and migration-ready code.</li> <li>Environments where automated testing and reproducible dev setups are valued.</li> </ul>","tags":["Database Design","Database Management"]},{"location":"projects/ems-db/#less-suited-for","title":"Less Suited For","text":"<ul> <li>Frontend/UI/UX-heavy roles.</li> <li>Roles requiring deep experience in cloud-native, distributed, or NoSQL systems.</li> <li>Security-focused or enterprise-scale production DB admin roles (without further experience).</li> </ul> <p>Summary: You demonstrate strong skills in relational database design, SQL, Python automation, and environment management. You are well-suited for roles focused on database engineering, backend development, and DevOps for database-driven projects. Expanding into CI/CD, security, and production operations would further broaden your profile.</p>","tags":["Database Design","Database Management"]},{"location":"projects/naukri-webscraper/","title":"Naukri Webscraper","text":"Quick Summary <p>Naukri Webscraper A Python tool that automates job searches on Naukri.com, enabling users to filter listings by skills and export structured data for analysis.</p> <ul> <li>Context: <code>Personal Project</code>, <code>Mar 2024 \u2013 Mar 2025</code>, <code>Python</code>, <code>Selenium</code>, <code>Pandas</code></li> <li>Role: Sole developer\u2014designed, implemented, tested, and documented the project</li> <li>Impact: Automated skill-based job search and CSV export, validated by automated tests and real-world data extraction</li> </ul>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#overview","title":"Overview","text":"<p>Naukri Webscraper is a Python-based automation tool that scrapes job listings from Naukri.com. It extracts job titles, companies, salaries, locations, and required skills, then filters results based on user-specified skills. The project outputs structured data as CSV files for further analysis. The project was developed independently as a personal automation and data analysis initiative.</p> <p>Recent updates:</p> <ul> <li>Added automated tests for core scraping and filtering logic (<code>test_project.py</code>) using <code>pytest</code></li> <li>Improved error handling and robustness in dynamic content extraction</li> <li>Updated documentation and usage instructions in <code>README.md</code></li> </ul>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#goals","title":"Goals","text":"<ul> <li>Automate the retrieval and filtering of job listings from Naukri.com based on user-defined skills.</li> <li>Simplify and accelerate the job search process by eliminating manual filtering.</li> <li>Provide structured, exportable data for further analysis.</li> </ul>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#responsibilities","title":"Responsibilities","text":"<ul> <li>Designed and implemented the scraping logic using Selenium WebDriver.</li> <li>Developed skill-based filtering and CSV export using Pandas.</li> <li>Addressed dynamic content loading and missing data scenarios.</li> <li>Authored automated tests with <code>pytest</code> to ensure code correctness and reliability.</li> <li>Wrote comprehensive documentation and usage instructions.</li> </ul>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#technologies-used","title":"Technologies Used","text":"<ul> <li>Languages: Python (primary language for all scripts and logic)</li> <li>Frameworks/Libraries: <ul> <li>Selenium (browser automation and web scraping)  </li> <li>Pandas (data manipulation and CSV export)</li> </ul> </li> <li>Testing: <ul> <li>pytest (for automated tests in <code>test_project.py</code>)</li> </ul> </li> <li>DevOps/Tools: <ul> <li>Git (version control)</li> <li>Chrome WebDriver (browser automation)</li> </ul> </li> <li>Documentation: <ul> <li>Markdown (<code>README.md</code> for usage and setup)</li> </ul> </li> </ul> Tools <ul> <li>Git (version control)</li> <li>Chrome WebDriver (browser automation)</li> <li>Markdown (project documentation)</li> <li>pytest (automated testing)</li> </ul>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#process","title":"Process","text":"<ol> <li>Planning: <ul> <li>Identified key data fields (title, company, salary, location, skills) to extract from Naukri.com.</li> </ul> </li> <li>Implementation: <ul> <li>Used Selenium to automate browser actions and extract job data.</li> <li>Employed Pandas for data structuring and CSV export.</li> <li>Developed a filtering mechanism for user-specified skills.</li> </ul> </li> <li>Testing: <ul> <li>Created automated tests (<code>test_project.py</code>) using <code>pytest</code> to validate scraping and filtering logic.</li> </ul> </li> <li>Documentation: <ul> <li>Documented setup, usage, and troubleshooting in <code>README.md</code>.</li> </ul> </li> </ol>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#challenges-solutions","title":"Challenges &amp;  Solutions","text":"<ol> <li> <p>Dynamic Content Loading</p> <ul> <li> Naukri.com uses JavaScript to render job listings, causing timing issues for scraping.</li> <li> Used Selenium's <code>WebDriverWait</code> to ensure elements are loaded before extraction.</li> </ul> </li> <li> <p>Complex HTML Structures</p> <ul> <li> Extracting data from inconsistent or nested HTML elements.</li> <li> Implemented a helper function (<code>get_text_or_default</code>) for robust text extraction.</li> </ul> </li> <li> <p>Performance Bottlenecks</p> <ul> <li> Slow scraping due to large result sets and dynamic content.</li> <li> Optimized data extraction loops and used efficient Pandas operations for filtering/export.</li> </ul> </li> <li> <p>Testing Automation</p> <ul> <li> Ensuring scraping logic remains reliable as site structure changes.</li> <li> Developed automated tests with <code>pytest</code> to validate core logic and catch regressions.</li> </ul> </li> </ol> Note : On Site Changes and Locators <ul> <li>The HTML structure and element locators (CSS selectors, XPaths) used in <code>project.py</code> are based on the current version of Naukri.com.  </li> <li>If the website updates its layout or class names, you may need to update these locators in the code to restore scraping functionality.  </li> <li>Review and adjust selectors in <code>project.py</code> if you encounter errors or missing data after a site update.</li> </ul>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#achievements","title":"Achievements","text":"<ul> <li>Automated the extraction and filtering of job listings from Naukri.com.</li> <li>Enabled skill-based filtering and CSV export for downstream analysis.</li> <li>Developed a test suite (<code>test_project.py</code>) using <code>pytest</code> to ensure reliability.</li> <li>Improved scraping robustness and error handling based on real-world site changes.</li> </ul>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#key-learnings","title":"Key Learnings","text":"<ul> <li>Gained practical experience with Selenium for dynamic web scraping.</li> <li>Enhanced skills in data manipulation and export using Pandas.</li> <li>Learned to write maintainable, testable code for web automation projects.</li> <li>Understood the importance of robust error handling and documentation.</li> <li>Applied <code>pytest</code> for effective and maintainable automated testing.</li> </ul>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#outcomes","title":"Outcomes","text":"<ul> <li>Successfully automated job search and filtering for Naukri.com.</li> <li>Produced structured CSV datasets for analysis.</li> <li>Provided a reusable, documented tool for job seekers and data analysts.</li> <li>Ensured code reliability through automated testing with <code>pytest</code>.</li> </ul>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#visuals","title":"Visuals","text":"","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#video-demo","title":"Video Demo","text":"","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#links","title":"Links","text":"<ul> <li>GitHub Repository</li> </ul>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#conclusion","title":"Conclusion","text":"<p>Naukri Webscraper demonstrates the power of Python automation for real-world data extraction and analysis. By combining Selenium and Pandas, the project streamlines job searches, enhances productivity, and provides actionable insights through structured data exports. The codebase is robust, tested with <code>pytest</code>, and well-documented for future use and extension.</p> AI Skill Assessment <p>Prompt<sup>1</sup> Source </p> <ol> <li> <p>This <code>AI skill assessment</code> was generated based on the skill-assessment-prompt.md and the provided project documentation. It is intended as an illustrative summary and should be interpreted in the context of the available code and documentation in codebase.\u00a0\u21a9</p> </li> </ol>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#strengths","title":"Strengths","text":"<ul> <li> <p>Web Scraping Automation</p> <ul> <li>Demonstrates strong proficiency with Selenium for browser automation, including headless operation, custom user agents, and dynamic navigation (e.g., paginated scraping, handling search forms).</li> <li>Robust handling of web elements using both CSS selectors and XPaths, with fallback/default logic for missing elements.</li> </ul> </li> <li> <p>Data Handling &amp; Export</p> <ul> <li>Uses Pandas effectively for data manipulation and CSV export.</li> <li>Implements structured data extraction with a clear schema (job title, company, salary, skills, etc.).</li> </ul> </li> <li> <p>Testing &amp; Quality Assurance</p> <ul> <li>Provides automated tests using <code>pytest</code>, including fixtures, mocking user input, and live web tests (with appropriate skips for anti-bot/site change issues).</li> <li>Tests cover both core scraping logic and utility functions.</li> </ul> </li> <li> <p>Documentation</p> <ul> <li>Comprehensive technical documentation (<code>doc.md</code>) and user-facing README.md with clear instructions, schema definitions, troubleshooting, and usage examples.</li> <li>Documents function purposes, parameters, and expected behaviors in code docstrings.</li> </ul> </li> <li> <p>User Interaction &amp; Error Handling</p> <ul> <li>Interactive CLI prompts for user input (search terms, page count, skill filters).</li> <li>Handles invalid input and exceptions gracefully (e.g., <code>KeyboardInterrupt</code>, <code>ValueError</code>, missing elements).</li> </ul> </li> <li> <p>Project Structure &amp; Packaging</p> <ul> <li>Uses pyproject.toml for dependency management and project metadata.</li> <li>Separates main logic, tests, and documentation cleanly.</li> </ul> </li> </ul>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#areas-for-growth","title":"Areas for Growth","text":"<ul> <li> <p>Security &amp; Anti-Bot Evasion</p> <ul> <li>No evidence of advanced anti-bot evasion techniques (e.g., proxy rotation, CAPTCHA handling, request throttling beyond simple sleep).</li> <li>No explicit handling of robots.txt or ethical scraping considerations in code.</li> </ul> </li> <li> <p>Scalability &amp; Performance</p> <ul> <li>Scraping is single-threaded and synchronous; not optimized for large-scale or parallel scraping.</li> <li>No batching, queuing, or distributed scraping logic.</li> </ul> </li> <li> <p>CI/CD &amp; DevOps</p> <ul> <li>No evidence of CI/CD pipelines, Dockerization, or deployment automation.</li> <li>No Makefile or scripts for environment setup/testing.</li> </ul> </li> <li> <p>Code Modularity &amp; Extensibility</p> <ul> <li>All logic is in a single script (<code>project.py</code>); could benefit from modularization (e.g., separating scraping, filtering, and CLI logic).</li> <li>No plugin or configuration system for adapting to site changes.</li> </ul> </li> <li> <p>Error Logging &amp; Monitoring</p> <ul> <li>Uses print statements for errors; lacks structured logging or monitoring for production use.</li> </ul> </li> <li> <p>GUI/UX</p> <ul> <li>No GUI or web interface; CLI-only interaction.</li> </ul> </li> </ul>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#role-suitability","title":"Role Suitability","text":"","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#best-fit-roles","title":"Best Fit Roles","text":"<ul> <li>Python Backend Developer<ul> <li>Strong evidence of backend scripting, data processing, and automation skills.</li> </ul> </li> <li>Web Scraping/Data Extraction Engineer<ul> <li>Demonstrated expertise in Selenium, data extraction, and handling dynamic web content.</li> </ul> </li> <li>QA Automation Engineer<ul> <li>Experience with automated testing, mocking, and test-driven development using <code>pytest</code>.</li> </ul> </li> <li>Technical Writer/Documentation Specialist<ul> <li>High-quality, thorough documentation and user guides.</li> </ul> </li> </ul>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#well-suited-for","title":"Well-Suited For","text":"<ul> <li>Data Analyst (with Python)<ul> <li>Familiarity with Pandas and CSV data workflows.</li> </ul> </li> <li>SDET (Software Development Engineer in Test)<ul> <li>Automated test coverage and test design.</li> </ul> </li> </ul>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/naukri-webscraper/#less-suited-for","title":"Less Suited For","text":"<ul> <li>Frontend Developer<ul> <li>No evidence of frontend/UI development (web or desktop).</li> </ul> </li> <li>DevOps Engineer<ul> <li>Lacks CI/CD, containerization, and deployment automation.</li> </ul> </li> <li>Cloud/Distributed Systems Engineer<ul> <li>No cloud integration, distributed scraping, or scalable architecture.</li> </ul> </li> </ul> <p>Summary: The developer demonstrates strong skills in Python scripting, web scraping automation with Selenium, data processing with Pandas, and automated testing with <code>pytest</code>. The codebase is well-documented and user-friendly, with robust error handling and interactive CLI features. Areas for growth include modularization, scalability, advanced anti-bot techniques, and DevOps practices. The developer is best suited for backend, automation, and data extraction roles, and less suited for frontend or DevOps-focused positions based on the current codebase.</p>","tags":["Python","Web Scraping","Selenium","Automation","Data Analysis"]},{"location":"projects/paraxcel/","title":"Paraxcel","text":"Quick Summary <p>Paraxcel A lightweight, local-first Python desktop application using Tkinter to convert Microsoft Word DOCX files containing multiple-choice questions into structured Excel spreadsheets.</p> <ul> <li>Context: <code>Python</code>, <code>Tkinter</code>, <code>Pandas</code>, <code>Pydantic</code>, <code>python-docx</code>, <code>Solo Project</code>, <code>Feb-Mar 2025</code></li> <li>Role: Sole developer responsible for design, implementation, testing, documentation, and packaging of the application.</li> <li>Impact: Created a tool that automates the extraction of questions and answers from DOCX files, reducing manual data entry time for educators and content creators, by implementing parsing logic with <code>python-docx</code> and structuring output with <code>pandas</code>.</li> </ul>","tags":["Python"]},{"location":"projects/paraxcel/#overview","title":"Overview","text":"<p>Paraxcel is a Python desktop application built with Tkinter that addresses the need for converting multiple-choice questions from DOCX files into an organized Excel format, it targets educators, content creators, and assessment professionals who need to manage question banks efficiently. The application provides a simple graphical user interface for file selection and conversion, running entirely locally.</p>","tags":["Python"]},{"location":"projects/paraxcel/#goals","title":"Goals","text":"<p>The primary goals for the Paraxcel project were:</p> <ul> <li>To automate the tedious and time-consuming manual process of extracting multiple-choice questions and their corresponding answers from Microsoft Word documents.</li> <li>To structure the extracted data into a usable and organized Excel format.</li> <li>To create a simple, reliable, and accessible desktop tool for educators and content creators.</li> </ul>","tags":["Python"]},{"location":"projects/paraxcel/#responsibilities","title":"Responsibilities","text":"<ul> <li>Designed the application architecture, including module separation (<code>docx_parser</code>, <code>excel_writer</code>, <code>model</code>, <code>para_utility</code>, <code>interface</code>) for maintainability and scalability.</li> <li>Implemented robust <code>DOCX</code> parsing using <code>python-docx</code> to accurately extract <code>questions</code>, <code>answer options</code>, and identify the <code>correct answer</code> based on formatting (<code>color/highlight</code>).</li> <li>Utilized pandas to structure extracted data into a standardized, clean format, enabling reliable export to <code>.xlsx</code> files.</li> <li>Built a user-friendly graphical interface with <code>Tkinter</code>, enabling users to easily select input files/folders and initiate the conversion process.</li> <li>Integrated Pydantic for rigorous data validation of extracted question data, ensuring data integrity before export.</li> <li>Created essential utility functions (<code>para_utility.py</code>) for text cleaning, format handling, and precise answer detection.</li> <li>Authored comprehensive technical (<code>doc.md</code>) and user (<code>README.md</code>) documentation.</li> <li>Packaged the application into a standalone executable using <code>PyInstaller</code> for straightforward distribution and use on Windows.</li> </ul>","tags":["Python"]},{"location":"projects/paraxcel/#technologies-used","title":"Technologies Used","text":"<ul> <li>Languages: Python</li> <li>GUI: Tkinter (Standard Python library) - For building the desktop graphical interface.</li> <li>DOCX Parsing: <code>python-docx</code> - For reading and analyzing <code>.docx</code> file content.</li> <li>Data Handling &amp; Excel Export: <code>pandas</code> - For structuring the extracted data and writing to <code>.xlsx</code> files.</li> <li>Data Validation: <code>Pydantic</code> - For validating the structure and types of extracted question data.</li> <li>Documentation: <code>Markdown</code> - For <code>README.md</code> and <code>doc.md</code>.</li> </ul> Tools <ul> <li>Version Control: Git</li> <li>Packaging: PyInstaller - For creating the standalone executable.</li> <li>Development Environment: VS Code</li> </ul>","tags":["Python"]},{"location":"projects/paraxcel/#process","title":"Process","text":"<p>The development process involved identifying the need for a simple DOCX-to-Excel conversion tool for MCQs and followed a structured approach focused on modularity and ease of use.</p> <ol> <li>Requirement Gathering: Defined the core functionality: parse DOCX files containing questions followed by four options and export them to Excel, including support for detecting marked answers.</li> <li>Technology Stack Selection: Chose libraries (<code>python-docx</code>, <code>pandas</code>, <code>Tkinter</code>, <code>Pydantic</code>) best suited for the task, balancing functionality with ease of deployment (local-first, standard libraries).</li> <li>Modular Implementation: Developed each component (<code>parsing</code>, <code>writing</code>, <code>GUI</code>, <code>validation</code>) as a distinct module.</li> <li>Testing &amp; Refinement: Used sample files to rigorously test parsing accuracy and output format.</li> <li>Documentation: Created user and technical guides to support adoption and understanding.</li> <li>Packaging: Prepared the application for distribution as a single executable.</li> </ol>","tags":["Python"]},{"location":"projects/paraxcel/#challenges-solutions","title":"Challenges &amp;  Solutions","text":"<ol> <li> <p>Handling Varied DOCX Formatting</p> <ul> <li> Parsing semi-structured DOCX files presented challenges due to inconsistencies in formatting, numbering, and spacing. Reliably detecting the correct answer based on subtle formatting like font color or highlighting was a key challenge.</li> <li> Developed flexible parsing logic (<code>parse_para</code>) designed to accommodate common variations. Implemented specialized utility functions (<code>remove_prefix</code>, <code>find_marked_answer</code>) that leverage python-docx's capabilities to accurately identify marked answers by inspecting run-level formatting properties. Documented input format expectations clearly to guide users.</li> </ul> </li> <li> <p>Ensuring Data Quality and Consistency</p> <ul> <li> Extracting data from a semi-structured format like DOCX risked incomplete or malformed records before export.</li> <li> Integrated Pydantic models (<code>Question</code>) to enforce a strict schema for extracted data. This validation step acts as a safeguard, ensuring that only correctly structured and typed data proceeds to the <code>Excel</code> export, preventing errors and ensuring reliable output.</li> </ul> </li> <li> <p>Creating an Accessible Tool for Non-Technical Users</p> <ul> <li> The goal was a tool usable by educators without programming knowledge, requiring a simple interface and easy installation.</li> <li> Built a straightforward and intuitive GUI using Tkinter, Python's standard library, minimizing external dependencies. Used <code>PyInstaller</code> to bundle the application and all its dependencies into a single, easy-to-distribute executable (<code>paraxcel.exe</code>), significantly lowering the barrier to entry for end-users.</li> </ul> </li> </ol>","tags":["Python"]},{"location":"projects/paraxcel/#achievements","title":"Achievements","text":"<ul> <li>Developed and launched Paraxcel, a functional desktop application, automating the conversion of MCQs from DOCX to a structured Excel format.</li> <li>Implemented advanced parsing features, including the ability to detect correct answers based on font color or highlighting within the DOCX file.</li> <li>Incorporated basic text formatting handling (superscript/subscript) during extraction for improved data fidelity.</li> <li>Provided clear, user-focused documentation (<code>README.md</code>) and technical insights (<code>doc.md</code>).</li> <li>Packaged the application into a convenient standalone executable using PyInstaller, simplifying deployment and usage.</li> </ul> <p>Impact: Enabled educators and content creators to save significant time and effort (quantified by reduced manual data entry hours) previously spent on manual data entry.</p>","tags":["Python"]},{"location":"projects/paraxcel/#key-learnings","title":"Key Learnings","text":"<ul> <li>Gained practical experience using the <code>python-docx</code> library to parse the structure and formatting of Word documents programmatically.</li> <li>Developed skills in building simple desktop GUIs with Python's built-in <code>Tkinter</code> library.</li> <li>Applied <code>Pydantic</code> for robust data validation in a data processing pipeline.</li> <li>Utilized <code>pandas</code> for efficient data structuring and exporting to Excel formats.</li> <li>Learned the process of packaging Python applications into standalone executables using <code>PyInstaller</code>, including handling dependencies and data files.</li> <li>Understood the challenges and importance of defining clear input format expectations when parsing semi-structured documents like DOCX.</li> </ul>","tags":["Python"]},{"location":"projects/paraxcel/#outcomes","title":"Outcomes","text":"<ul> <li>A working, local-first desktop application (<code>paraxcel.exe</code>) capable of converting DOCX files (containing questions and 4 options) into structured Excel (<code>.xlsx</code>) files.</li> <li>Source code is available on GitHub, along with documentation and sample files.</li> <li>A video demonstration showcasing the application's functionality.</li> </ul>","tags":["Python"]},{"location":"projects/paraxcel/#visuals","title":"Visuals","text":"<p>Docx Input</p> <p>Q1. What is the capital of France? A. Berlin B. Madrid C. Paris (Highlighted as correct) D. Rome  </p> <p>\u2705 Excel Output</p> Question Option 1 Option 2 Option 3 Option 4 Answer Index What is the capital of France? Berlin Madrid Paris Rome 3","tags":["Python"]},{"location":"projects/paraxcel/#screenshots","title":"\ud83d\uddbc\ufe0f Screenshots","text":"<p>Paraxcel Tkinter GUI showing file/folder selection fields and buttons.</p> <p></p> <p>Sample input DOCX file snippet showing question/option format.</p> <p></p> <p>Resulting Excel file snippet showing structured data.</p>","tags":["Python"]},{"location":"projects/paraxcel/#video-demo","title":"\ud83d\udd17 Video Demo","text":"","tags":["Python"]},{"location":"projects/paraxcel/#links","title":"Links","text":"<ul> <li>GitHub Repository</li> </ul>","tags":["Python"]},{"location":"projects/paraxcel/#conclusion","title":"Conclusion","text":"<p>Paraxcel successfully provides a targeted solution for automating the often tedious task of extracting multiple-choice questions from DOCX files into a more usable Excel format. By leveraging libraries like <code>python-docx</code>, <code>pandas</code>, <code>Pydantic</code>, and <code>Tkinter</code>, the project delivers a functional, easy-to-use desktop tool for educators and content creators. Key takeaways include the practical application of these libraries for document parsing, data handling, validation, GUI development, and application packaging, resulting in a useful utility that addresses a specific workflow challenge.</p> AI Skill Assessment <p>Prompt<sup>1</sup> Source </p> <ol> <li> <p>This AI skill assessment was generated based on the skill-assessment-prompt.md and the provided project documentation. It is intended as an illustrative summary and should be interpreted in the context of the available code and documentation in codebase.\u00a0\u21a9</p> </li> </ol>","tags":["Python"]},{"location":"projects/paraxcel/#strengths","title":"Strengths","text":"<ul> <li>Python Application Development: Proven ability to design, develop, and package a complete, modular desktop application.</li> <li>GUI Development (Tkinter): Experience building functional graphical interfaces for user interaction.</li> <li>Document Parsing &amp; Data Processing: Skilled in extracting structured data from complex document formats (<code>.docx</code>) and processing it using <code>pandas</code>.</li> <li>Data Validation: Practical application of <code>Pydantic</code> for ensuring data integrity and correctness.</li> <li>Comprehensive Documentation: Ability to create clear technical and user-focused documentation.</li> <li>Application Packaging &amp; Distribution: Experience using <code>PyInstaller</code> for creating standalone executables and managing dependencies.</li> <li>CI/CD Implementation: Basic experience setting up automated workflows for testing, security checks, and builds using GitHub Actions.</li> <li>Software Reliability Basics: Inclusion of testing tools and security scanning indicates an understanding of foundational quality practices.</li> </ul>","tags":["Python"]},{"location":"projects/paraxcel/#areas-for-improvement","title":"Areas for Improvement","text":"<ul> <li>Testing Depth: Expanding test coverage and visibility would further strengthen quality assurance processes.</li> <li>Advanced Error Handling: Implementing more granular logging and exception handling could enhance application robustness.</li> <li>Performance Optimization: Exploring techniques for handling very large files more efficiently could improve scalability.</li> <li>UI/UX: For projects requiring more complex interfaces, exploring modern GUI frameworks might be beneficial.</li> <li>Cross-Platform Deployment: Expanding build support beyond Windows would increase application accessibility.</li> </ul>","tags":["Python"]},{"location":"projects/paraxcel/#relevant-roles","title":"Relevant Roles","text":"","tags":["Python"]},{"location":"projects/paraxcel/#strong-fit","title":"Strong Fit","text":"<ul> <li>Python Application Developer: Directly aligns with the project's nature.</li> <li>Automation Engineer: Demonstrates strong skills in automating data extraction and processing workflows.</li> </ul>","tags":["Python"]},{"location":"projects/paraxcel/#good-fit","title":"Good Fit","text":"<ul> <li>Backend Developer (Data Focus): Relevant experience in data parsing, validation, and structuring.</li> <li>Junior DevOps/Build Engineer: Basic experience with CI/CD automation and application packaging.</li> </ul>","tags":["Python"]},{"location":"projects/paraxcel/#less-direct-fit","title":"Less Direct Fit","text":"<ul> <li>Frontend Web Developer: No web technology experience shown.</li> <li>Data Scientist/ML Engineer: Project focuses on extraction, not analysis or modeling.</li> <li>Senior DevOps/SRE: Lacks infrastructure, monitoring, or cloud services.</li> <li>Mobile Developer: No mobile development experience shown.</li> </ul>","tags":["Python"]},{"location":"projects/paraxcel/#conclusion_1","title":"Conclusion","text":"<p>This project effectively showcases capabilities in end-to-end Python application development, particularly in document processing, data handling, and automation. The inclusion of data validation, packaging, and basic CI/CD demonstrates a well-rounded approach to software development. This experience is highly relevant for roles focused on Python application development, automation, and data processing pipelines.</p>","tags":["Python"]},{"location":"projects/s3-faker/","title":"S3 Faker","text":"<p>S3 Faker is a tool designed to generate fake data based on a JSON configuration file. The generated data can be saved locally and also uploaded to an AWS S3 bucket. This project is ideal for testing and development purposes, allowing developers to simulate S3 environments without the need for actual AWS resources. Key features include data generation using the Faker library, support for multiple output formats (CSV, JSON, Parquet), and integration with AWS S3 via s3fs.</p>"},{"location":"projects/s3-faker/#responsibilities","title":"Responsibilities","text":"<ul> <li>Designed and implemented the core functionalities of the S3 Faker project.</li> <li>Developed scripts and modules to accurately simulate S3 behavior.</li> <li>Led the integration of the project with existing development and testing pipelines.</li> <li>Ensured the project adhered to best practices in terms of security and performance.</li> <li>Coordinated with team members to gather requirements and provide technical guidance.</li> </ul>"},{"location":"projects/s3-faker/#technologies-used","title":"Technologies Used","text":"<ul> <li>Languages: Python, PowerShell, Shell</li> <li>Frameworks/Libraries: Faker, Pandas, fsspec</li> <li>Tools: Git, Docker, LocalStack, AWS CLI</li> </ul>"},{"location":"projects/s3-faker/#challenges-and-solutions","title":"Challenges and Solutions","text":"<ul> <li>Challenge: Simulating the comprehensive feature set of Amazon S3, including edge cases.</li> <li>Solution: Conducted extensive research on S3 APIs and utilized <code>fsspec</code> to implement accurate simulations. Developed custom scripts to handle edge cases and ensure robustness.</li> <li>Challenge: Ensuring performance and scalability of the local S3 environment.</li> <li>Solution: Optimized code and utilized Docker for containerization, allowing for scalable and isolated testing environments.</li> </ul>"},{"location":"projects/s3-faker/#achievements","title":"Achievements","text":"<ul> <li>Successfully created a fully functional S3 simulation environment, reducing reliance on actual S3 resources by 80%.</li> <li>Integrated the project with CI/CD pipelines, significantly speeding up the development and testing cycles.</li> <li>Received positive feedback from team members and external testers for the accuracy and reliability of the simulation.</li> </ul>"},{"location":"projects/s3-faker/#key-learnings","title":"Key Learnings","text":"<ul> <li>Gained in-depth knowledge of Amazon S3 APIs and their intricacies.</li> <li>Enhanced skills in Python and PowerShell scripting.</li> <li>Improved understanding of containerization and its benefits in development and testing environments.</li> <li>Learned the importance of thorough testing and documentation in ensuring project success.</li> </ul>"},{"location":"projects/s3-faker/#link-to-project","title":"Link to Project","text":"<ul> <li>GitHub Repository</li> </ul>"},{"location":"projects/s3-faker/#screenshots","title":"Screenshots","text":""},{"location":"projects/test-site/","title":"Test Management Site","text":"In-Hurry Summary <p>Test Management Site A demo vanilla javascripting application for managing tests, enabling test creation, execution, and result tracking. Aims to showcase basic web application functionality using dynamic UI updates and local storage.</p> <ul> <li>Context: <code>Personal Project</code>, <code>Jun 2024</code>, <code>HTML</code>, <code>CSS</code>, <code>JavaScript</code>, <code>Bootstrap</code></li> <li>Role: Sole Developer - Responsible for designing, developing, and implementing all features.</li> <li>Impact: Demonstrated full-stack capabilities by creating a functional test management system, showcasing skills in UI design, dynamic content loading, and local data storage.</li> </ul>","tags":["Frontend"]},{"location":"projects/test-site/#overview","title":"Overview","text":"<p>The Test Management Site is a demo vanilla javascripts application developed in April 2025 to showcase test management capabilities. It allows users to create, manage, take tests, and view results. The application uses HTML, CSS, JavaScript, and Bootstrap for a responsive and dynamic user interface.</p>","tags":["Frontend"]},{"location":"projects/test-site/#goals","title":"Goals","text":"<ul> <li>Provide a platform to create, update, and delete tests.</li> <li>Enable users to take tests with a specified duration.</li> <li>Display test results and history.</li> <li>Implement user management features like login, logout, and password changes.</li> <li>Showcase dynamic UI updates and local data storage.</li> </ul>","tags":["Frontend"]},{"location":"projects/test-site/#responsibilities","title":"Responsibilities","text":"<ul> <li>Designed and developed the entire application from scratch.</li> <li>Implemented dynamic UI updates using JavaScript.</li> <li>Managed data storage using <code>localStorage</code>.</li> <li>Created user authentication and session management features.</li> <li>Integrated Bootstrap for responsive design and UI components.</li> </ul>","tags":["Frontend"]},{"location":"projects/test-site/#technologies-used","title":"Technologies Used","text":"<ul> <li>Languages: JavaScript, HTML, CSS</li> <li>Frameworks/Libraries: Bootstrap, PapaParse, XLSX.js, Plotly.js</li> <li>Tools: Visual Studio Code, GitHub Actions (for CI/CD and static site deployment)</li> <li>Browser APIs: localStorage, sessionStorage, Fetch API</li> <li>Other: Modular JavaScript (ES6 modules), CSS custom properties, GitHub Pages (hosting)</li> </ul>","tags":["Frontend"]},{"location":"projects/test-site/#process","title":"Process","text":"<p>The project followed an iterative development approach. Initially, the basic HTML structure and CSS styling were set up. JavaScript was then used to dynamically load content, manage user sessions, and handle data storage. Bootstrap was integrated to ensure a responsive design. Challenges were addressed through continuous debugging and refinement of the code.</p>","tags":["Frontend"]},{"location":"projects/test-site/#challenges-solutions","title":"Challenges &amp;  Solutions","text":"<ol> <li> <p>Dynamic Content Loading </p> <ul> <li> Loading and updating content dynamically using JavaScript.  </li> <li> Used <code>fetch</code> API to load HTML templates and JavaScript to manipulate the DOM, ensuring smooth transitions and updates.</li> </ul> </li> <li> <p>Data Management with <code>localStorage</code> </p> <ul> <li> Managing and persisting data using <code>localStorage</code>.  </li> <li> Implemented functions to serialize and deserialize data, ensuring data integrity and persistence across sessions.</li> </ul> </li> <li> <p>Responsive Design </p> <ul> <li> Ensuring the application is responsive across different devices.  </li> <li> Utilized Bootstrap's grid system and CSS media queries to create a responsive layout.</li> </ul> </li> </ol>","tags":["Frontend"]},{"location":"projects/test-site/#achievements","title":"Achievements","text":"<ul> <li>Successfully implemented all core features: test management, test taking, results display, and user management.</li> <li>Created a dynamic and responsive user interface.</li> <li>Demonstrated proficiency in JavaScript, HTML, CSS, and Bootstrap.</li> <li>Implemented data persistence using <code>localStorage</code>.</li> </ul>","tags":["Frontend"]},{"location":"projects/test-site/#key-learnings","title":"Key Learnings","text":"<ul> <li>Gained a deeper understanding of dynamic content loading and manipulation using JavaScript.</li> <li>Learned how to effectively use <code>localStorage</code> for data persistence.</li> <li>Improved skills in responsive web design using Bootstrap.</li> <li>Enhanced problem-solving abilities through debugging and refining the code.</li> </ul>","tags":["Frontend"]},{"location":"projects/test-site/#outcomes","title":"Outcomes","text":"<p>The project resulted in a functional test management application that showcases dynamic UI updates and local data storage. The application allows users to create, manage, take tests, and view results.</p>","tags":["Frontend"]},{"location":"projects/test-site/#visuals","title":"Visuals","text":"","tags":["Frontend"]},{"location":"projects/test-site/#screenshot-project-images","title":"Screenshot Project Images","text":"<p>Here are some images showcasing the project:</p>","tags":["Frontend"]},{"location":"projects/test-site/#login-page","title":"Login Page","text":"","tags":["Frontend"]},{"location":"projects/test-site/#register-page","title":"Register Page","text":"","tags":["Frontend"]},{"location":"projects/test-site/#test-selection","title":"Test Selection","text":"","tags":["Frontend"]},{"location":"projects/test-site/#test-page","title":"Test Page","text":"","tags":["Frontend"]},{"location":"projects/test-site/#test-timeout","title":"Test Timeout","text":"","tags":["Frontend"]},{"location":"projects/test-site/#dashboard","title":"Dashboard","text":"","tags":["Frontend"]},{"location":"projects/test-site/#user-management","title":"User Management","text":"","tags":["Frontend"]},{"location":"projects/test-site/#crud-operations","title":"CRUD Operations","text":"","tags":["Frontend"]},{"location":"projects/test-site/#test-results","title":"Test Results","text":"","tags":["Frontend"]},{"location":"projects/test-site/#error-handling","title":"Error Handling","text":"","tags":["Frontend"]},{"location":"projects/test-site/#links","title":"Links","text":"<ul> <li>Live Project</li> <li>GitHub Repository</li> </ul>","tags":["Frontend"]},{"location":"projects/test-site/#conclusion","title":"Conclusion","text":"<p>The Test Management Site project was a valuable learning experience that allowed me to demonstrate my full-stack capabilities. By creating a functional application with dynamic UI updates and local data storage, I showcased my skills in JavaScript, HTML, CSS, and Bootstrap. Success was measured by the successful implementation of all core features and the creation of a responsive and dynamic user interface.</p> AI Skill Assessment <p>Prompt<sup>1</sup> Source </p> <ol> <li> <p>This <code>AI skill assessment</code> was generated based on the skill-assessment-prompt.md and the provided project documentation. It is intended as an illustrative summary and should be interpreted in the context of the available code and documentation in codebase.\u00a0\u21a9</p> </li> </ol>","tags":["Frontend"]},{"location":"projects/test-site/#strengths","title":"Strengths","text":"<ul> <li> <p>Frontend Engineering (HTML/CSS/JS):</p> <ul> <li>Demonstrates strong skills in vanilla JavaScript for DOM manipulation, modular code organization, and dynamic UI updates (e.g., main.js, <code>js/tests_content.js</code>).</li> <li>Uses modern CSS with custom properties, responsive design, and theme toggling (see <code>styles/css/styles.css</code>).</li> <li>Implements modular HTML with reusable components loaded dynamically (e.g., sidebar, topnav, profile, as seen in sidebar.js and HTML assets).</li> </ul> </li> <li> <p>Data Handling &amp; Storage:</p> <ul> <li>Effectively uses <code>localStorage</code> and <code>sessionStorage</code> for persistent and session-based data (user profiles, test data, results).</li> <li>Handles file uploads and parsing for CSV and Excel formats using third-party libraries (PapaParse, XLSX.js), with robust error handling and user feedback (<code>js/utility/file_handle.js</code>).</li> </ul> </li> <li> <p>Object-Oriented Design:</p> <ul> <li>Defines clear, extensible classes for domain entities (User, Profiles, Test, Question, Result, etc.) with encapsulated logic (<code>js/utility/temp_data.js</code>, <code>js/utility/temp_profile.js</code>).</li> </ul> </li> <li> <p>User Experience &amp; Usability:</p> <ul> <li>Provides detailed user feedback and error messages throughout the UI (e.g., form validation, alerts for missing data, dynamic content updates).</li> <li>Implements accessibility features such as keyboard navigation and clear visual cues for active elements.</li> </ul> </li> <li> <p>Documentation &amp; Readability:</p> <ul> <li>Includes a comprehensive readme.md with project overview, features, technical details, folder structure, and screenshots.</li> <li>Uses descriptive variable/function names and inline comments for clarity.</li> </ul> </li> <li> <p>Basic DevOps Awareness:</p> <ul> <li>Includes a GitHub Actions workflow for static site deployment to GitHub Pages (<code>.github/workflows/deploy.yml</code>).</li> </ul> </li> </ul>","tags":["Frontend"]},{"location":"projects/test-site/#areas-for-growth","title":"Areas for Growth","text":"<ul> <li> <p>Testing:</p> <ul> <li>No evidence of automated unit, integration, or end-to-end tests. All logic appears to be tested manually via the UI.</li> <li>No test framework or test scripts present.</li> </ul> </li> <li> <p>Security:</p> <ul> <li>User authentication is handled entirely client-side with passwords stored in plain text in localStorage/sessionStorage.</li> <li>No input sanitization or protection against XSS/CSRF.</li> <li>No encryption or secure handling of sensitive data.</li> </ul> </li> <li> <p>Scalability &amp; Backend Integration:</p> <ul> <li>The application is entirely client-side; there is no backend API, database, or server-side logic.</li> <li>Not suitable for multi-user or production environments without significant changes.</li> </ul> </li> <li> <p>Accessibility &amp; Internationalization:</p> <ul> <li>While some accessibility is present, there is no evidence of ARIA roles, screen reader support, or internationalization/localization.</li> </ul> </li> <li> <p>Advanced DevOps/CI/CD:</p> <ul> <li>Only basic static site deployment is present; no evidence of automated testing, linting, or code quality checks in CI.</li> </ul> </li> <li> <p>Code Reuse &amp; DRY Principles:</p> <ul> <li>Some repeated logic (e.g., dynamic form handling, error messages) could be further abstracted for maintainability.</li> </ul> </li> </ul>","tags":["Frontend"]},{"location":"projects/test-site/#role-suitability","title":"Role Suitability","text":"","tags":["Frontend"]},{"location":"projects/test-site/#best-fit-roles","title":"Best Fit Roles","text":"<ul> <li> <p>Frontend Developer (Vanilla JS/HTML/CSS):</p> <ul> <li>Demonstrated ability to build interactive, modular, and responsive web UIs from scratch without frameworks.</li> </ul> </li> <li> <p>UI/UX Engineer (Prototype/Demo Level):</p> <ul> <li>Strong focus on user flows, feedback, and dynamic content for demo or MVP applications.</li> </ul> </li> <li> <p>Web Application Prototyper:</p> <ul> <li>Skilled at quickly assembling functional prototypes using client-side technologies and third-party libraries.</li> </ul> </li> </ul>","tags":["Frontend"]},{"location":"projects/test-site/#well-suited-for","title":"Well-Suited For","text":"<ul> <li> <p>Technical Writer/Documenter:</p> <ul> <li>Good documentation practices and clear code organization.</li> </ul> </li> <li> <p>Client-Side Data Engineer:</p> <ul> <li>Experience with data parsing, transformation, and storage in browser environments.</li> </ul> </li> </ul>","tags":["Frontend"]},{"location":"projects/test-site/#less-suited-for","title":"Less Suited For","text":"<ul> <li>Backend Developer / Full Stack Engineer:<ul> <li>No evidence of backend, API, or database design/implementation.</li> </ul> </li> <li>DevOps Engineer (Advanced):<ul> <li>Only basic CI/CD; lacks advanced automation, monitoring, or infrastructure-as-code.</li> </ul> </li> <li>Security Engineer:<ul> <li>No secure authentication, authorization, or data protection practices.</li> </ul> </li> <li>QA/Test Automation Engineer:<ul> <li>No automated testing or test infrastructure.</li> </ul> </li> </ul> <p>Summary: The developer demonstrates strong skills in vanilla JavaScript, modular frontend architecture, dynamic UI/UX, and client-side data handling. The codebase is well-documented, readable, and suitable for demo or prototype-level applications. However, there is a lack of automated testing, security best practices, backend integration, and advanced DevOps. The developer is best suited for roles focused on frontend development, rapid prototyping, and UI/UX engineering in client-side environments.</p>","tags":["Frontend"]},{"location":"blog/archive/2025/","title":"2025","text":""}]}